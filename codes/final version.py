import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings('ignore')
from dm_test import quick_dm_analysis

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - ä¼˜åŒ–ç‰ˆï¼ˆé‡‡ç”¨ä»£ç äºŒç‰¹å¾å·¥ç¨‹ï¼‰".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šç§»é™¤éšè—çŠ¶æ€ç‰¹å¾ï¼Œç®€åŒ–ç‰¹å¾å·¥ç¨‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ".center(100))
print("=" * 100)

# ========== åŠ è½½æ•°æ® ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ ==========
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# ========== æ·»åŠ æ»åç‰¹å¾ ==========
def add_features(X, y):
    """å®Œå…¨ä½¿ç”¨ä»£ç äºŒçš„ç‰¹å¾ç­–ç•¥"""
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# ========== å®šä¹‰æ¨¡å‹ ==========
def build_simple_lstm(input_shape):
    """ä»£ç äºŒçš„LSTMç»“æ„"""
    return Sequential([
        layers.LSTM(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


def build_simple_gru(input_shape):
    """ä»£ç äºŒçš„GRUç»“æ„"""
    return Sequential([
        layers.GRU(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


# ========== OOFé¢„æµ‹ç”Ÿæˆï¼ˆç§»é™¤éšè—çŠ¶æ€æå–ï¼‰==========
def get_oof_predictions(X_seq, y_seq, model_type='lstm', n_splits=5):
    """å®Œå…¨é‡‡ç”¨ä»£ç äºŒçš„OOFç­–ç•¥ï¼šåªè¿”å›é¢„æµ‹å€¼ï¼Œä¸æå–éšè—çŠ¶æ€"""
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")

    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = build_simple_lstm((X_seq.shape[1], X_seq.shape[2]))
        else:
            model = build_simple_gru((X_seq.shape[1], X_seq.shape[2]))

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()

        fold += 1

    return oof_preds  # åªè¿”å›é¢„æµ‹å€¼ï¼Œä¸è¿”å›éšè—çŠ¶æ€


print("\n" + "=" * 100)
print("ç¬¬ä¸€æ­¥ï¼šç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(100))
print("=" * 100)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF RÂ²: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF RÂ²: {r2_score(y_train_seq, gru_oof_preds):.4f}")

# ========== è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ==========
print("\n" + "=" * 100)
print("ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹".center(100))
print("=" * 100)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=80,
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    ),
    layers.Dropout(0.3),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(units=100, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    layers.Dropout(0.3),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

# è·å–é¢„æµ‹
lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

lstm_train_pred = lstm_final.predict(X_train_seq, verbose=0).flatten()
gru_train_pred = gru_final.predict(X_train_seq, verbose=0).flatten()

# è¿‡æ‹Ÿåˆè¯Šæ–­
lstm_train_r2 = r2_score(y_train_seq, lstm_train_pred)
lstm_test_r2 = r2_score(y_test_seq, lstm_test_pred)
gru_train_r2 = r2_score(y_train_seq, gru_train_pred)
gru_test_r2 = r2_score(y_test_seq, gru_test_pred)

print(f"\nã€è¿‡æ‹Ÿåˆè¯Šæ–­ã€‘")
print(f"LSTM - è®­ç»ƒRÂ²: {lstm_train_r2:.4f}, æµ‹è¯•RÂ²: {lstm_test_r2:.4f}, å·®è·: {lstm_train_r2 - lstm_test_r2:.4f}")
print(f"GRU  - è®­ç»ƒRÂ²: {gru_train_r2:.4f}, æµ‹è¯•RÂ²: {gru_test_r2:.4f}, å·®è·: {gru_train_r2 - gru_test_r2:.4f}")

overfitting_detected = max(lstm_train_r2 - lstm_test_r2, gru_train_r2 - gru_test_r2) > 0.15
if overfitting_detected:
    print(f"âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œå°†é‡‡ç”¨ä¿å®ˆæ®‹å·®å­¦ä¹ ç­–ç•¥ï¼")
else:
    print(f"âœ“ è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½ï¼Œå¯å°è¯•å¤šç§æ®‹å·®ç­–ç•¥")


# ========== ã€å…³é”®ä¿®æ”¹ã€‘é‡‡ç”¨ä»£ç äºŒçš„ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========

def create_simplified_features(X_flat, lstm_preds, gru_preds):
    """ä»£ç äºŒçš„ç®€åŒ–ç‰¹å¾ï¼šåŸå§‹ + é¢„æµ‹ + å¹³å‡ + å·®å¼‚ï¼ˆç§»é™¤éšè—çŠ¶æ€ï¼‰"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


def create_minimal_features(X_flat, lstm_preds, gru_preds):
    """æœ€å°åŒ–ç‰¹å¾ï¼šåŸå§‹ + é¢„æµ‹"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    return np.hstack(features_list)


# ========== æ®‹å·®å­¦ä¹ è®­ç»ƒå‡½æ•° ==========
def train_conservative_xgboost(X_train, y_train):
    """ä¿å®ˆXGBoostå‚æ•°"""
    model = XGBRegressor(
        n_estimators=100,
        learning_rate=0.01,
        max_depth=3,
        min_child_weight=5,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_ridge_model(X_train, y_train, alpha=10.0):
    """Ridgeçº¿æ€§å›å½’"""
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    return model


# ========== æ®‹å·®å¤„ç†å‡½æ•° ==========
def clip_residual(residual_pred, threshold=2.0):
    """å‰ªè£æç«¯æ®‹å·®å€¼"""
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    """åŠ æƒæ®‹å·®ä¿®æ­£"""
    return base_pred + weight * residual_pred


# ========== åŸºå‡†ï¼šç®€å•å¹³å‡ ==========
print("\n" + "=" * 100)
print("ã€åŸºå‡†ã€‘ç®€å•å¹³å‡èåˆ".center(100))
print("=" * 100)

avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"ç®€å•å¹³å‡æµ‹è¯•é›†RÂ²: {avg_r2:.4f}")

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds
avg_oof_preds = (lstm_oof_preds + gru_oof_preds) / 2
avg_oof_residual = y_train_seq - avg_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# ========== ç­–ç•¥é›†åˆ ==========
strategies_results = {}

# å‡†å¤‡ç‰¹å¾ï¼ˆç§»é™¤éšè—çŠ¶æ€ï¼‰
basic_features_train = X_train_flat[:len(gru_oof_preds)]
basic_features_test = X_test_flat

simplified_train = create_simplified_features(
    basic_features_train, lstm_oof_preds, gru_oof_preds
)
simplified_test = create_simplified_features(
    basic_features_test, lstm_test_pred, gru_test_pred
)

minimal_train = create_minimal_features(basic_features_train, lstm_oof_preds, gru_oof_preds)
minimal_test = create_minimal_features(basic_features_test, lstm_test_pred, gru_test_pred)

print(f"\nç‰¹å¾ç»´åº¦:")
print(f"  ç®€åŒ–ç‰¹å¾ï¼ˆä»£ç äºŒé£æ ¼ï¼‰: {simplified_train.shape[1]} ç»´")
print(f"  æœ€å°åŒ–ç‰¹å¾: {minimal_train.shape[1]} ç»´")

# ========== ç­–ç•¥1ï¼šLSTMæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥1ã€‘LSTMæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_lstm_conservative = train_conservative_xgboost(simplified_train, lstm_oof_residual)
lstm_residual = xgb_lstm_conservative.predict(simplified_test)
pred_lstm_residual = lstm_test_pred + lstm_residual

r2_lstm_residual = r2_score(y_test_seq, pred_lstm_residual)
print(f"âœ“ RÂ²: {r2_lstm_residual:.4f} (vsç®€å•å¹³å‡: {r2_lstm_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '] = pred_lstm_residual

# ========== ç­–ç•¥2ï¼šGRUæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥2ã€‘GRUæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_gru_conservative = train_conservative_xgboost(simplified_train, gru_oof_residual)
gru_residual = xgb_gru_conservative.predict(simplified_test)
pred_gru_residual = gru_test_pred + gru_residual

r2_gru_residual = r2_score(y_test_seq, pred_gru_residual)
print(f"âœ“ RÂ²: {r2_gru_residual:.4f} (vsç®€å•å¹³å‡: {r2_gru_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '] = pred_gru_residual

# ========== ç­–ç•¥3ï¼šåŒæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥3ã€‘åŒæ®‹å·®å­¦ä¹ ï¼š(LSTM+GRU)/2 + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_dual = train_conservative_xgboost(simplified_train, avg_oof_residual)
dual_residual = xgb_dual.predict(simplified_test)
pred_dual = avg_test_pred + dual_residual

r2_dual = r2_score(y_test_seq, pred_dual)
print(f"âœ“ RÂ²: {r2_dual:.4f} (vsç®€å•å¹³å‡: {r2_dual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '] = pred_dual

# ========== ç­–ç•¥4ï¼šæ®‹å·®å‰ªè£ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥4ã€‘GRUæ®‹å·®å­¦ä¹  + æ®‹å·®å‰ªè£".center(100))
print("=" * 100)

gru_residual_clipped = clip_residual(gru_residual, threshold=2.0)
pred_gru_clipped = gru_test_pred + gru_residual_clipped

r2_gru_clipped = r2_score(y_test_seq, pred_gru_clipped)
print(f"âœ“ RÂ²: {r2_gru_clipped:.4f} (vsç®€å•å¹³å‡: {r2_gru_clipped - avg_r2:+.4f})")
print(f"  æ®‹å·®å‰ªè£å‰std: {np.std(gru_residual):.6f}, å‰ªè£åstd: {np.std(gru_residual_clipped):.6f}")
strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'] = pred_gru_clipped

# ========== ç­–ç•¥5ï¼šåŠ æƒèåˆï¼ˆ30%ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥5ã€‘GRUæ®‹å·®å­¦ä¹  + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_gru_weighted = weighted_residual_correction(gru_test_pred, gru_residual, weight=0.3)

r2_gru_weighted = r2_score(y_test_seq, pred_gru_weighted)
print(f"âœ“ RÂ²: {r2_gru_weighted:.4f} (vsç®€å•å¹³å‡: {r2_gru_weighted - avg_r2:+.4f})")
strategies_results['ç­–ç•¥5-åŠ æƒèåˆ30%'] = pred_gru_weighted

# ========== ç­–ç•¥6ï¼šç»ˆæç»„åˆ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥6ã€‘ç»ˆæç»„åˆï¼šæ®‹å·®å‰ªè£ + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_ultimate = weighted_residual_correction(gru_test_pred, gru_residual_clipped, weight=0.3)

r2_ultimate = r2_score(y_test_seq, pred_ultimate)
print(f"âœ“ RÂ²: {r2_ultimate:.4f} (vsç®€å•å¹³å‡: {r2_ultimate - avg_r2:+.4f})")
strategies_results['ç­–ç•¥6-ç»ˆæç»„åˆ'] = pred_ultimate

# ========== ç­–ç•¥7ï¼šåŠ¨æ€æƒé‡èåˆ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥7ã€‘åŠ¨æ€æƒé‡èåˆï¼šRidgeå­¦ä¹ LSTMå’ŒGRUæƒé‡".center(100))
print("=" * 100)

lstm_res_train = xgb_lstm_conservative.predict(simplified_train)
gru_res_train = xgb_gru_conservative.predict(simplified_train)

lstm_res_test = xgb_lstm_conservative.predict(simplified_test)
gru_res_test = xgb_gru_conservative.predict(simplified_test)

meta_features_train = np.column_stack([
    lstm_oof_preds + lstm_res_train,
    gru_oof_preds + gru_res_train
])
meta_features_test = np.column_stack([
    lstm_test_pred + lstm_res_test,
    gru_test_pred + gru_res_test
])

meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_features_train, y_train_seq)
pred_dynamic = meta_model.predict(meta_features_test)

lstm_weight = meta_model.coef_[0]
gru_weight = meta_model.coef_[1]

r2_dynamic = r2_score(y_test_seq, pred_dynamic)
print(f"å­¦ä¹ åˆ°çš„æƒé‡: LSTM={lstm_weight:.4f}, GRU={gru_weight:.4f}")
print(f"âœ“ RÂ²: {r2_dynamic:.4f} (vsç®€å•å¹³å‡: {r2_dynamic - avg_r2:+.4f})")
strategies_results['ç­–ç•¥7-åŠ¨æ€æƒé‡èåˆ'] = pred_dynamic

# ========== ç­–ç•¥8ï¼šRidgeæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥8ã€‘Ridgeæ®‹å·®å­¦ä¹ ï¼šæœ€ä¿å®ˆçš„çº¿æ€§æ¨¡å‹".center(100))
print("=" * 100)

ridge_model = train_ridge_model(simplified_train, gru_oof_residual, alpha=10.0)
ridge_residual = ridge_model.predict(simplified_test)
pred_ridge = gru_test_pred + ridge_residual

r2_ridge = r2_score(y_test_seq, pred_ridge)
print(f"âœ“ RÂ²: {r2_ridge:.4f} (vsç®€å•å¹³å‡: {r2_ridge - avg_r2:+.4f})")
strategies_results['ç­–ç•¥8-Ridgeæ®‹å·®'] = pred_ridge

# ========== ç»¼åˆå¯¹æ¯” ==========
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

all_strategies = {
    'LSTMå•æ¨¡å‹': lstm_test_pred,
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç®€å•å¹³å‡(åŸºçº¿)': avg_test_pred,
    **strategies_results
}

print(f"\n{'ç­–ç•¥':<30} {'RÂ²':>10} {'vsåŸºçº¿':>10} {'MAE':>12} {'RMSE':>12}")
print("-" * 75)

results_list = []
for name, pred in all_strategies.items():
    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    improvement = r2 - avg_r2
    results_list.append((name, r2, improvement, mae, rmse, pred))
    print(f"{name:<30} {r2:>10.4f} {improvement:>10.4f} {mae:>12.6f} {rmse:>12.6f}")

# æ’åº
results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’åï¼ˆæŒ‰RÂ²é™åºï¼‰".center(100))
print("=" * 100)

best_r2 = results_list[0][1]
best_name = results_list[0][0]

for rank, (name, r2, improvement, mae, rmse, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<30} RÂ²={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_name} (RÂ² = {best_r2:.4f})")

# ========== åŸå§‹å°ºåº¦è¯„ä¼° ==========
print("\n" + "=" * 100)
print("åŸå§‹å°ºåº¦æ€§èƒ½å¯¹æ¯”".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))
strategies_original = {}

print(f"\n{'ç­–ç•¥':<30} {'RÂ²':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 75)

for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    strategies_original[name] = pred_original

    r2 = r2_score(y_test_original, pred_original)
    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))
    mape = np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))

    print(f"{name:<30} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")

# ========== å¯è§†åŒ– ==========
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

print("\n" + "=" * 100)
print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨".center(100))
print("=" * 100)

# 1. è®­ç»ƒè¿‡ç¨‹
fig = plt.figure(figsize=(16, 5))

plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(lstm_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('LSTMæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(gru_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('GRUæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '01_training_process.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾1: è®­ç»ƒè¿‡ç¨‹æ›²çº¿")

# 2. æ€§èƒ½æ’åæŸ±çŠ¶å›¾
fig, ax = plt.subplots(figsize=(16, 8))

strategy_names = [name for name, _, _, _, _, _ in results_list]
r2_scores = [r2 for _, r2, _, _, _, _ in results_list]
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(strategy_names)))

bars = ax.barh(range(len(strategy_names)), r2_scores, color=colors, alpha=0.8)

ax.axvline(x=avg_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

for i, (bar, r2, improvement) in enumerate(zip(bars, r2_scores, [imp for _, _, imp, _, _, _ in results_list])):
    label = f'{r2:.4f}'
    if improvement > 0:
        label += f' (+{improvement:.4f})'
        color = 'green'
    elif improvement < 0:
        label += f' ({improvement:.4f})'
        color = 'red'
    else:
        color = 'black'

    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(strategy_names)))
ax.set_yticklabels(strategy_names, fontsize=10)
ax.set_xlabel('RÂ² Score', fontsize=12, fontweight='bold')
ax.set_title('æ‰€æœ‰ç­–ç•¥æ€§èƒ½æ’åå¯¹æ¯”', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(results_directory + '02_performance_ranking.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾2: æ€§èƒ½æ’åå¯¹æ¯”")

# 3. Top5ç­–ç•¥é¢„æµ‹å¯¹æ¯”
fig, axes = plt.subplots(3, 2, figsize=(18, 15))
axes = axes.flatten()

top5_strategies = results_list[:6]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top5_strategies):
    ax = axes[idx]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    r2_original = r2_score(y_test_original, pred_original)

    ax.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax.plot(pred_original, label=name, linewidth=2, alpha=0.8)
    ax.set_title(f'{name}\nRÂ²={r2_original:.4f} (vsåŸºçº¿: {improvement:+.4f})',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('æ ·æœ¬åºå·', fontsize=9)
    ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '03_top6_strategies_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾3: Top6ç­–ç•¥é¢„æµ‹å¯¹æ¯”")

# 4. æ®‹å·®åˆ†æå¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

residuals_dict = {
    'ç®€å•å¹³å‡': y_test_seq - avg_test_pred,
    'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ': y_test_seq - strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥4-æ®‹å·®å‰ªè£': y_test_seq - strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'],
    'ç­–ç•¥8-Ridgeæ®‹å·®': y_test_seq - strategies_results['ç­–ç•¥8-Ridgeæ®‹å·®']
}

for idx, (name, residual) in enumerate(residuals_dict.items()):
    ax = axes[idx // 2, idx % 2]
    ax.hist(residual, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
    ax.axvline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax.set_xlabel('æ®‹å·®')
    ax.set_ylabel('é¢‘æ•°')
    ax.text(0.05, 0.95, f'å‡å€¼={np.mean(residual):.5f}\nstd={np.std(residual):.5f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + '04_residual_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾4: æ®‹å·®åˆ†å¸ƒåˆ†æ")

# 5. æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”
fig, ax = plt.subplots(figsize=(16, 6))

for name, residual in residuals_dict.items():
    ax.plot(residual, label=name, linewidth=2, alpha=0.7)

ax.axhline(0, color='black', linestyle='--', linewidth=1)
ax.set_title('ä¸åŒç­–ç•¥æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xlabel('æ ·æœ¬åºå·', fontsize=12)
ax.set_ylabel('æ®‹å·®', fontsize=12)
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '05_residual_timeseries.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾5: æ®‹å·®æ—¶é—´åºåˆ—")

# 6. ç»¼åˆå¯¹æ¯”å›¾
plt.figure(figsize=(18, 8))
plt.plot(y_test_original, label='çœŸå®å€¼', linewidth=3, color='black', alpha=0.9, zorder=10)

key_strategies_for_plot = [
    ('ç®€å•å¹³å‡(åŸºçº¿)', strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'], 'gray'),
    ('ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '], 'blue'),
    ('ç­–ç•¥6-ç»ˆæç»„åˆ', strategies_original['ç­–ç•¥6-ç»ˆæç»„åˆ'], 'green'),
    ('ç­–ç•¥7-åŠ¨æ€æƒé‡èåˆ', strategies_original['ç­–ç•¥7-åŠ¨æ€æƒé‡èåˆ'], 'red'),
]

for name, pred, color in key_strategies_for_plot:
    plt.plot(pred, label=name, linewidth=1.8, alpha=0.7, color=color)

plt.title('å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”', fontsize=16, fontweight='bold')
plt.xlabel('æ ·æœ¬åºå·', fontsize=12)
plt.ylabel('ç‰ç±³ä»·æ ¼', fontsize=12)
plt.legend(fontsize=11, loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(results_directory + '06_key_strategies_comprehensive.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾6: å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”")

# 7. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

metrics_data = {}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    metrics_data[name] = {
        'RÂ²': r2_score(y_test_original, pred_original),
        'MAE': mean_absolute_error(y_test_original, pred_original),
        'RMSE': sqrt(mean_squared_error(y_test_original, pred_original)),
        'MAPE': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))
    }

top8_names = [name for name, _, _, _, _, _ in results_list[:min(8, len(results_list))]]
colors_bar = plt.cm.viridis(np.linspace(0, 1, len(top8_names)))

axes[0, 0].bar(range(len(top8_names)), [metrics_data[m]['RÂ²'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[0, 0].set_title('RÂ² åˆ†æ•°å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 0].set_ylabel('RÂ² Score')
axes[0, 0].set_xticks(range(len(top8_names)))
axes[0, 0].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[0, 0].grid(True, alpha=0.3, axis='y')

axes[0, 1].bar(range(len(top8_names)), [metrics_data[m]['MAE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[0, 1].set_title('MAE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].set_xticks(range(len(top8_names)))
axes[0, 1].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[0, 1].grid(True, alpha=0.3, axis='y')

axes[1, 0].bar(range(len(top8_names)), [metrics_data[m]['RMSE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[1, 0].set_title('RMSE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 0].set_ylabel('RMSE')
axes[1, 0].set_xticks(range(len(top8_names)))
axes[1, 0].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[1, 0].grid(True, alpha=0.3, axis='y')

axes[1, 1].bar(range(len(top8_names)), [metrics_data[m]['MAPE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[1, 1].set_title('MAPE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 1].set_ylabel('MAPE')
axes[1, 1].set_xticks(range(len(top8_names)))
axes[1, 1].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + '07_metrics_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾7: å¤šæŒ‡æ ‡å¯¹æ¯”")

# 8. é¢„æµ‹è¯¯å·®åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

top4_for_error = results_list[:4]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top4_for_error):
    ax = axes[idx // 2, idx % 2]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    errors = pred_original - y_test_original.flatten()

    ax.scatter(y_test_original, errors, alpha=0.5, s=30)
    ax.axhline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} - é¢„æµ‹è¯¯å·®åˆ†æ', fontsize=11, fontweight='bold')
    ax.set_xlabel('çœŸå®å€¼')
    ax.set_ylabel('é¢„æµ‹è¯¯å·®')
    ax.grid(True, alpha=0.3)

    ax.text(0.05, 0.95, f'MAE={mae:.2f}\nRMSE={rmse:.2f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))

plt.tight_layout()
plt.savefig(results_directory + '08_prediction_error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾8: é¢„æµ‹è¯¯å·®åˆ†æ")

# 9. æ¨¡å‹æ”¹è¿›æ•ˆæœé›·è¾¾å›¾
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='polar')

radar_strategies = ['ç®€å•å¹³å‡(åŸºçº¿)', 'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', 'ç­–ç•¥6-ç»ˆæç»„åˆ', 'ç­–ç•¥8-Ridgeæ®‹å·®']
categories = ['RÂ²', 'MAE', 'RMSE', 'MAPE']
N = len(categories)

angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

for strategy in radar_strategies:
    pred_original = strategies_original[strategy]

    r2_val = metrics_data[strategy]['RÂ²']
    mae_val = 1 / (1 + metrics_data[strategy]['MAE'] / 100)
    rmse_val = 1 / (1 + metrics_data[strategy]['RMSE'] / 100)
    mape_val = 1 / (1 + metrics_data[strategy]['MAPE'] * 100)

    values = [r2_val, mae_val, rmse_val, mape_val]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=strategy)
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories)
ax.set_ylim(0, 1)
ax.set_title('å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax.grid(True)

plt.tight_layout()
plt.savefig(results_directory + '09_performance_radar.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾9: æ€§èƒ½é›·è¾¾å›¾")

# 10. æ®‹å·®ç®±çº¿å›¾å¯¹æ¯”
fig, ax = plt.subplots(figsize=(14, 6))

residual_data = []
residual_labels = []

for name, residual in residuals_dict.items():
    residual_data.append(residual)
    residual_labels.append(name)

bp = ax.boxplot(residual_data, labels=residual_labels, patch_artist=True)

for patch, color in zip(bp['boxes'], plt.cm.Set3(np.linspace(0, 1, len(residual_data)))):
    patch.set_facecolor(color)

ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)
ax.set_title('æ®‹å·®åˆ†å¸ƒç®±çº¿å›¾å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_ylabel('æ®‹å·®å€¼')
ax.grid(True, alpha=0.3, axis='y')
plt.xticks(rotation=15, ha='right')

plt.tight_layout()
plt.savefig(results_directory + '10_residual_boxplot.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾10: æ®‹å·®ç®±çº¿å›¾")

print("\nâœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶ä¿å­˜ï¼")

# ========== ä¿å­˜æ¨¡å‹å’Œç»“æœ ==========
import pickle

print("\n" + "=" * 100)
print("ä¿å­˜æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_final.h5')
gru_final.save(results_directory + 'gru_final.h5')

# ä¿å­˜XGBoostæ¨¡å‹
with open(results_directory + 'xgb_lstm_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_lstm_conservative, f)

with open(results_directory + 'xgb_gru_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_gru_conservative, f)

with open(results_directory + 'xgb_dual.pkl', 'wb') as f:
    pickle.dump(xgb_dual, f)

with open(results_directory + 'ridge_meta_model.pkl', 'wb') as f:
    pickle.dump(meta_model, f)

with open(results_directory + 'ridge_residual_model.pkl', 'wb') as f:
    pickle.dump(ridge_model, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    predictions_dict[name.replace('/', '_').replace('(', '').replace(')', '')] = pred_original.flatten()

results_df = pd.DataFrame(predictions_dict)
results_df.to_csv(results_directory + 'all_predictions.csv', index=False)

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
metrics_list = []
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    metrics_list.append({
        'strategy': name,
        'r2': r2_score(y_test_original, pred_original),
        'mae': mean_absolute_error(y_test_original, pred_original),
        'rmse': sqrt(mean_squared_error(y_test_original, pred_original)),
        'mape': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8))),
        'improvement_vs_baseline': r2_score(y_test_original, pred_original) - r2_score(y_test_original,
                                                                                       strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'])
    })

metrics_df = pd.DataFrame(metrics_list)
metrics_df = metrics_df.sort_values('r2', ascending=False)
metrics_df.to_csv(results_directory + 'performance_metrics.csv', index=False)

print("\nâœ“ ä¿å­˜å®Œæˆï¼")
print(f"  - lstm_final.h5")
print(f"  - gru_final.h5")
print(f"  - xgb_lstm_conservative.pkl")
print(f"  - xgb_gru_conservative.pkl")
print(f"  - xgb_dual.pkl")
print(f"  - ridge_meta_model.pkl")
print(f"  - ridge_residual_model.pkl")
print(f"  - scalers.pkl")
print(f"  - all_predictions.csv")
print(f"  - performance_metrics.csv")

# ========== æœ€ç»ˆæ€»ç»“ ==========
print("\n" + "=" * 100)
print("ğŸ‰ ä¼˜åŒ–ç‰ˆèåˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆé‡‡ç”¨ä»£ç äºŒç‰¹å¾å·¥ç¨‹ï¼‰ï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š æ ¸å¿ƒæ”¹è¿›:")
print(f"  âœ“ é‡‡ç”¨ä»£ç äºŒçš„ç®€åŒ–ç‰¹å¾å·¥ç¨‹ï¼ˆç§»é™¤éšè—çŠ¶æ€ï¼‰")
print(f"  âœ“ ç‰¹å¾ç»´åº¦: 274ç»´ â†’ 74ç»´ï¼ˆé™ä½70%ï¼‰")
print(f"  âœ“ OOFé¢„æµ‹é˜²æ­¢ä¿¡æ¯æ³„éœ²")
print(f"  âœ“ è¿‡æ‹Ÿåˆè¯Šæ–­æœºåˆ¶")
print(f"  âœ“ ä¿ç•™8ç§é«˜æ•ˆæ®‹å·®å­¦ä¹ ç­–ç•¥")
print(f"  âœ“ 10å¼ é«˜è´¨é‡å¯è§†åŒ–å›¾è¡¨")

print(f"\nğŸ“ˆ å®éªŒç»“æœ:")
print(f"  LSTMå•æ¨¡å‹: RÂ² = {lstm_test_r2:.4f}")
print(f"  GRUå•æ¨¡å‹:  RÂ² = {gru_test_r2:.4f}")
print(f"  åŸºçº¿ï¼ˆç®€å•å¹³å‡ï¼‰: RÂ² = {avg_r2:.4f}")
print(f"  æœ€ä½³ç­–ç•¥: {best_name}")
print(f"  æœ€ä½³æ€§èƒ½: RÂ² = {best_r2:.4f}")
print(f"  æ€§èƒ½æå‡: {best_r2 - avg_r2:+.4f} ({(best_r2 - avg_r2) / avg_r2 * 100:+.2f}%)")

print(f"\nğŸ† Top5ç­–ç•¥æ’å:")
for rank, (name, r2, improvement, _, _, _) in enumerate(results_list[:5], 1):
    print(f"  {rank}. {name:<30} RÂ²={r2:.4f} (æ”¹è¿›: {improvement:+.4f})")

print(f"\nğŸ’¡ ç­–ç•¥åˆ†æ:")
if best_r2 > avg_r2 + 0.01:
    print(f"  âœ… æ®‹å·®å­¦ä¹ ç­–ç•¥æ˜¾è‘—æå‡æ€§èƒ½ï¼")
    print(f"  âœ… æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨: {best_name}")
elif best_r2 > avg_r2:
    print(f"  âš¡ æ®‹å·®å­¦ä¹ ç­–ç•¥ç•¥æœ‰æå‡")
    print(f"  ğŸ’¡ å¯æ ¹æ®è®¡ç®—æˆæœ¬é€‰æ‹©ç®€å•å¹³å‡æˆ–{best_name}")
else:
    print(f"  âš ï¸  æ®‹å·®å­¦ä¹ æœªè¶…è¿‡åŸºçº¿")
    print(f"  ğŸ’¡ å»ºè®®ç»§ç»­ä¼˜åŒ–åŸºç¡€æ¨¡å‹æˆ–ä½¿ç”¨ç®€å•å¹³å‡")

print(f"\nğŸ” è¿‡æ‹Ÿåˆåˆ†æ:")
if overfitting_detected:
    print(f"  âš ï¸  åŸºç¡€æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆï¼ˆLSTMå·®è·={lstm_train_r2 - lstm_test_r2:.4f}, GRUå·®è·={gru_train_r2 - gru_test_r2:.4f}ï¼‰")
    print(f"  ğŸ’¡ å·²é‡‡ç”¨ç®€åŒ–ç‰¹å¾+ä¿å®ˆç­–ç•¥ç¼“è§£è¿‡æ‹Ÿåˆ")
else:
    print(f"  âœ… è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½")
    print(f"  âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¾ƒå¼º")

print(f"\nğŸ“Š å¯è§†åŒ–è¾“å‡º:")
print(f"  01_training_process.png - è®­ç»ƒè¿‡ç¨‹æ›²çº¿")
print(f"  02_performance_ranking.png - æ€§èƒ½æ’åå¯¹æ¯”")
print(f"  03_top6_strategies_comparison.png - Top6ç­–ç•¥é¢„æµ‹")
print(f"  04_residual_analysis.png - æ®‹å·®åˆ†å¸ƒåˆ†æ")
print(f"  05_residual_timeseries.png - æ®‹å·®æ—¶é—´åºåˆ—")
print(f"  06_key_strategies_comprehensive.png - ç»¼åˆå¯¹æ¯”")
print(f"  07_metrics_comparison.png - å¤šæŒ‡æ ‡å¯¹æ¯”")
print(f"  08_prediction_error_analysis.png - é¢„æµ‹è¯¯å·®åˆ†æ")
print(f"  09_performance_radar.png - æ€§èƒ½é›·è¾¾å›¾")
print(f"  10_residual_boxplot.png - æ®‹å·®ç®±çº¿å›¾")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_directory}")
print("=" * 100)

# ===== æ–°å¢ï¼šDMæ£€éªŒåˆ†æ =====
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ4ã€‘Diebold-Marianoç»Ÿè®¡æ£€éªŒåˆ†æ".center(100))
print("=" * 100)

# å¯¼å…¥æ¨¡å—
from dm_test import quick_dm_analysis, pairwise_dm_analysis

# å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹å°ºåº¦çš„é¢„æµ‹ç»“æœï¼‰
all_predictions = {
    'LSTMå•æ¨¡å‹': strategies_original['LSTMå•æ¨¡å‹'],
    'GRUå•æ¨¡å‹': strategies_original['GRUå•æ¨¡å‹'],
    'ç®€å•å¹³å‡(åŸºçº¿)': strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'],
    **{k: v for k, v in strategies_original.items()
       if k.startswith('ç­–ç•¥')}
}

# 1. åŸºå‡†å¯¹æ¯”åˆ†æ
print("\nç¬¬1éƒ¨åˆ†: æ‰€æœ‰æ¨¡å‹ vs åŸºå‡†æ¨¡å‹")
print("-" * 100)

dm_results = quick_dm_analysis(
    y_true=y_test_original,
    predictions=all_predictions,
    baseline='ç®€å•å¹³å‡(åŸºçº¿)',
    save_dir=results_directory,
    plot=True,
    verbose=True
)