import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge  # Ridgeå›å½’
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - å¢å¼ºç‰ˆ".center(100))
print("æ–°å¢ï¼šRidgeæ®‹å·®å­¦ä¹  + ä¿å®ˆXGBoostå‚æ•° + ç®€åŒ–ç‰¹å¾".center(100))
print("=" * 100)

# ========== åŠ è½½æ•°æ® ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ ==========
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# æ·»åŠ æ»åç‰¹å¾
def add_features(X, y, is_train=True):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train, is_train=True)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test, is_train=False)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# å®šä¹‰æ¨¡å‹
def build_simple_lstm(input_shape):
    return Sequential([
        layers.LSTM(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


def build_simple_gru(input_shape):
    return Sequential([
        layers.GRU(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


# OOFé¢„æµ‹ç”Ÿæˆ
def get_oof_predictions(X_seq, y_seq, model_type='lstm', n_splits=5):
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")
    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = build_simple_lstm((X_seq.shape[1], X_seq.shape[2]))
        else:
            model = build_simple_gru((X_seq.shape[1], X_seq.shape[2]))

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()
        fold += 1

    return oof_preds


print("\n" + "=" * 100)
print("ç¬¬ä¸€æ­¥ï¼šç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(100))
print("=" * 100)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF RÂ²: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF RÂ²: {r2_score(y_train_seq, gru_oof_preds):.4f}")

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
print("\n" + "=" * 100)
print("ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹".center(100))
print("=" * 100)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=80,
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    ),
    layers.Dropout(0.3),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(units=100, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

print(f"\nLSTMæµ‹è¯•é›†RÂ²: {r2_score(y_test_seq, lstm_test_pred):.4f}")
print(f"GRUæµ‹è¯•é›†RÂ²: {r2_score(y_test_seq, gru_test_pred):.4f}")


# ========== ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========
def create_enhanced_xgboost_features(X_flat, lstm_preds, gru_preds):
    """å¢å¼ºç‰¹å¾ï¼ˆ80ç»´ï¼‰"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append((lstm_preds + gru_preds).reshape(-1, 1))
    features_list.append((lstm_preds - gru_preds).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    features_list.append((lstm_preds * gru_preds).reshape(-1, 1))
    features_list.append(np.maximum(lstm_preds, gru_preds).reshape(-1, 1))
    features_list.append(np.minimum(lstm_preds, gru_preds).reshape(-1, 1))
    disagreement = np.abs(lstm_preds - gru_preds)
    confidence = 1 / (1 + disagreement)
    features_list.append(confidence.reshape(-1, 1))
    weighted_avg = 0.5 * lstm_preds + 0.5 * gru_preds
    features_list.append(weighted_avg.reshape(-1, 1))
    return np.hstack(features_list)


def create_simplified_features(X_flat, lstm_preds, gru_preds):
    """ç®€åŒ–ç‰¹å¾ï¼ˆ74ç»´ï¼‰- åªä¿ç•™4ä¸ªæœ€å…³é”®çš„å¢å¼ºç‰¹å¾"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


print("\n" + "=" * 100)
print("æ„é€ ç‰¹å¾é›†".center(100))
print("=" * 100)

# æ„é€ å¢å¼ºç‰¹å¾
enhanced_train_features = create_enhanced_xgboost_features(
    X_train_flat[:len(lstm_oof_preds)],
    lstm_oof_preds,
    gru_oof_preds
)

enhanced_test_features = create_enhanced_xgboost_features(
    X_test_flat,
    lstm_test_pred,
    gru_test_pred
)

# æ„é€ ç®€åŒ–ç‰¹å¾
simplified_train_features = create_simplified_features(
    X_train_flat[:len(lstm_oof_preds)],
    lstm_oof_preds,
    gru_oof_preds
)

simplified_test_features = create_simplified_features(
    X_test_flat,
    lstm_test_pred,
    gru_test_pred
)

print(f"\nâœ“ ç‰¹å¾ç»´åº¦:")
print(f"  åŸå§‹ç‰¹å¾: {X_train_flat.shape[1]} ç»´")
print(f"  å¢å¼ºç‰¹å¾: {enhanced_train_features.shape[1]} ç»´")
print(f"  ç®€åŒ–ç‰¹å¾: {simplified_train_features.shape[1]} ç»´")


# ========== å®šä¹‰è®­ç»ƒå‡½æ•° ==========
def train_conservative_xgboost(X_train, y_train):
    """ä¿å®ˆXGBoostå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰"""
    model = XGBRegressor(
        n_estimators=100,  # 500 â†’ 100
        learning_rate=0.01,  # 0.03 â†’ 0.01
        max_depth=3,  # 4 â†’ 3
        min_child_weight=5,  # æ–°å¢
        subsample=0.7,  # 0.8 â†’ 0.7
        colsample_bytree=0.7,  # 0.8 â†’ 0.7
        reg_alpha=0.1,  # æ–°å¢ L1
        reg_lambda=1.0,  # æ–°å¢ L2
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_ridge_model(X_train, y_train, alpha=10.0):
    """Ridgeçº¿æ€§å›å½’ï¼ˆæœ€ä¿å®ˆï¼Œå‚è€ƒå®éªŒ8ï¼‰"""
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    return model


# ========== ç­–ç•¥1ï¼šç®€å•å¹³å‡èåˆ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥1ã€‘ç®€å•å¹³å‡èåˆï¼š(LSTM + GRU) / 2".center(100))
print("=" * 100)

avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"ç®€å•å¹³å‡æµ‹è¯•é›†RÂ²: {avg_r2:.4f}")

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# ========== ç­–ç•¥2ï¼šLSTMæ®‹å·®å­¦ä¹ ï¼ˆä¿å®ˆXGBoost + å¢å¼ºç‰¹å¾ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥2ã€‘LSTMæ®‹å·®å­¦ä¹ ï¼šLSTMåŸºç¡€ + ä¿å®ˆXGBoostæ®‹å·®ä¿®æ­£ã€å¢å¼ºç‰¹å¾ã€‘".center(100))
print("=" * 100)

xgb_lstm_residual_model = train_conservative_xgboost(enhanced_train_features, lstm_oof_residual)
lstm_residual_pred = xgb_lstm_residual_model.predict(enhanced_test_features)
lstm_residual_strategy_pred = lstm_test_pred + lstm_residual_pred

lstm_residual_r2 = r2_score(y_test_seq, lstm_residual_strategy_pred)
print(f"âœ“ LSTMæ®‹å·®å­¦ä¹ ç­–ç•¥æµ‹è¯•é›†RÂ²: {lstm_residual_r2:.4f}")
print(f"  æ”¹è¿›: {lstm_residual_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")

# ========== ç­–ç•¥3ï¼šGRUæ®‹å·®å­¦ä¹ ï¼ˆä¿å®ˆXGBoost + å¢å¼ºç‰¹å¾ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥3ã€‘GRUæ®‹å·®å­¦ä¹ ï¼šGRUåŸºç¡€ + ä¿å®ˆXGBoostæ®‹å·®ä¿®æ­£ã€å¢å¼ºç‰¹å¾ã€‘".center(100))
print("=" * 100)

xgb_gru_residual_model = train_conservative_xgboost(enhanced_train_features, gru_oof_residual)
gru_residual_pred = xgb_gru_residual_model.predict(enhanced_test_features)
gru_residual_strategy_pred = gru_test_pred + gru_residual_pred

gru_residual_r2 = r2_score(y_test_seq, gru_residual_strategy_pred)
print(f"âœ“ GRUæ®‹å·®å­¦ä¹ ç­–ç•¥æµ‹è¯•é›†RÂ²: {gru_residual_r2:.4f}")
print(f"  æ”¹è¿›: {gru_residual_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")

# ========== ç­–ç•¥4ï¼šåŠ æƒå¹³å‡Stackingï¼ˆåŸºäºéªŒè¯é›†è¡¨ç°ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥4ã€‘åŠ æƒå¹³å‡Stackingï¼šåŸºäºOOFè¡¨ç°å­¦ä¹ æƒé‡".center(100))
print("=" * 100)

# è®¡ç®—LSTMå’ŒGRUåœ¨OOFä¸Šçš„RÂ²
lstm_oof_r2 = r2_score(y_train_seq, lstm_oof_preds)
gru_oof_r2 = r2_score(y_train_seq, gru_oof_preds)

# åŸºäºRÂ²è®¡ç®—æƒé‡ï¼ˆè¡¨ç°å¥½çš„æƒé‡æ›´é«˜ï¼‰
total_r2 = lstm_oof_r2 + gru_oof_r2
lstm_weight_r2 = lstm_oof_r2 / total_r2
gru_weight_r2 = gru_oof_r2 / total_r2

print(f"\nåŸºäºOOF RÂ²çš„æƒé‡:")
print(f"  LSTM OOF RÂ²: {lstm_oof_r2:.4f} â†’ æƒé‡: {lstm_weight_r2:.4f}")
print(f"  GRU OOF RÂ²:  {gru_oof_r2:.4f} â†’ æƒé‡: {gru_weight_r2:.4f}")

weighted_stacking_pred = lstm_weight_r2 * lstm_test_pred + gru_weight_r2 * gru_test_pred
weighted_stacking_r2 = r2_score(y_test_seq, weighted_stacking_pred)

print(f"\nâœ“ åŠ æƒå¹³å‡Stackingæµ‹è¯•é›†RÂ²: {weighted_stacking_r2:.4f}")
print(f"  æ”¹è¿›: {weighted_stacking_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")
print(f"  ã€è¯´æ˜ã€‘æ ¹æ®è®­ç»ƒé›†è¡¨ç°è‡ªåŠ¨åˆ†é…æƒé‡ï¼Œæ— éœ€é¢å¤–è®­ç»ƒ")

# ========== ç­–ç•¥5ï¼šåŒæ®‹å·®å­¦ä¹ ï¼ˆä¿å®ˆXGBoost + å¢å¼ºç‰¹å¾ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥5ã€‘åŒæ®‹å·®å­¦ä¹ ï¼š(LSTM+GRU)/2åŸºç¡€ + ä¿å®ˆXGBoostæ®‹å·®ã€å¢å¼ºç‰¹å¾ã€‘".center(100))
print("=" * 100)

avg_oof_preds = (lstm_oof_preds + gru_oof_preds) / 2
avg_oof_residual = y_train_seq - avg_oof_preds

xgb_dual_model = train_conservative_xgboost(enhanced_train_features, avg_oof_residual)
avg_test_pred_for_residual = (lstm_test_pred + gru_test_pred) / 2
dual_residual_pred = xgb_dual_model.predict(enhanced_test_features)
dual_strategy_pred = avg_test_pred_for_residual + dual_residual_pred

dual_r2 = r2_score(y_test_seq, dual_strategy_pred)
print(f"âœ“ åŒæ®‹å·®å­¦ä¹ ç­–ç•¥æµ‹è¯•é›†RÂ²: {dual_r2:.4f}")
print(f"  æ”¹è¿›: {dual_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")

# ========== ç­–ç•¥6ï¼šåŠ¨æ€æƒé‡æ®‹å·®èåˆï¼ˆä¿å®ˆXGBoost + ç®€åŒ–ç‰¹å¾ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥6ã€‘åŠ¨æ€æƒé‡æ®‹å·®èåˆï¼šRidgeå›å½’å­¦ä¹ æœ€ä¼˜æƒé‡ã€ç®€åŒ–ç‰¹å¾ã€‘".center(100))
print("=" * 100)

print("\nç¬¬ä¸€æ­¥ï¼šè®­ç»ƒLSTMæ®‹å·®ä¿®æ­£æ¨¡å‹ï¼ˆä¿å®ˆXGBoostï¼‰...")
xgb_lstm_dynamic = train_conservative_xgboost(simplified_train_features, lstm_oof_residual)
lstm_res_train = xgb_lstm_dynamic.predict(simplified_train_features)
lstm_res_test = xgb_lstm_dynamic.predict(simplified_test_features)

print("\nç¬¬äºŒæ­¥ï¼šè®­ç»ƒGRUæ®‹å·®ä¿®æ­£æ¨¡å‹ï¼ˆä¿å®ˆXGBoostï¼‰...")
xgb_gru_dynamic = train_conservative_xgboost(simplified_train_features, gru_oof_residual)
gru_res_train = xgb_gru_dynamic.predict(simplified_train_features)
gru_res_test = xgb_gru_dynamic.predict(simplified_test_features)

print("\nç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨Ridgeå›å½’å­¦ä¹ æœ€ä¼˜èåˆæƒé‡...")
meta_features_train = np.column_stack([
    lstm_oof_preds + lstm_res_train,
    gru_oof_preds + gru_res_train
])

meta_features_test = np.column_stack([
    lstm_test_pred + lstm_res_test,
    gru_test_pred + gru_res_test
])

meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_features_train, y_train_seq)

lstm_weight = meta_model.coef_[0]
gru_weight = meta_model.coef_[1]
intercept = meta_model.intercept_

print(f"\nâœ“ å­¦ä¹ åˆ°çš„åŠ¨æ€æƒé‡:")
print(f"  LSTMæƒé‡: {lstm_weight:.4f}")
print(f"  GRUæƒé‡:  {gru_weight:.4f}")
print(f"  æˆªè·é¡¹:   {intercept:.4f}")

dynamic_weight_pred = meta_model.predict(meta_features_test)

dynamic_weight_r2 = r2_score(y_test_seq, dynamic_weight_pred)
print(f"\nâœ“ åŠ¨æ€æƒé‡æ®‹å·®èåˆæµ‹è¯•é›†RÂ²: {dynamic_weight_r2:.4f}")
print(f"  æ”¹è¿›: {dynamic_weight_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")

# ========== ã€æ–°å¢ã€‘ç­–ç•¥7ï¼šLSTM+GRU Ridgeæ®‹å·®å­¦ä¹ ï¼ˆå‚è€ƒå®éªŒ8ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥7ã€‘LSTM Ridgeæ®‹å·®å­¦ä¹ ï¼šLSTMåŸºç¡€ + Ridgeæ®‹å·®ä¿®æ­£ã€ç®€åŒ–ç‰¹å¾, alpha=10ã€‘".center(100))
print("=" * 100)

print("\nä½¿ç”¨Ridgeå›å½’å­¦ä¹ LSTMæ®‹å·®ï¼ˆalpha=10ï¼‰...")
ridge_lstm_model = train_ridge_model(simplified_train_features, lstm_oof_residual, alpha=10.0)
ridge_lstm_residual_pred = ridge_lstm_model.predict(simplified_test_features)
ridge_lstm_strategy_pred = lstm_test_pred + ridge_lstm_residual_pred

ridge_lstm_r2 = r2_score(y_test_seq, ridge_lstm_strategy_pred)
print(f"âœ“ LSTM Ridgeæ®‹å·®å­¦ä¹ æµ‹è¯•é›†RÂ²: {ridge_lstm_r2:.4f}")
print(f"  æ”¹è¿›: {ridge_lstm_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")
print(f"  vs XGBoostæ®‹å·®: {ridge_lstm_r2 - lstm_residual_r2:.4f}")

# ========== ã€æ–°å¢ã€‘ç­–ç•¥8ï¼šGRU Ridgeæ®‹å·®å­¦ä¹ ï¼ˆå‚è€ƒå®éªŒ8ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥8ã€‘GRU Ridgeæ®‹å·®å­¦ä¹ ï¼šGRUåŸºç¡€ + Ridgeæ®‹å·®ä¿®æ­£ã€ç®€åŒ–ç‰¹å¾, alpha=10ã€‘".center(100))
print("=" * 100)

print("\nä½¿ç”¨Ridgeå›å½’å­¦ä¹ GRUæ®‹å·®ï¼ˆalpha=10ï¼‰...")
ridge_gru_model = train_ridge_model(simplified_train_features, gru_oof_residual, alpha=10.0)
ridge_gru_residual_pred = ridge_gru_model.predict(simplified_test_features)
ridge_gru_strategy_pred = gru_test_pred + ridge_gru_residual_pred

ridge_gru_r2 = r2_score(y_test_seq, ridge_gru_strategy_pred)
print(f"âœ“ GRU Ridgeæ®‹å·®å­¦ä¹ æµ‹è¯•é›†RÂ²: {ridge_gru_r2:.4f}")
print(f"  æ”¹è¿›: {ridge_gru_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")
print(f"  vs XGBoostæ®‹å·®: {ridge_gru_r2 - gru_residual_r2:.4f}")

# ========== ã€æ–°å¢ã€‘ç­–ç•¥9ï¼šåŒæ®‹å·®Ridgeå­¦ä¹ ï¼ˆå‚è€ƒå®éªŒ8ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥9ã€‘åŒæ®‹å·®Ridgeå­¦ä¹ ï¼š(LSTM+GRU)/2åŸºç¡€ + Ridgeæ®‹å·®ã€ç®€åŒ–ç‰¹å¾, alpha=10ã€‘".center(100))
print("=" * 100)

print("\nä½¿ç”¨Ridgeå›å½’å­¦ä¹ å¹³å‡æ®‹å·®ï¼ˆalpha=10ï¼‰...")
ridge_dual_model = train_ridge_model(simplified_train_features, avg_oof_residual, alpha=10.0)
ridge_dual_residual_pred = ridge_dual_model.predict(simplified_test_features)
ridge_dual_strategy_pred = avg_test_pred_for_residual + ridge_dual_residual_pred

ridge_dual_r2 = r2_score(y_test_seq, ridge_dual_strategy_pred)
print(f"âœ“ åŒæ®‹å·®Ridgeå­¦ä¹ æµ‹è¯•é›†RÂ²: {ridge_dual_r2:.4f}")
print(f"  æ”¹è¿›: {ridge_dual_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")
print(f"  vs XGBoostæ®‹å·®: {ridge_dual_r2 - dual_r2:.4f}")

# ========== ã€æ–°å¢ã€‘ç­–ç•¥10ï¼šRidge Stackingï¼ˆæœ€ç®€å•çš„èåˆï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥10ã€‘Ridge Stackingï¼šRidgeç›´æ¥èåˆLSTMå’ŒGRUé¢„æµ‹ã€alpha=1.0ã€‘".center(100))
print("=" * 100)

# æœ€ç®€å•çš„å…ƒç‰¹å¾ï¼šåªç”¨LSTMå’ŒGRUçš„é¢„æµ‹
ridge_stacking_train = np.column_stack([lstm_oof_preds, gru_oof_preds])
ridge_stacking_test = np.column_stack([lstm_test_pred, gru_test_pred])

print(f"Ridge Stackingå…ƒç‰¹å¾ç»´åº¦: {ridge_stacking_train.shape[1]} ç»´")

ridge_stacking_model = Ridge(alpha=1.0)
ridge_stacking_model.fit(ridge_stacking_train, y_train_seq)

ridge_stacking_pred = ridge_stacking_model.predict(ridge_stacking_test)
ridge_stacking_r2 = r2_score(y_test_seq, ridge_stacking_pred)

lstm_stacking_weight = ridge_stacking_model.coef_[0]
gru_stacking_weight = ridge_stacking_model.coef_[1]
stacking_intercept = ridge_stacking_model.intercept_

print(f"\nâœ“ å­¦ä¹ åˆ°çš„Stackingæƒé‡:")
print(f"  LSTMæƒé‡: {lstm_stacking_weight:.4f}")
print(f"  GRUæƒé‡:  {gru_stacking_weight:.4f}")
print(f"  æˆªè·é¡¹:   {stacking_intercept:.4f}")
print(f"  æƒé‡å’Œ:   {lstm_stacking_weight + gru_stacking_weight:.4f}")

print(f"\nâœ“ Ridge Stackingæµ‹è¯•é›†RÂ²: {ridge_stacking_r2:.4f}")
print(f"  æ”¹è¿›: {ridge_stacking_r2 - avg_r2:.4f} vs ç®€å•å¹³å‡")

# ========== æ€§èƒ½å¯¹æ¯” ==========
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

strategies = {
    'LSTMå•æ¨¡å‹': lstm_test_pred,
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç­–ç•¥1-ç®€å•å¹³å‡': avg_test_pred,
    'ç­–ç•¥2-LSTMæ®‹å·®(XGB)': lstm_residual_strategy_pred,
    'ç­–ç•¥3-GRUæ®‹å·®(XGB)': gru_residual_strategy_pred,
    'ç­–ç•¥4-åŠ æƒå¹³å‡Stacking': weighted_stacking_pred,
    'ç­–ç•¥5-åŒæ®‹å·®(XGB)': dual_strategy_pred,
    'ç­–ç•¥6-åŠ¨æ€æƒé‡èåˆ': dynamic_weight_pred,
    'ç­–ç•¥7-LSTMæ®‹å·®(Ridge)': ridge_lstm_strategy_pred,
    'ç­–ç•¥8-GRUæ®‹å·®(Ridge)': ridge_gru_strategy_pred,
    'ç­–ç•¥9-åŒæ®‹å·®(Ridge)': ridge_dual_strategy_pred,
    'ç­–ç•¥10-Ridge Stacking': ridge_stacking_pred,
}

print("\n{:<30} {:<12} {:<12} {:<12} {:<12}".format("ç­–ç•¥", "RÂ²", "MAE", "RMSE", "MAPE"))
print("-" * 80)

for name, pred in strategies.items():
    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    mape = np.mean(np.abs((pred - y_test_seq) / (y_test_seq + 1e-8)))
    print(f"{name:<30} {r2:<12.4f} {mae:<12.6f} {rmse:<12.6f} {mape:<12.6f}")

# ========== åå½’ä¸€åŒ–å¹¶è¯„ä¼°åŸå§‹å°ºåº¦ ==========
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆåŸå§‹å°ºåº¦ï¼‰".center(100))
print("=" * 100)

strategies_original = {}
y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))

for name, pred in strategies.items():
    strategies_original[name] = y_scaler.inverse_transform(pred.reshape(-1, 1))

print("\n{:<30} {:<12} {:<12} {:<12} {:<12}".format("ç­–ç•¥", "RÂ²", "MAE", "RMSE", "MAPE"))
print("-" * 80)

best_r2 = -np.inf
best_strategy = None

for name, pred in strategies_original.items():
    r2 = r2_score(y_test_original, pred)
    mae = mean_absolute_error(y_test_original, pred)
    rmse = sqrt(mean_squared_error(y_test_original, pred))
    mape = np.mean(np.abs((pred - y_test_original) / (y_test_original + 1e-8)))
    print(f"{name:<30} {r2:<12.4f} {mae:<12.4f} {rmse:<12.4f} {mape:<12.6f}")

    if r2 > best_r2:
        best_r2 = r2
        best_strategy = name

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy} (RÂ² = {best_r2:.4f})")

# ========== XGBoost vs Ridge æ®‹å·®å­¦ä¹ å¯¹æ¯”åˆ†æ ==========
print("\n" + "=" * 100)
print("XGBoost vs Ridge æ®‹å·®å­¦ä¹ æ•ˆæœå¯¹æ¯”".center(100))
print("=" * 100)

comparison_data = {
    'LSTMæ®‹å·®': {
        'XGBoost': lstm_residual_r2,
        'Ridge': ridge_lstm_r2,
        'diff': ridge_lstm_r2 - lstm_residual_r2
    },
    'GRUæ®‹å·®': {
        'XGBoost': gru_residual_r2,
        'Ridge': ridge_gru_r2,
        'diff': ridge_gru_r2 - gru_residual_r2
    },
    'åŒæ®‹å·®': {
        'XGBoost': dual_r2,
        'Ridge': ridge_dual_r2,
        'diff': ridge_dual_r2 - dual_r2
    }
}

print("\n{:<15} {:<15} {:<15} {:<15}".format("æ®‹å·®ç±»å‹", "XGBoost RÂ²", "Ridge RÂ²", "å·®å¼‚"))
print("-" * 60)
for residual_type, scores in comparison_data.items():
    diff_sign = "+" if scores['diff'] > 0 else ""
    print(f"{residual_type:<15} {scores['XGBoost']:<15.4f} {scores['Ridge']:<15.4f} {diff_sign}{scores['diff']:<15.4f}")

avg_diff = np.mean([v['diff'] for v in comparison_data.values()])
print(f"\nå¹³å‡å·®å¼‚: {avg_diff:+.4f}")
if avg_diff > 0:
    print("âœ“ Ridgeå›å½’åœ¨æ®‹å·®å­¦ä¹ ä¸Šå¹³å‡è¡¨ç°æ›´å¥½ï¼ˆæ›´ç¨³å®šï¼‰")
else:
    print("âœ“ XGBooståœ¨æ®‹å·®å­¦ä¹ ä¸Šå¹³å‡è¡¨ç°æ›´å¥½ï¼ˆæ›´çµæ´»ï¼‰")

# ========== å¯è§†åŒ– ==========
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

# 1. æ‰€æœ‰ç­–ç•¥é¢„æµ‹å¯¹æ¯”ï¼ˆ6x2å¸ƒå±€ï¼‰
fig = plt.figure(figsize=(20, 24))

plot_data = [
    ('LSTMå•æ¨¡å‹', strategies_original['LSTMå•æ¨¡å‹'], 'blue'),
    ('GRUå•æ¨¡å‹', strategies_original['GRUå•æ¨¡å‹'], 'green'),
    ('ç­–ç•¥1-ç®€å•å¹³å‡', strategies_original['ç­–ç•¥1-ç®€å•å¹³å‡'], 'purple'),
    ('ç­–ç•¥2-LSTMæ®‹å·®(XGB)', strategies_original['ç­–ç•¥2-LSTMæ®‹å·®(XGB)'], 'orange'),
    ('ç­–ç•¥3-GRUæ®‹å·®(XGB)', strategies_original['ç­–ç•¥3-GRUæ®‹å·®(XGB)'], 'cyan'),
    ('ç­–ç•¥4-åŠ æƒå¹³å‡Stacking', strategies_original['ç­–ç•¥4-åŠ æƒå¹³å‡Stacking'], 'red'),
    ('ç­–ç•¥5-åŒæ®‹å·®(XGB)', strategies_original['ç­–ç•¥5-åŒæ®‹å·®(XGB)'], 'brown'),
    ('ç­–ç•¥6-åŠ¨æ€æƒé‡èåˆ', strategies_original['ç­–ç•¥6-åŠ¨æ€æƒé‡èåˆ'], 'magenta'),
    ('ç­–ç•¥7-LSTMæ®‹å·®(Ridge)', strategies_original['ç­–ç•¥7-LSTMæ®‹å·®(Ridge)'], 'coral'),
    ('ç­–ç•¥8-GRUæ®‹å·®(Ridge)', strategies_original['ç­–ç•¥8-GRUæ®‹å·®(Ridge)'], 'teal'),
    ('ç­–ç•¥9-åŒæ®‹å·®(Ridge)', strategies_original['ç­–ç•¥9-åŒæ®‹å·®(Ridge)'], 'gold'),
    ('ç­–ç•¥10-Ridge Stacking', strategies_original['ç­–ç•¥10-Ridge Stacking'], 'lime'),
]

for idx, (name, pred, color) in enumerate(plot_data, 1):
    plt.subplot(6, 2, idx)
    plt.plot(y_test_original, label="çœŸå®å€¼", linewidth=2.5, color='black', alpha=0.7)
    plt.plot(pred, label=name, linewidth=2, alpha=0.8, color=color)
    r2 = r2_score(y_test_original, pred)
    plt.title(f"{name} (RÂ²={r2:.4f})", fontsize=12, fontweight='bold')
    plt.xlabel('æ ·æœ¬åºå·', fontsize=9)
    plt.ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    plt.legend(fontsize=8)
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'all_strategies_with_ridge.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 2. XGBoost vs Ridge æ®‹å·®å­¦ä¹ å¯¹æ¯”
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 2.1 LSTMæ®‹å·®å¯¹æ¯”
axes[0, 0].plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
axes[0, 0].plot(strategies_original['ç­–ç•¥2-LSTMæ®‹å·®(XGB)'],
                label='XGBoostæ®‹å·®', linewidth=2, alpha=0.8, color='orange')
axes[0, 0].plot(strategies_original['ç­–ç•¥7-LSTMæ®‹å·®(Ridge)'],
                label='Ridgeæ®‹å·®', linewidth=2, alpha=0.8, color='coral')
r2_xgb = r2_score(y_test_original, strategies_original['ç­–ç•¥2-LSTMæ®‹å·®(XGB)'])
r2_ridge = r2_score(y_test_original, strategies_original['ç­–ç•¥7-LSTMæ®‹å·®(Ridge)'])
axes[0, 0].set_title(f'LSTMæ®‹å·®å­¦ä¹ å¯¹æ¯”\nXGB: {r2_xgb:.4f} | Ridge: {r2_ridge:.4f}',
                     fontsize=11, fontweight='bold')
axes[0, 0].set_xlabel('æ ·æœ¬åºå·')
axes[0, 0].set_ylabel('ç‰ç±³ä»·æ ¼')
axes[0, 0].legend(fontsize=8)
axes[0, 0].grid(True, alpha=0.3)

# 2.2 GRUæ®‹å·®å¯¹æ¯”
axes[0, 1].plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
axes[0, 1].plot(strategies_original['ç­–ç•¥3-GRUæ®‹å·®(XGB)'],
                label='XGBoostæ®‹å·®', linewidth=2, alpha=0.8, color='cyan')
axes[0, 1].plot(strategies_original['ç­–ç•¥8-GRUæ®‹å·®(Ridge)'],
                label='Ridgeæ®‹å·®', linewidth=2, alpha=0.8, color='teal')
r2_xgb = r2_score(y_test_original, strategies_original['ç­–ç•¥3-GRUæ®‹å·®(XGB)'])
r2_ridge = r2_score(y_test_original, strategies_original['ç­–ç•¥8-GRUæ®‹å·®(Ridge)'])
axes[0, 1].set_title(f'GRUæ®‹å·®å­¦ä¹ å¯¹æ¯”\nXGB: {r2_xgb:.4f} | Ridge: {r2_ridge:.4f}',
                     fontsize=11, fontweight='bold')
axes[0, 1].set_xlabel('æ ·æœ¬åºå·')
axes[0, 1].set_ylabel('ç‰ç±³ä»·æ ¼')
axes[0, 1].legend(fontsize=8)
axes[0, 1].grid(True, alpha=0.3)

# 2.3 åŒæ®‹å·®å¯¹æ¯”
axes[0, 2].plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
axes[0, 2].plot(strategies_original['ç­–ç•¥5-åŒæ®‹å·®(XGB)'],
                label='XGBoostæ®‹å·®', linewidth=2, alpha=0.8, color='brown')
axes[0, 2].plot(strategies_original['ç­–ç•¥9-åŒæ®‹å·®(Ridge)'],
                label='Ridgeæ®‹å·®', linewidth=2, alpha=0.8, color='gold')
r2_xgb = r2_score(y_test_original, strategies_original['ç­–ç•¥5-åŒæ®‹å·®(XGB)'])
r2_ridge = r2_score(y_test_original, strategies_original['ç­–ç•¥9-åŒæ®‹å·®(Ridge)'])
axes[0, 2].set_title(f'åŒæ®‹å·®å­¦ä¹ å¯¹æ¯”\nXGB: {r2_xgb:.4f} | Ridge: {r2_ridge:.4f}',
                     fontsize=11, fontweight='bold')
axes[0, 2].set_xlabel('æ ·æœ¬åºå·')
axes[0, 2].set_ylabel('ç‰ç±³ä»·æ ¼')
axes[0, 2].legend(fontsize=8)
axes[0, 2].grid(True, alpha=0.3)

# 2.4 æ®‹å·®åˆ†å¸ƒå¯¹æ¯” - LSTM
residual_xgb_lstm = y_test_seq - lstm_residual_strategy_pred
residual_ridge_lstm = y_test_seq - ridge_lstm_strategy_pred
axes[1, 0].hist(residual_xgb_lstm, bins=30, color='orange', alpha=0.6,
                edgecolor='black', label='XGBoost')
axes[1, 0].hist(residual_ridge_lstm, bins=30, color='coral', alpha=0.6,
                edgecolor='black', label='Ridge')
axes[1, 0].axvline(0, color='black', linestyle='--', linewidth=2)
axes[1, 0].set_title(f'LSTMæ®‹å·®åˆ†å¸ƒ\nXGB std={np.std(residual_xgb_lstm):.5f} | Ridge std={np.std(residual_ridge_lstm):.5f}',
                     fontsize=10, fontweight='bold')
axes[1, 0].set_xlabel('æ®‹å·®')
axes[1, 0].set_ylabel('é¢‘æ•°')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3, axis='y')

# 2.5 æ®‹å·®åˆ†å¸ƒå¯¹æ¯” - GRU
residual_xgb_gru = y_test_seq - gru_residual_strategy_pred
residual_ridge_gru = y_test_seq - ridge_gru_strategy_pred
axes[1, 1].hist(residual_xgb_gru, bins=30, color='cyan', alpha=0.6,
                edgecolor='black', label='XGBoost')
axes[1, 1].hist(residual_ridge_gru, bins=30, color='teal', alpha=0.6,
                edgecolor='black', label='Ridge')
axes[1, 1].axvline(0, color='black', linestyle='--', linewidth=2)
axes[1, 1].set_title(f'GRUæ®‹å·®åˆ†å¸ƒ\nXGB std={np.std(residual_xgb_gru):.5f} | Ridge std={np.std(residual_ridge_gru):.5f}',
                     fontsize=10, fontweight='bold')
axes[1, 1].set_xlabel('æ®‹å·®')
axes[1, 1].set_ylabel('é¢‘æ•°')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3, axis='y')

# 2.6 æ®‹å·®æ ‡å‡†å·®å¯¹æ¯”
residual_std_comparison = {
    'LSTM-XGB': np.std(residual_xgb_lstm),
    'LSTM-Ridge': np.std(residual_ridge_lstm),
    'GRU-XGB': np.std(residual_xgb_gru),
    'GRU-Ridge': np.std(residual_ridge_gru),
}
colors_std = ['orange', 'coral', 'cyan', 'teal']
axes[1, 2].bar(range(len(residual_std_comparison)),
               list(residual_std_comparison.values()),
               color=colors_std, alpha=0.7)
axes[1, 2].set_title('æ®‹å·®æ ‡å‡†å·®å¯¹æ¯”ï¼ˆè¶Šå°è¶Šå¥½ï¼‰', fontsize=11, fontweight='bold')
axes[1, 2].set_ylabel('æ ‡å‡†å·®')
axes[1, 2].set_xticks(range(len(residual_std_comparison)))
axes[1, 2].set_xticklabels(residual_std_comparison.keys(), rotation=15, ha='right')
axes[1, 2].grid(True, alpha=0.3, axis='y')

for i, (name, std) in enumerate(residual_std_comparison.items()):
    axes[1, 2].text(i, std + 0.001, f'{std:.5f}',
                    ha='center', va='bottom', fontweight='bold', fontsize=8)

plt.tight_layout()
plt.savefig(results_directory + 'xgboost_vs_ridge_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 3. æ€§èƒ½æ’åå›¾
fig, ax = plt.subplots(figsize=(14, 9))

improvement_data = {}
for name, pred in strategies_original.items():
    r2 = r2_score(y_test_original, pred)
    improvement_data[name] = r2

sorted_strategies = sorted(improvement_data.items(), key=lambda x: x[1], reverse=True)
names = [name for name, _ in sorted_strategies]
values = [value for _, value in sorted_strategies]

colors_bar = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(names)))
bars = ax.barh(range(len(names)), values, color=colors_bar, alpha=0.8)

# æ·»åŠ åŸºçº¿
baseline_r2 = improvement_data['ç­–ç•¥1-ç®€å•å¹³å‡']
ax.axvline(x=baseline_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, value) in enumerate(zip(bars, values)):
    improvement = value - baseline_r2
    label = f'{value:.4f}'
    if improvement > 0:
        label += f' (+{improvement:.4f})'
        color = 'green'
    elif improvement < 0:
        label += f' ({improvement:.4f})'
        color = 'red'
    else:
        color = 'black'

    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(names)))
ax.set_yticklabels(names, fontsize=9)
ax.set_xlabel('RÂ² Score', fontsize=12, fontweight='bold')
ax.set_title('æ‰€æœ‰ç­–ç•¥RÂ²æ€§èƒ½æ’åï¼ˆå«XGBoost + Ridgeæ®‹å·®å­¦ä¹ ï¼‰', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(results_directory + 'performance_ranking_with_ridge.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 4. RÂ²å¯¹æ¯”æŸ±çŠ¶å›¾ï¼ˆåˆ†ç»„ï¼‰
fig, ax = plt.subplots(figsize=(16, 8))

categories = ['LSTMæ®‹å·®', 'GRUæ®‹å·®', 'åŒæ®‹å·®']
xgb_scores = [
    r2_score(y_test_original, strategies_original['ç­–ç•¥2-LSTMæ®‹å·®(XGB)']),
    r2_score(y_test_original, strategies_original['ç­–ç•¥3-GRUæ®‹å·®(XGB)']),
    r2_score(y_test_original, strategies_original['ç­–ç•¥5-åŒæ®‹å·®(XGB)'])
]
ridge_scores = [
    r2_score(y_test_original, strategies_original['ç­–ç•¥7-LSTMæ®‹å·®(Ridge)']),
    r2_score(y_test_original, strategies_original['ç­–ç•¥8-GRUæ®‹å·®(Ridge)']),
    r2_score(y_test_original, strategies_original['ç­–ç•¥9-åŒæ®‹å·®(Ridge)'])
]

x_pos = np.arange(len(categories))
width = 0.35

bars1 = ax.bar(x_pos - width / 2, xgb_scores, width, label='ä¿å®ˆXGBoost', color='steelblue', alpha=0.8)
bars2 = ax.bar(x_pos + width / 2, ridge_scores, width, label='Ridge(alpha=10)', color='coral', alpha=0.8)

# æ·»åŠ åŸºçº¿
ax.axhline(y=baseline_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height + 0.002,
                f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)

ax.set_ylabel('RÂ² Score', fontsize=12, fontweight='bold')
ax.set_title('XGBoost vs Ridge æ®‹å·®å­¦ä¹ æ•ˆæœå¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xticks(x_pos)
ax.set_xticklabels(categories)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + 'xgb_vs_ridge_bar_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# ========== ä¿å­˜æ¨¡å‹ ==========
import pickle

print("\n" + "=" * 100)
print("ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_final_model.h5')
gru_final.save(results_directory + 'gru_final_model.h5')

# ä¿å­˜XGBoostæ¨¡å‹ï¼ˆä¿å®ˆå‚æ•°ç‰ˆæœ¬ï¼‰
with open(results_directory + 'xgb_lstm_residual_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_lstm_residual_model, f)

with open(results_directory + 'xgb_gru_residual_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_gru_residual_model, f)

with open(results_directory + 'xgb_dual_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_dual_model, f)

with open(results_directory + 'xgb_lstm_dynamic_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_lstm_dynamic, f)

with open(results_directory + 'xgb_gru_dynamic_conservative.pkl', 'wb') as f:
    pickle.dump(xgb_gru_dynamic, f)

# ä¿å­˜Ridgeå›å½’æ¨¡å‹
with open(results_directory + 'ridge_dynamic_meta_model.pkl', 'wb') as f:
    pickle.dump(meta_model, f)

with open(results_directory + 'ridge_lstm_residual_model.pkl', 'wb') as f:
    pickle.dump(ridge_lstm_model, f)

with open(results_directory + 'ridge_gru_residual_model.pkl', 'wb') as f:
    pickle.dump(ridge_gru_model, f)

with open(results_directory + 'ridge_dual_residual_model.pkl', 'wb') as f:
    pickle.dump(ridge_dual_model, f)

with open(results_directory + 'ridge_stacking_model.pkl', 'wb') as f:
    pickle.dump(ridge_stacking_model, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜åŠ æƒStackingæƒé‡ä¿¡æ¯
weighted_stacking_info = pd.DataFrame({
    'model': ['LSTM', 'GRU'],
    'oof_r2': [lstm_oof_r2, gru_oof_r2],
    'weight': [lstm_weight_r2, gru_weight_r2]
})
weighted_stacking_info.to_csv(results_directory + 'weighted_stacking_info.csv', index=False)

# ä¿å­˜é¢„æµ‹ç»“æœ
results_df = pd.DataFrame({
    'true_value': y_test_original.flatten(),
    'lstm': strategies_original['LSTMå•æ¨¡å‹'].flatten(),
    'gru': strategies_original['GRUå•æ¨¡å‹'].flatten(),
    'simple_avg': strategies_original['ç­–ç•¥1-ç®€å•å¹³å‡'].flatten(),
    'lstm_residual_xgb': strategies_original['ç­–ç•¥2-LSTMæ®‹å·®(XGB)'].flatten(),
    'gru_residual_xgb': strategies_original['ç­–ç•¥3-GRUæ®‹å·®(XGB)'].flatten(),
    'stacking_xgb': strategies_original['ç­–ç•¥4-åŠ æƒå¹³å‡Stacking'].flatten(),
    'dual_residual_xgb': strategies_original['ç­–ç•¥5-åŒæ®‹å·®(XGB)'].flatten(),
    'dynamic_weight': strategies_original['ç­–ç•¥6-åŠ¨æ€æƒé‡èåˆ'].flatten(),
    'lstm_residual_ridge': strategies_original['ç­–ç•¥7-LSTMæ®‹å·®(Ridge)'].flatten(),
    'gru_residual_ridge': strategies_original['ç­–ç•¥8-GRUæ®‹å·®(Ridge)'].flatten(),
    'dual_residual_ridge': strategies_original['ç­–ç•¥9-åŒæ®‹å·®(Ridge)'].flatten(),
})
results_df.to_csv(results_directory + 'all_predictions_xgb_ridge.csv', index=False)

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
metrics_data = {}
for name, pred in strategies_original.items():
    metrics_data[name] = {
        'RÂ²': r2_score(y_test_original, pred),
        'MAE': mean_absolute_error(y_test_original, pred),
        'RMSE': sqrt(mean_squared_error(y_test_original, pred)),
        'MAPE': np.mean(np.abs((pred - y_test_original) / (y_test_original + 1e-8)))
    }

performance_df = pd.DataFrame([
    {'strategy': name, **metrics}
    for name, metrics in metrics_data.items()
])
performance_df = performance_df.sort_values('RÂ²', ascending=False)
performance_df.to_csv(results_directory + 'performance_metrics_xgb_ridge.csv', index=False)

print("\nâœ“ æ¨¡å‹ä¿å­˜å®Œæˆï¼")
print(f"  - LSTM/GRUæœ€ç»ˆæ¨¡å‹")
print(f"  - ä¿å®ˆXGBoostæ¨¡å‹ x 5ä¸ª")
print(f"  - Ridgeå›å½’æ¨¡å‹ x 4ä¸ª (alpha=10)")
print(f"  - å½’ä¸€åŒ–å™¨")
print(f"  - æ‰€æœ‰é¢„æµ‹ç»“æœ")
print(f"  - æ€§èƒ½æŒ‡æ ‡")

# ========== æœ€ç»ˆæ€»ç»“ ==========
print("\n" + "=" * 100)
print("ğŸ‰ å¢å¼ºç‰ˆèåˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå«Ridgeæ®‹å·®å­¦ä¹ ï¼‰ï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š å®ç°çš„èåˆç­–ç•¥ï¼ˆå…±12ä¸ªï¼‰:")
print(f"  åŸºç¡€æ¨¡å‹:")
print(f"    - LSTMå•æ¨¡å‹")
print(f"    - GRUå•æ¨¡å‹")
print(f"  ç®€å•èåˆ:")
print(f"    1. ç®€å•å¹³å‡ï¼š(LSTM + GRU) / 2")
print(f"  XGBoostæ®‹å·®å­¦ä¹ ï¼ˆä¿å®ˆå‚æ•° + å¢å¼ºç‰¹å¾ï¼‰:")
print(f"    2. LSTMæ®‹å·®å­¦ä¹ ")
print(f"    3. GRUæ®‹å·®å­¦ä¹ ")
print(f"    4. åŠ æƒå¹³å‡Stackingï¼ˆåŸºäºOOFè¡¨ç°ï¼‰")
print(f"    5. åŒæ®‹å·®å­¦ä¹ ")
print(f"    6. åŠ¨æ€æƒé‡èåˆ")
print(f"  Ridgeæ®‹å·®å­¦ä¹ ï¼ˆalpha=10 + ç®€åŒ–ç‰¹å¾ï¼‰â­æ–°å¢:")
print(f"    7. LSTM Ridgeæ®‹å·®å­¦ä¹ ")
print(f"    8. GRU Ridgeæ®‹å·®å­¦ä¹ ")
print(f"    9. åŒæ®‹å·®Ridgeå­¦ä¹ ")
print(f"  Ridge Stackingï¼ˆalpha=1.0ï¼‰â­æ–°å¢:")
print(f"    10. Ridgeç›´æ¥èåˆLSTMå’ŒGRU")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy}")
print(f"   æµ‹è¯•é›†RÂ²: {best_r2:.4f}")

# æ€§èƒ½æ’å
print(f"\nğŸ“ˆ æ‰€æœ‰ç­–ç•¥æ€§èƒ½æ’åï¼ˆæŒ‰RÂ²é™åºï¼‰:")
sorted_r2_scores = sorted(
    [(name, r2_score(y_test_original, pred)) for name, pred in strategies_original.items()],
    key=lambda x: x[1], reverse=True
)

for rank, (name, r2) in enumerate(sorted_r2_scores, 1):
    improvement = r2 - baseline_r2
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"   {marker} {rank:>2}. {name:<30} RÂ² = {r2:.4f}  (vsç®€å•å¹³å‡: {improvement:+.4f})")

print(f"\nğŸ’¡ XGBoost vs Ridge æ®‹å·®å­¦ä¹ å¯¹æ¯”:")
print(f"  ã€LSTMæ®‹å·®ã€‘")
print(f"    XGBoost: {lstm_residual_r2:.4f}")
print(f"    Ridge:   {ridge_lstm_r2:.4f}")
print(f"    å·®å¼‚:    {ridge_lstm_r2 - lstm_residual_r2:+.4f}")
print(f"  ã€GRUæ®‹å·®ã€‘")
print(f"    XGBoost: {gru_residual_r2:.4f}")
print(f"    Ridge:   {ridge_gru_r2:.4f}")
print(f"    å·®å¼‚:    {ridge_gru_r2 - gru_residual_r2:+.4f}")
print(f"  ã€åŒæ®‹å·®ã€‘")
print(f"    XGBoost: {dual_r2:.4f}")
print(f"    Ridge:   {ridge_dual_r2:.4f}")
print(f"    å·®å¼‚:    {ridge_dual_r2 - dual_r2:+.4f}")

print(f"\nğŸ” å…³é”®å‘ç°:")
if avg_diff > 0:
    print(f"  âœ“ Ridgeå›å½’åœ¨æ®‹å·®å­¦ä¹ ä¸Šå¹³å‡è¡¨ç°æ›´å¥½ (å¹³å‡å·®å¼‚: {avg_diff:+.4f})")
    print(f"  âœ“ Ridgeä¼˜åŠ¿ï¼šæ›´ç¨³å®šã€æ›´ä¿å®ˆã€é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print(f"  âœ“ é€‚ç”¨åœºæ™¯ï¼šæ ·æœ¬é‡è¾ƒå°ã€åŸºç¡€æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆ")
else:
    print(f"  âœ“ XGBooståœ¨æ®‹å·®å­¦ä¹ ä¸Šå¹³å‡è¡¨ç°æ›´å¥½ (å¹³å‡å·®å¼‚: {avg_diff:+.4f})")
    print(f"  âœ“ XGBoostä¼˜åŠ¿ï¼šæ›´çµæ´»ã€æ•æ‰éçº¿æ€§æ®‹å·®æ¨¡å¼")
    print(f"  âœ“ é€‚ç”¨åœºæ™¯ï¼šæ ·æœ¬é‡å……è¶³ã€æ®‹å·®å­˜åœ¨å¤æ‚éçº¿æ€§å…³ç³»")

print(f"\nâ­ æŠ€æœ¯å‚æ•°:")
print(f"  ã€ä¿å®ˆXGBoostã€‘")
print(f"    n_estimators: 100, learning_rate: 0.01, max_depth: 3")
print(f"    min_child_weight: 5, subsample: 0.7, colsample_bytree: 0.7")
print(f"    reg_alpha: 0.1 (L1), reg_lambda: 1.0 (L2)")
print(f"  ã€Ridgeå›å½’ã€‘")
print(f"    alpha: 10.0 (å¼ºæ­£åˆ™åŒ–)")
print(f"    ç‰¹å¾: ç®€åŒ–ç‰¹å¾ï¼ˆ74ç»´ï¼‰")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {results_directory}")
print("=" * 100)