import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 120)
print("å¯¹ç§°æ®‹å·®å­¦ä¹ å®éªŒ - LSTM vs GRU å…¨é¢å¯¹æ¯”".center(120))
print("æ ¸å¿ƒæ”¹è¿›ï¼šä¸ºLSTMå’ŒGRUåˆ†åˆ«è¿›è¡Œ9ç§ç­–ç•¥å®éªŒï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åŸºç¡€æ¨¡å‹".center(120))
print("=" * 120)

# ========== åŠ è½½æ•°æ® ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ ==========
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# æ·»åŠ æ»åç‰¹å¾
def add_features(X, y):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# å®šä¹‰æ¨¡å‹
def build_simple_lstm(input_shape):
    return Sequential([
        layers.LSTM(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


def build_simple_gru(input_shape):
    return Sequential([
        layers.GRU(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


# OOFé¢„æµ‹ç”Ÿæˆ
def get_oof_predictions(X_seq, y_seq, model_type='lstm', n_splits=5):
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")
    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = build_simple_lstm((X_seq.shape[1], X_seq.shape[2]))
        else:
            model = build_simple_gru((X_seq.shape[1], X_seq.shape[2]))

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()
        fold += 1

    return oof_preds


print("\n" + "=" * 120)
print("ç¬¬ä¸€æ­¥ï¼šç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(120))
print("=" * 120)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF RÂ²: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF RÂ²: {r2_score(y_train_seq, gru_oof_preds):.4f}")

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
print("\n" + "=" * 120)
print("ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹".center(120))
print("=" * 120)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=80,
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    ),
    layers.Dropout(0.3),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(units=100, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

# è¯Šæ–­è¿‡æ‹Ÿåˆ
lstm_train_pred = lstm_final.predict(X_train_seq, verbose=0).flatten()
gru_train_pred = gru_final.predict(X_train_seq, verbose=0).flatten()

lstm_train_r2 = r2_score(y_train_seq, lstm_train_pred)
lstm_test_r2 = r2_score(y_test_seq, lstm_test_pred)
gru_train_r2 = r2_score(y_train_seq, gru_train_pred)
gru_test_r2 = r2_score(y_test_seq, gru_test_pred)

print(f"\nè¿‡æ‹Ÿåˆè¯Šæ–­:")
print(f"LSTM - è®­ç»ƒRÂ²: {lstm_train_r2:.4f}, æµ‹è¯•RÂ²: {lstm_test_r2:.4f}, å·®è·: {lstm_train_r2 - lstm_test_r2:.4f}")
print(f"GRU  - è®­ç»ƒRÂ²: {gru_train_r2:.4f}, æµ‹è¯•RÂ²: {gru_test_r2:.4f}, å·®è·: {gru_train_r2 - gru_test_r2:.4f}")

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")


# ========== ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========

def create_original_features(X_flat, pred1, pred2):
    """åŸå§‹å¢å¼ºç‰¹å¾ï¼ˆ80ç»´ï¼‰"""
    features_list = [X_flat]
    features_list.append(pred1.reshape(-1, 1))
    features_list.append(pred2.reshape(-1, 1))
    features_list.append((pred1 + pred2).reshape(-1, 1))
    features_list.append((pred1 - pred2).reshape(-1, 1))
    features_list.append(np.abs(pred1 - pred2).reshape(-1, 1))
    features_list.append((pred1 * pred2).reshape(-1, 1))
    features_list.append(np.maximum(pred1, pred2).reshape(-1, 1))
    features_list.append(np.minimum(pred1, pred2).reshape(-1, 1))
    disagreement = np.abs(pred1 - pred2)
    confidence = 1 / (1 + disagreement)
    features_list.append(confidence.reshape(-1, 1))
    weighted_avg = 0.5 * pred1 + 0.5 * pred2
    features_list.append(weighted_avg.reshape(-1, 1))
    return np.hstack(features_list)


def create_simplified_features(X_flat, pred1, pred2):
    """ç®€åŒ–ç‰¹å¾ï¼ˆ74ç»´ï¼‰"""
    features_list = [X_flat]
    features_list.append(pred1.reshape(-1, 1))
    features_list.append(pred2.reshape(-1, 1))
    features_list.append(((pred1 + pred2) / 2).reshape(-1, 1))
    features_list.append(np.abs(pred1 - pred2).reshape(-1, 1))
    return np.hstack(features_list)


def create_minimal_features(X_flat, pred1, pred2):
    """æœ€å°åŒ–ç‰¹å¾ï¼ˆ72ç»´ï¼‰"""
    features_list = [X_flat]
    features_list.append(pred1.reshape(-1, 1))
    features_list.append(pred2.reshape(-1, 1))
    return np.hstack(features_list)


# ========== æ®‹å·®å­¦ä¹ è®­ç»ƒå‡½æ•° ==========

def train_original_xgboost(X_train, y_train):
    """åŸå§‹XGBoostå‚æ•°"""
    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.03,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_conservative_xgboost(X_train, y_train):
    """ä¿å®ˆXGBoostå‚æ•°"""
    model = XGBRegressor(
        n_estimators=100,
        learning_rate=0.01,
        max_depth=3,
        min_child_weight=5,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_xgboost_with_early_stopping(X_train, y_train):
    """å¸¦æ—©åœçš„XGBoost"""
    split_point = int(len(X_train) * 0.85)
    X_tr, X_val = X_train[:split_point], X_train[split_point:]
    y_tr, y_val = y_train[:split_point], y_train[split_point:]

    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.01,
        max_depth=3,
        min_child_weight=5,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        verbosity=0,
        early_stopping_rounds=20
    )

    try:
        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
    except TypeError:
        model = XGBRegressor(
            n_estimators=500,
            learning_rate=0.01,
            max_depth=3,
            min_child_weight=5,
            subsample=0.7,
            colsample_bytree=0.7,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=42,
            verbosity=0
        )
        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=False)
    return model


def train_ridge_model(X_train, y_train, alpha=10.0):
    """Ridgeçº¿æ€§å›å½’"""
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    return model


# ========== æ®‹å·®å¤„ç†å‡½æ•° ==========

def clip_residual(residual_pred, threshold=2.0):
    """å‰ªè£æç«¯æ®‹å·®å€¼"""
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    """åŠ æƒæ®‹å·®ä¿®æ­£"""
    return base_pred + weight * residual_pred


# ========== ã€æ ¸å¿ƒæ”¹è¿›ã€‘å¯¹ç§°å®éªŒæ¡†æ¶ ==========

def run_symmetric_experiments(base_model_name, base_oof_preds, base_test_pred,
                              base_residual, other_oof_preds, other_test_pred,
                              X_train_flat, X_test_flat, y_test_seq):
    """
    ä¸ºå•ä¸ªåŸºç¡€æ¨¡å‹è¿è¡Œæ‰€æœ‰9ä¸ªå®éªŒ

    å‚æ•°:
        base_model_name: åŸºç¡€æ¨¡å‹åç§° ('LSTM' or 'GRU')
        base_oof_preds: åŸºç¡€æ¨¡å‹çš„OOFé¢„æµ‹
        base_test_pred: åŸºç¡€æ¨¡å‹çš„æµ‹è¯•é›†é¢„æµ‹
        base_residual: åŸºç¡€æ¨¡å‹çš„æ®‹å·®
        other_oof_preds: å¦ä¸€ä¸ªæ¨¡å‹çš„OOFé¢„æµ‹ï¼ˆç”¨äºç‰¹å¾ï¼‰
        other_test_pred: å¦ä¸€ä¸ªæ¨¡å‹çš„æµ‹è¯•é›†é¢„æµ‹ï¼ˆç”¨äºç‰¹å¾ï¼‰
        X_train_flat, X_test_flat: æ‰å¹³åŒ–ç‰¹å¾
        y_test_seq: æµ‹è¯•é›†çœŸå®å€¼
    """

    print(f"\n{'=' * 120}")
    print(f"è¿è¡Œ{base_model_name}çš„9ä¸ªå¯¹ç§°å®éªŒ".center(120))
    print(f"{'=' * 120}")

    results = {}
    basic_features_train = X_train_flat[:len(base_oof_preds)]
    basic_features_test = X_test_flat

    # å‡†å¤‡ç‰¹å¾
    original_train = create_original_features(basic_features_train, base_oof_preds, other_oof_preds)
    original_test = create_original_features(basic_features_test, base_test_pred, other_test_pred)

    simplified_train = create_simplified_features(basic_features_train, base_oof_preds, other_oof_preds)
    simplified_test = create_simplified_features(basic_features_test, base_test_pred, other_test_pred)

    minimal_train = create_minimal_features(basic_features_train, base_oof_preds, other_oof_preds)
    minimal_test = create_minimal_features(basic_features_test, base_test_pred, other_test_pred)

    # å®éªŒ1ï¼šåŸå§‹æ–¹æ³•
    print(f"\n{base_model_name}-å®éªŒ1ï¼šåŸå§‹ç‰¹å¾ + æ¿€è¿›XGBoost")
    model1 = train_original_xgboost(original_train, base_residual)
    residual1 = model1.predict(original_test)
    pred1 = base_test_pred + residual1
    r2_1 = r2_score(y_test_seq, pred1)
    print(f"  RÂ²: {r2_1:.4f}")
    results[f'{base_model_name}-å®éªŒ1-åŸå§‹æ–¹æ³•'] = {'pred': pred1, 'r2': r2_1}

    # å®éªŒ2ï¼šç®€åŒ–ç‰¹å¾ + æ¿€è¿›XGBoost
    print(f"\n{base_model_name}-å®éªŒ2ï¼šç®€åŒ–ç‰¹å¾ + æ¿€è¿›XGBoost")
    model2 = train_original_xgboost(simplified_train, base_residual)
    residual2 = model2.predict(simplified_test)
    pred2 = base_test_pred + residual2
    r2_2 = r2_score(y_test_seq, pred2)
    print(f"  RÂ²: {r2_2:.4f}")
    results[f'{base_model_name}-å®éªŒ2-ç®€åŒ–ç‰¹å¾'] = {'pred': pred2, 'r2': r2_2}

    # å®éªŒ3ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost
    print(f"\n{base_model_name}-å®éªŒ3ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost")
    model3 = train_conservative_xgboost(simplified_train, base_residual)
    residual3 = model3.predict(simplified_test)
    pred3 = base_test_pred + residual3
    r2_3 = r2_score(y_test_seq, pred3)
    print(f"  RÂ²: {r2_3:.4f}")
    results[f'{base_model_name}-å®éªŒ3-ä¿å®ˆå‚æ•°'] = {'pred': pred3, 'r2': r2_3}

    # å®éªŒ4ï¼šç®€åŒ–ç‰¹å¾ + æ—©åœXGBoost
    print(f"\n{base_model_name}-å®éªŒ4ï¼šç®€åŒ–ç‰¹å¾ + æ—©åœXGBoost")
    model4 = train_xgboost_with_early_stopping(simplified_train, base_residual)
    residual4 = model4.predict(simplified_test)
    pred4 = base_test_pred + residual4
    r2_4 = r2_score(y_test_seq, pred4)
    print(f"  RÂ²: {r2_4:.4f}")
    results[f'{base_model_name}-å®éªŒ4-æ—©åœæœºåˆ¶'] = {'pred': pred4, 'r2': r2_4}

    # å®éªŒ5ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£
    print(f"\n{base_model_name}-å®éªŒ5ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£")
    model5 = train_conservative_xgboost(simplified_train, base_residual)
    residual5 = model5.predict(simplified_test)
    residual5_clipped = clip_residual(residual5, threshold=2.0)
    pred5 = base_test_pred + residual5_clipped
    r2_5 = r2_score(y_test_seq, pred5)
    print(f"  RÂ²: {r2_5:.4f}")
    results[f'{base_model_name}-å®éªŒ5-æ®‹å·®å‰ªè£'] = {'pred': pred5, 'r2': r2_5}

    # å®éªŒ6ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + åŠ æƒèåˆ(50%)
    print(f"\n{base_model_name}-å®éªŒ6ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + åŠ æƒèåˆ(50%)")
    model6 = train_conservative_xgboost(simplified_train, base_residual)
    residual6 = model6.predict(simplified_test)
    pred6 = weighted_residual_correction(base_test_pred, residual6, weight=0.5)
    r2_6 = r2_score(y_test_seq, pred6)
    print(f"  RÂ²: {r2_6:.4f}")
    results[f'{base_model_name}-å®éªŒ6-åŠ æƒèåˆ50%'] = {'pred': pred6, 'r2': r2_6}

    # å®éªŒ7ï¼šæœ€å°åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost
    print(f"\n{base_model_name}-å®éªŒ7ï¼šæœ€å°åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost")
    model7 = train_conservative_xgboost(minimal_train, base_residual)
    residual7 = model7.predict(minimal_test)
    pred7 = base_test_pred + residual7
    r2_7 = r2_score(y_test_seq, pred7)
    print(f"  RÂ²: {r2_7:.4f}")
    results[f'{base_model_name}-å®éªŒ7-æœ€å°ç‰¹å¾'] = {'pred': pred7, 'r2': r2_7}

    # å®éªŒ8ï¼šç®€åŒ–ç‰¹å¾ + Ridgeå›å½’
    print(f"\n{base_model_name}-å®éªŒ8ï¼šç®€åŒ–ç‰¹å¾ + Ridgeå›å½’")
    model8 = train_ridge_model(simplified_train, base_residual, alpha=10.0)
    residual8 = model8.predict(simplified_test)
    pred8 = base_test_pred + residual8
    r2_8 = r2_score(y_test_seq, pred8)
    print(f"  RÂ²: {r2_8:.4f}")
    results[f'{base_model_name}-å®éªŒ8-Ridgeå›å½’'] = {'pred': pred8, 'r2': r2_8}

    # å®éªŒ9ï¼šç»ˆæç»„åˆï¼ˆç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£ + åŠ æƒèåˆ30%ï¼‰
    print(f"\n{base_model_name}-å®éªŒ9ï¼šç»ˆæç»„åˆ")
    model9 = train_conservative_xgboost(simplified_train, base_residual)
    residual9 = model9.predict(simplified_test)
    residual9_clipped = clip_residual(residual9, threshold=2.0)
    pred9 = weighted_residual_correction(base_test_pred, residual9_clipped, weight=0.3)
    r2_9 = r2_score(y_test_seq, pred9)
    print(f"  RÂ²: {r2_9:.4f}")
    results[f'{base_model_name}-å®éªŒ9-ç»ˆæç»„åˆ'] = {'pred': pred9, 'r2': r2_9}

    return results


# ========== è¿è¡Œå¯¹ç§°å®éªŒ ==========

print("\n" + "=" * 120)
print("ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œå¯¹ç§°æ®‹å·®å­¦ä¹ å®éªŒ".center(120))
print("=" * 120)

# åŸºçº¿
avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"\nåŸºçº¿ï¼ˆç®€å•å¹³å‡ï¼‰RÂ²: {avg_r2:.4f}")

# è¿è¡ŒLSTMçš„9ä¸ªå®éªŒ
lstm_results = run_symmetric_experiments(
    'LSTM', lstm_oof_preds, lstm_test_pred, lstm_oof_residual,
    gru_oof_preds, gru_test_pred,
    X_train_flat, X_test_flat, y_test_seq
)

# è¿è¡ŒGRUçš„9ä¸ªå®éªŒ
gru_results = run_symmetric_experiments(
    'GRU', gru_oof_preds, gru_test_pred, gru_oof_residual,
    lstm_oof_preds, lstm_test_pred,
    X_train_flat, X_test_flat, y_test_seq
)

# åˆå¹¶æ‰€æœ‰ç»“æœ
all_results = {
    'LSTMå•æ¨¡å‹': {'pred': lstm_test_pred, 'r2': lstm_test_r2},
    'GRUå•æ¨¡å‹': {'pred': gru_test_pred, 'r2': gru_test_r2},
    'ç®€å•å¹³å‡': {'pred': avg_test_pred, 'r2': avg_r2},
    **lstm_results,
    **gru_results
}

# ========== ç»“æœåˆ†æ ==========

print("\n" + "=" * 120)
print("å¯¹ç§°å®éªŒç»“æœæ±‡æ€»".center(120))
print("=" * 120)

# æŒ‰RÂ²æ’åº
sorted_results = sorted(all_results.items(), key=lambda x: x[1]['r2'], reverse=True)

print(f"\n{'æ’å':<5} {'ç­–ç•¥':<50} {'RÂ²':>10} {'vsåŸºçº¿':>10} {'vså•æ¨¡å‹':>12}")
print("-" * 120)

for rank, (name, data) in enumerate(sorted_results, 1):
    r2 = data['r2']
    vs_baseline = r2 - avg_r2

    # åˆ¤æ–­æ˜¯LSTMç³»è¿˜æ˜¯GRUç³»
    if 'LSTM' in name:
        vs_single = r2 - lstm_test_r2
    elif 'GRU' in name:
        vs_single = r2 - gru_test_r2
    else:
        vs_single = 0.0

    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:<4} {name:<50} {r2:>10.4f} {vs_baseline:>+10.4f} {vs_single:>+12.4f}")

# ========== åˆ†ç»„å¯¹æ¯”åˆ†æ ==========

print("\n" + "=" * 120)
print("åˆ†ç»„å¯¹æ¯”åˆ†æ".center(120))
print("=" * 120)

# åˆ†ç¦»LSTMå’ŒGRUçš„ç»“æœ
lstm_experiment_results = {k: v for k, v in all_results.items() if 'LSTM-å®éªŒ' in k}
gru_experiment_results = {k: v for k, v in all_results.items() if 'GRU-å®éªŒ' in k}

print(f"\n{'å®éªŒç¼–å·':<30} {'LSTM RÂ²':>12} {'GRU RÂ²':>12} {'å·®è·':>12} {'æ›´ä¼˜è€…':>10}")
print("-" * 120)

for i in range(1, 10):
    lstm_key = f'LSTM-å®éªŒ{i}-' + ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶',
                                 'æ®‹å·®å‰ªè£', 'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ'][i - 1]
    gru_key = f'GRU-å®éªŒ{i}-' + ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶',
                               'æ®‹å·®å‰ªè£', 'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ'][i - 1]

    lstm_r2 = lstm_experiment_results[lstm_key]['r2']
    gru_r2 = gru_experiment_results[gru_key]['r2']
    diff = lstm_r2 - gru_r2
    winner = 'LSTM' if diff > 0 else 'GRU' if diff < 0 else 'å¹³å±€'

    exp_name = ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶', 'æ®‹å·®å‰ªè£',
                'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ'][i - 1]

    print(f"å®éªŒ{i}-{exp_name:<22} {lstm_r2:>12.4f} {gru_r2:>12.4f} {diff:>+12.4f} {winner:>10}")

# ç»Ÿè®¡èƒœè´Ÿ
lstm_wins = sum(1 for i in range(1, 10) if
                lstm_experiment_results[f'LSTM-å®éªŒ{i}-' + ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶',
                                                          'æ®‹å·®å‰ªè£', 'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ'][i - 1]]['r2'] >
                gru_experiment_results[f'GRU-å®éªŒ{i}-' + ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶',
                                                        'æ®‹å·®å‰ªè£', 'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ'][i - 1]]['r2'])

gru_wins = 9 - lstm_wins

print(f"\nå¯¹ç§°å®éªŒèƒœè´Ÿç»Ÿè®¡:")
print(f"  LSTMèƒœå‡º: {lstm_wins}/9 åœº")
print(f"  GRUèƒœå‡º: {gru_wins}/9 åœº")

# ========== æ‰¾å‡ºæœ€ä½³ç­–ç•¥ ==========

best_strategy_name = sorted_results[0][0]
best_r2 = sorted_results[0][1]['r2']
best_pred = sorted_results[0][1]['pred']

print(f"\nğŸ† å…¨å±€æœ€ä½³ç­–ç•¥: {best_strategy_name}")
print(f"   RÂ² = {best_r2:.4f}")
print(f"   ç›¸æ¯”åŸºçº¿æå‡: {best_r2 - avg_r2:+.4f}")
print(f"   ç›¸æ¯”å•æ¨¡å‹æå‡: {best_r2 - max(lstm_test_r2, gru_test_r2):+.4f}")

# ========== å¯è§†åŒ– ==========

results_directory = "./Predict_Symmetric/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

# 1. å¯¹ç§°å®éªŒå¯¹æ¯”å›¾
fig, axes = plt.subplots(3, 3, figsize=(20, 15))
axes = axes.flatten()

experiment_names = ['åŸå§‹æ–¹æ³•', 'ç®€åŒ–ç‰¹å¾', 'ä¿å®ˆå‚æ•°', 'æ—©åœæœºåˆ¶', 'æ®‹å·®å‰ªè£',
                    'åŠ æƒèåˆ50%', 'æœ€å°ç‰¹å¾', 'Ridgeå›å½’', 'ç»ˆæç»„åˆ']

for i, exp_name in enumerate(experiment_names):
    ax = axes[i]

    lstm_key = f'LSTM-å®éªŒ{i + 1}-{exp_name}'
    gru_key = f'GRU-å®éªŒ{i + 1}-{exp_name}'

    lstm_r2 = lstm_experiment_results[lstm_key]['r2']
    gru_r2 = gru_experiment_results[gru_key]['r2']

    bars = ax.bar(['LSTM', 'GRU'], [lstm_r2, gru_r2],
                  color=['#FF6B6B', '#4ECDC4'], alpha=0.7, edgecolor='black', linewidth=1.5)

    # æ·»åŠ åŸºçº¿
    ax.axhline(y=avg_r2, color='orange', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)
    ax.axhline(y=lstm_test_r2, color='red', linestyle=':', linewidth=1.5, label='LSTMå•æ¨¡å‹', alpha=0.5)
    ax.axhline(y=gru_test_r2, color='blue', linestyle=':', linewidth=1.5, label='GRUå•æ¨¡å‹', alpha=0.5)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, r2 in zip(bars, [lstm_r2, gru_r2]):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height,
                f'{r2:.4f}',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    ax.set_title(f'å®éªŒ{i + 1}: {exp_name}', fontsize=11, fontweight='bold')
    ax.set_ylabel('RÂ² Score', fontsize=9)
    ax.set_ylim(min(lstm_r2, gru_r2) - 0.05, max(lstm_r2, gru_r2) + 0.05)
    ax.grid(True, alpha=0.3, axis='y')

    if i == 0:
        ax.legend(fontsize=7, loc='upper left')

plt.tight_layout()
plt.savefig(results_directory + 'symmetric_experiments_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=False)

# 2. Top 10ç­–ç•¥å¯¹æ¯”
fig, ax = plt.subplots(figsize=(16, 10))

top_10 = sorted_results[:10]
names = [name for name, _ in top_10]
r2_scores = [data['r2'] for _, data in top_10]

# æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®é¢œè‰²
colors = []
for name in names:
    if 'LSTM' in name:
        colors.append('#FF6B6B')
    elif 'GRU' in name:
        colors.append('#4ECDC4')
    else:
        colors.append('#95E1D3')

bars = ax.barh(range(len(names)), r2_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

# æ·»åŠ åŸºçº¿
ax.axvline(x=avg_r2, color='orange', linestyle='--', linewidth=2.5, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.8)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, r2) in enumerate(zip(bars, r2_scores)):
    improvement = r2 - avg_r2
    label = f'{r2:.4f} ({improvement:+.4f})'
    color = 'green' if improvement > 0 else 'red'

    ax.text(bar.get_width() + 0.002, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(names)))
ax.set_yticklabels(names, fontsize=10)
ax.set_xlabel('RÂ² Score', fontsize=12, fontweight='bold')
ax.set_title('Top 10 ç­–ç•¥æ€§èƒ½æ’åï¼ˆå¯¹ç§°å®éªŒï¼‰', fontsize=14, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='lower right')
ax.grid(True, alpha=0.3, axis='x')

# æ·»åŠ å›¾ä¾‹è¯´æ˜
from matplotlib.patches import Patch

legend_elements = [
    Patch(facecolor='#FF6B6B', alpha=0.8, label='LSTMç³»åˆ—'),
    Patch(facecolor='#4ECDC4', alpha=0.8, label='GRUç³»åˆ—'),
    Patch(facecolor='#95E1D3', alpha=0.8, label='æ··åˆç­–ç•¥')
]
ax.legend(handles=legend_elements, loc='lower right', fontsize=10)

plt.tight_layout()
plt.savefig(results_directory + 'top10_strategies_ranking.png', dpi=300, bbox_inches='tight')
plt.show(block=False)

# 3. å®éªŒç±»å‹å¯¹æ¯”ï¼ˆåˆ†ç»„å¯¹æ¯”ï¼‰
fig, ax = plt.subplots(figsize=(14, 8))

x = np.arange(len(experiment_names))
width = 0.35

lstm_scores = [lstm_experiment_results[f'LSTM-å®éªŒ{i + 1}-{name}']['r2'] for i, name in enumerate(experiment_names)]
gru_scores = [gru_experiment_results[f'GRU-å®éªŒ{i + 1}-{name}']['r2'] for i, name in enumerate(experiment_names)]

bars1 = ax.bar(x - width / 2, lstm_scores, width, label='LSTM', color='#FF6B6B', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width / 2, gru_scores, width, label='GRU', color='#4ECDC4', alpha=0.8, edgecolor='black')

# æ·»åŠ åŸºçº¿
ax.axhline(y=avg_r2, color='orange', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=8, rotation=0)

ax.set_xlabel('å®éªŒç±»å‹', fontsize=12, fontweight='bold')
ax.set_ylabel('RÂ² Score', fontsize=12, fontweight='bold')
ax.set_title('LSTM vs GRU å¯¹ç§°å®éªŒå…¨é¢å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(experiment_names, rotation=45, ha='right', fontsize=10)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + 'lstm_vs_gru_full_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=False)

# 4. æœ€ä½³ç­–ç•¥é¢„æµ‹æ›²çº¿
fig, axes = plt.subplots(2, 2, figsize=(18, 12))

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))

# æ‰¾å‡ºLSTMå’ŒGRUå„è‡ªæœ€ä½³ç­–ç•¥
best_lstm = max(lstm_experiment_results.items(), key=lambda x: x[1]['r2'])
best_gru = max(gru_experiment_results.items(), key=lambda x: x[1]['r2'])

strategies_to_plot = [
    ('å…¨å±€æœ€ä½³', best_pred, sorted_results[0][1]['r2']),
    ('LSTMæœ€ä½³', best_lstm[1]['pred'], best_lstm[1]['r2']),
    ('GRUæœ€ä½³', best_gru[1]['pred'], best_gru[1]['r2']),
    ('ç®€å•å¹³å‡', avg_test_pred, avg_r2)
]

for idx, (name, pred, r2) in enumerate(strategies_to_plot):
    ax = axes[idx // 2, idx % 2]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))

    ax.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax.plot(pred_original, label=name, linewidth=2, alpha=0.8)

    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))

    ax.set_title(f'{name}\nRÂ²={r2:.4f}, MAE={mae:.2f}, RMSE={rmse:.2f}',
                 fontsize=12, fontweight='bold')
    ax.set_xlabel('æ ·æœ¬åºå·', fontsize=10)
    ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=10)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'best_strategies_predictions.png', dpi=300, bbox_inches='tight')
plt.show(block=False)

# 5. èƒœè´ŸçŸ©é˜µçƒ­åŠ›å›¾
fig, ax = plt.subplots(figsize=(12, 10))

# åˆ›å»ºå¯¹æ¯”çŸ©é˜µ
comparison_matrix = np.zeros((9, 2))  # 9ä¸ªå®éªŒ x 2ä¸ªæ¨¡å‹

for i in range(9):
    exp_name = experiment_names[i]
    lstm_r2 = lstm_experiment_results[f'LSTM-å®éªŒ{i + 1}-{exp_name}']['r2']
    gru_r2 = gru_experiment_results[f'GRU-å®éªŒ{i + 1}-{exp_name}']['r2']

    comparison_matrix[i, 0] = lstm_r2
    comparison_matrix[i, 1] = gru_r2

im = ax.imshow(comparison_matrix, cmap='RdYlGn', aspect='auto', vmin=0.6, vmax=0.9)

# è®¾ç½®åˆ»åº¦
ax.set_xticks([0, 1])
ax.set_xticklabels(['LSTM', 'GRU'], fontsize=12, fontweight='bold')
ax.set_yticks(range(9))
ax.set_yticklabels([f'å®éªŒ{i + 1}: {name}' for i, name in enumerate(experiment_names)], fontsize=10)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i in range(9):
    for j in range(2):
        text = ax.text(j, i, f'{comparison_matrix[i, j]:.4f}',
                       ha="center", va="center", color="black", fontsize=10, fontweight='bold')

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(im, ax=ax)
cbar.set_label('RÂ² Score', fontsize=12, fontweight='bold')

ax.set_title('LSTM vs GRU å¯¹ç§°å®éªŒçƒ­åŠ›å›¾', fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig(results_directory + 'lstm_gru_heatmap.png', dpi=300, bbox_inches='tight')
plt.show(block=False)

# 6. æ”¹è¿›æ•ˆæœè¶‹åŠ¿å›¾
fig, ax = plt.subplots(figsize=(14, 8))

improvements_lstm = [lstm_experiment_results[f'LSTM-å®éªŒ{i + 1}-{name}']['r2'] - lstm_test_r2
                     for i, name in enumerate(experiment_names)]
improvements_gru = [gru_experiment_results[f'GRU-å®éªŒ{i + 1}-{name}']['r2'] - gru_test_r2
                    for i, name in enumerate(experiment_names)]

x = np.arange(len(experiment_names))

ax.plot(x, improvements_lstm, marker='o', linewidth=2.5, markersize=10,
        label='LSTMæ”¹è¿›', color='#FF6B6B', alpha=0.8)
ax.plot(x, improvements_gru, marker='s', linewidth=2.5, markersize=10,
        label='GRUæ”¹è¿›', color='#4ECDC4', alpha=0.8)

ax.axhline(y=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)

# æ ‡æ³¨æ•°å€¼
for i, (imp_l, imp_g) in enumerate(zip(improvements_lstm, improvements_gru)):
    ax.text(i, imp_l + 0.002, f'{imp_l:+.3f}', ha='center', va='bottom', fontsize=8, color='#FF6B6B')
    ax.text(i, imp_g - 0.002, f'{imp_g:+.3f}', ha='center', va='top', fontsize=8, color='#4ECDC4')

ax.set_xlabel('å®éªŒç±»å‹', fontsize=12, fontweight='bold')
ax.set_ylabel('ç›¸æ¯”å•æ¨¡å‹çš„RÂ²æ”¹è¿›', fontsize=12, fontweight='bold')
ax.set_title('æ®‹å·®å­¦ä¹ æ”¹è¿›æ•ˆæœè¶‹åŠ¿å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(experiment_names, rotation=45, ha='right', fontsize=10)
ax.legend(fontsize=11, loc='best')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'improvement_trends.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# ========== ä¿å­˜ç»“æœ ==========

print("\n" + "=" * 120)
print("ä¿å­˜å¯¹ç§°å®éªŒç»“æœ".center(120))
print("=" * 120)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, data in all_results.items():
    pred_original = y_scaler.inverse_transform(data['pred'].reshape(-1, 1))
    predictions_dict[name.replace('/', '_').replace('-', '_')] = pred_original.flatten()

results_df = pd.DataFrame(predictions_dict)
results_df.to_csv(results_directory + 'symmetric_all_predictions.csv', index=False)

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
metrics_data = []
for name, data in all_results.items():
    pred_original = y_scaler.inverse_transform(data['pred'].reshape(-1, 1))

    base_model = 'LSTM' if 'LSTM' in name else 'GRU' if 'GRU' in name else 'Mixed'
    vs_single = data['r2'] - (lstm_test_r2 if base_model == 'LSTM' else gru_test_r2 if base_model == 'GRU' else 0)

    metrics_data.append({
        'strategy': name,
        'base_model': base_model,
        'r2': data['r2'],
        'mae': mean_absolute_error(y_test_original, pred_original),
        'rmse': sqrt(mean_squared_error(y_test_original, pred_original)),
        'mape': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8))),
        'vs_baseline': data['r2'] - avg_r2,
        'vs_single_model': vs_single
    })

metrics_df = pd.DataFrame(metrics_data)
metrics_df = metrics_df.sort_values('r2', ascending=False)
metrics_df.to_csv(results_directory + 'symmetric_performance_metrics.csv', index=False)

# ä¿å­˜å¯¹æ¯”åˆ†æ
comparison_data = []
for i in range(9):
    exp_name = experiment_names[i]
    lstm_key = f'LSTM-å®éªŒ{i + 1}-{exp_name}'
    gru_key = f'GRU-å®éªŒ{i + 1}-{exp_name}'

    lstm_r2 = lstm_experiment_results[lstm_key]['r2']
    gru_r2 = gru_experiment_results[gru_key]['r2']

    comparison_data.append({
        'experiment': f'å®éªŒ{i + 1}',
        'experiment_name': exp_name,
        'lstm_r2': lstm_r2,
        'gru_r2': gru_r2,
        'difference': lstm_r2 - gru_r2,
        'winner': 'LSTM' if lstm_r2 > gru_r2 else 'GRU' if gru_r2 > lstm_r2 else 'Tie'
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df.to_csv(results_directory + 'lstm_gru_comparison.csv', index=False)

print("\nâœ“ ä¿å­˜å®Œæˆï¼")
print(f"  - symmetric_all_predictions.csv (æ‰€æœ‰ç­–ç•¥é¢„æµ‹)")
print(f"  - symmetric_performance_metrics.csv (æ€§èƒ½æŒ‡æ ‡)")
print(f"  - lstm_gru_comparison.csv (LSTM vs GRUå¯¹æ¯”)")
print(f"  - 6å¼ å¯è§†åŒ–å›¾è¡¨")

# ========== æœ€ç»ˆæ€»ç»“æŠ¥å‘Š ==========

print("\n" + "=" * 120)
print("ğŸ‰ å¯¹ç§°å®éªŒå®Œæˆï¼æœ€ç»ˆæ€»ç»“æŠ¥å‘Š".center(120))
print("=" * 120)

print(f"\nğŸ“Š åŸºç¡€æ¨¡å‹æ€§èƒ½:")
print(f"  LSTMå•æ¨¡å‹: RÂ² = {lstm_test_r2:.4f}")
print(f"  GRUå•æ¨¡å‹:  RÂ² = {gru_test_r2:.4f}")
print(f"  ç®€å•å¹³å‡:   RÂ² = {avg_r2:.4f}")
print(f"  æ›´ä¼˜åŸºç¡€æ¨¡å‹: {'LSTM' if lstm_test_r2 > gru_test_r2 else 'GRU'}")

print(f"\nğŸ† å¯¹ç§°å®éªŒç»“æœ:")
print(f"  å…¨å±€æœ€ä½³ç­–ç•¥: {best_strategy_name}")
print(f"  æœ€ä½³RÂ²: {best_r2:.4f}")
print(f"  ç›¸æ¯”åŸºçº¿æå‡: {best_r2 - avg_r2:+.4f} ({(best_r2 - avg_r2) / avg_r2 * 100:+.2f}%)")

print(f"\n  LSTMæœ€ä½³ç­–ç•¥: {best_lstm[0]}")
print(f"  LSTMæœ€ä½³RÂ²: {best_lstm[1]['r2']:.4f} (æå‡: {best_lstm[1]['r2'] - lstm_test_r2:+.4f})")

print(f"\n  GRUæœ€ä½³ç­–ç•¥: {best_gru[0]}")
print(f"  GRUæœ€ä½³RÂ²: {best_gru[1]['r2']:.4f} (æå‡: {best_gru[1]['r2'] - gru_test_r2:+.4f})")

print(f"\nğŸ“ˆ å¯¹ç§°å®éªŒç»Ÿè®¡:")
print(f"  LSTMèƒœå‡ºåœºæ¬¡: {lstm_wins}/9")
print(f"  GRUèƒœå‡ºåœºæ¬¡: {gru_wins}/9")
print(f"  æ•´ä½“æ›´ä¼˜è€…: {'LSTM' if lstm_wins > gru_wins else 'GRU' if gru_wins > lstm_wins else 'å¹³æ‰‹'}")

print(f"\nğŸ’¡ æ ¸å¿ƒå‘ç°:")
avg_lstm_improvement = np.mean(improvements_lstm)
avg_gru_improvement = np.mean(improvements_gru)
print(f"  LSTMå¹³å‡æ”¹è¿›: {avg_lstm_improvement:+.4f}")
print(f"  GRUå¹³å‡æ”¹è¿›: {avg_gru_improvement:+.4f}")

positive_lstm = sum(1 for x in improvements_lstm if x > 0)
positive_gru = sum(1 for x in improvements_gru if x > 0)
print(f"  LSTMæ­£å‘æ”¹è¿›æ¬¡æ•°: {positive_lstm}/9")
print(f"  GRUæ­£å‘æ”¹è¿›æ¬¡æ•°: {positive_gru}/9")

print(f"\nğŸ¯ æœ€ä½³å®è·µå»ºè®®:")
if best_r2 > avg_r2:
    print(f"  âœ… æ®‹å·®å­¦ä¹ åœ¨æœ¬æ•°æ®é›†ä¸Šæœ‰æ•ˆ")
    print(f"  âœ… æ¨èä½¿ç”¨: {best_strategy_name}")
    print(f"  âœ… é¢„æœŸæ€§èƒ½æå‡: {(best_r2 - avg_r2) / avg_r2 * 100:.2f}%")
else:
    print(f"  âš ï¸ æ®‹å·®å­¦ä¹ æœªè¶…è¿‡ç®€å•å¹³å‡")
    print(f"  ğŸ’¡ å»ºè®®ä¼˜å…ˆæ”¹è¿›åŸºç¡€æ¨¡å‹")

print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_directory}")
print("=" * 120)