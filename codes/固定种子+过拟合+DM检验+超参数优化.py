"""
=====================================================================================
LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - Optunaä¼˜åŒ–å®Œæ•´ç‰ˆ
Complete Time Series Forecasting with Optuna Optimization
=====================================================================================

ç‰ˆæœ¬ä¿¡æ¯ (Version Info):
-----------------------
åŸå§‹ç‰ˆæœ¬: é›†æˆç®€åŒ–ç‰ˆè¿‡æ‹Ÿåˆæ£€æµ‹
ä¼˜åŒ–ç‰ˆæœ¬: 2.0 - é›†æˆOptunaè‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–
é›†æˆæ–¹å¼: å³æ’å³ç”¨ï¼Œå¯é€šè¿‡å¼€å…³å¯ç”¨/ç¦ç”¨

æ ¸å¿ƒæ”¹è¿› (Key Improvements):
---------------------------
âœ… é›†æˆç‹¬ç«‹Optunaä¼˜åŒ–æ¨¡å—
âœ… ä¿ç•™æ‰€æœ‰åŸæœ‰åŠŸèƒ½ï¼ˆè¿‡æ‹Ÿåˆæ£€æµ‹ã€å¤šç­–ç•¥èåˆã€å®Œæ•´å¯è§†åŒ–ï¼‰
âœ… ä¿ç•™DMæ£€éªŒï¼ˆDiebold-Mariano Testï¼‰
âœ… é€šè¿‡USE_OPTUNA_OPTIMIZATIONå¼€å…³æ§åˆ¶
âœ… è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå’Œå¯è§†åŒ–
âœ… é›¶ä¾µå…¥è®¾è®¡ï¼Œä¸å½±å“åŸä»£ç é€»è¾‘

ä½¿ç”¨è¯´æ˜ (Usage):
----------------
1. ç¡®ä¿optuna_optimizer.pyåœ¨åŒç›®å½•ä¸‹
2. è®¾ç½®USE_OPTUNA_OPTIMIZATION = Trueå¯ç”¨ä¼˜åŒ–
3. è¿è¡Œä»£ç ï¼Œè‡ªåŠ¨ä¼˜åŒ–å¹¶ä½¿ç”¨æœ€ä¼˜å‚æ•°
4. æŸ¥çœ‹./optuna_results/ç›®å½•è·å–ä¼˜åŒ–æŠ¥å‘Š

Author: AI Research Team
Date: 2025-10
=====================================================================================
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import random
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from xgboost import XGBRegressor
import warnings
import pickle

warnings.filterwarnings('ignore')

# =====================================================================================
# âœ¨ NEW: å¯¼å…¥ç‹¬ç«‹Optunaä¼˜åŒ–æ¨¡å—
# =====================================================================================
try:
    from optuna_optimizer import OptunaOptimizer
    OPTUNA_AVAILABLE = True
    print("[INFO] âœ… Optunaä¼˜åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("[WARNING] âš ï¸ æœªæ‰¾åˆ°optuna_optimizer.pyï¼Œå°†ä½¿ç”¨æ‰‹åŠ¨å‚æ•°")
    print("[INFO] ä¸‹è½½åœ°å€: è¯·å°†optuna_optimizer.pyæ”¾åœ¨åŒç›®å½•ä¸‹")

# =====================================================================================
# âœ¨ NEW: Optunaä¼˜åŒ–é…ç½®ï¼ˆå…¨å±€å¼€å…³ï¼‰
# =====================================================================================
USE_OPTUNA_OPTIMIZATION = True  # ğŸ”„ è®¾ç½®ä¸ºFalseåˆ™ä½¿ç”¨åŸæœ‰æ‰‹åŠ¨å‚æ•°
OPTUNA_CONFIG = {
    'lstm_trials': 20,      # å‡å°‘åˆ°20æ¬¡
    'gru_trials': 20,
    'xgb_trials': 10,
    'timeout_hours': 2,     # æœ€å¤š2å°æ—¶
    'enable_pruning': True,
    'verbose': True,
}

# ä¸­æ–‡æ˜¾ç¤ºè®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - Optunaè‡ªåŠ¨ä¼˜åŒ–ç‰ˆ".center(100))
else:
    print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - æ‰‹åŠ¨å‚æ•°ç‰ˆ".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šä»…å¯¹åŸºæ¨¡å‹è¿›è¡Œå­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹ + å®Œæ•´è®­ç»ƒæµç¨‹".center(100))
print("=" * 100)


# =====================================================================================
# SECTION 1: å…¨å±€ç§å­å›ºå®š
# =====================================================================================
def set_global_seed(seed=12):
    """å…¨å±€å›ºå®šéšæœºç§å­ - ç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

    try:
        tf.config.experimental.enable_op_determinism()
    except:
        pass

    print(f"âœ… å…¨å±€éšæœºç§å­å·²å›ºå®š: {seed}")


GLOBAL_SEED = 12
set_global_seed(GLOBAL_SEED)

# =====================================================================================
# SECTION 2: è¿‡æ‹Ÿåˆæ£€æµ‹æ¨¡å—ï¼ˆä¿æŒåŸæ ·ï¼‰
# =====================================================================================
class OverfittingDetector:
    """è¿‡æ‹Ÿåˆæ£€æµ‹å™¨ - åŸºäºå­¦ä¹ æ›²çº¿åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, output_dir='./overfitting_analysis/'):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.results = {}

    def sequential_learning_curve(self, model_builder, X_seq, y_seq,
                                  model_name='Model',
                                  train_sizes=None,
                                  epochs=100,
                                  batch_size=32):
        """ä¸ºåºåˆ—æ¨¡å‹ï¼ˆLSTM/GRUï¼‰ç”Ÿæˆå­¦ä¹ æ›²çº¿"""
        print(f"\n{'=' * 80}")
        print(f"å­¦ä¹ æ›²çº¿åˆ†æ: {model_name}".center(80))
        print(f"{'=' * 80}")

        if train_sizes is None:
            train_sizes = np.linspace(0.1, 1.0, 10)

        val_split_idx = int(len(X_seq) * 0.8)
        X_train_pool = X_seq[:val_split_idx]
        y_train_pool = y_seq[:val_split_idx]
        X_val = X_seq[val_split_idx:]
        y_val = y_seq[val_split_idx:]

        train_scores = []
        val_scores = []
        train_losses = []
        val_losses = []
        sample_counts = []

        for idx, train_size in enumerate(train_sizes):
            tf.random.set_seed(GLOBAL_SEED + idx)
            np.random.seed(GLOBAL_SEED + idx)

            if train_size <= 1.0:
                n_samples = int(len(X_train_pool) * train_size)
            else:
                n_samples = int(train_size)

            n_samples = max(n_samples, batch_size)

            print(f"è®­ç»ƒæ ·æœ¬æ•°: {n_samples}/{len(X_train_pool)}", end=' ')

            X_train_subset = X_train_pool[:n_samples]
            y_train_subset = y_train_pool[:n_samples]

            model = model_builder((X_seq.shape[1], X_seq.shape[2]))
            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

            early_stop = EarlyStopping(monitor='val_loss', patience=15,
                                       restore_best_weights=True, verbose=0)

            history = model.fit(
                X_train_subset, y_train_subset,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[early_stop],
                shuffle=False,
                verbose=0
            )

            train_pred = model.predict(X_train_subset, verbose=0).flatten()
            val_pred = model.predict(X_val, verbose=0).flatten()

            train_mse = mean_squared_error(y_train_subset, train_pred)
            val_mse = mean_squared_error(y_val, val_pred)
            train_r2 = r2_score(y_train_subset, train_pred)
            val_r2 = r2_score(y_val, val_pred)

            train_scores.append(train_r2)
            val_scores.append(val_r2)
            train_losses.append(train_mse)
            val_losses.append(val_mse)
            sample_counts.append(n_samples)

            print(f"â†’ è®­ç»ƒR^2={train_r2:.4f}, éªŒè¯R^2={val_r2:.4f}, å·®è·={train_r2 - val_r2:.4f}")

        self.results[model_name] = {
            'sample_counts': sample_counts,
            'train_scores': train_scores,
            'val_scores': val_scores,
            'train_losses': train_losses,
            'val_losses': val_losses
        }

        self._diagnose_overfitting(model_name, train_scores, val_scores)
        return sample_counts, train_scores, val_scores, train_losses, val_losses

    def _diagnose_overfitting(self, model_name, train_scores, val_scores):
        """è¯Šæ–­è¿‡æ‹ŸåˆçŠ¶æ€"""
        print(f"\n{'=' * 80}")
        print(f"è¿‡æ‹Ÿåˆè¯Šæ–­: {model_name}".center(80))
        print(f"{'=' * 80}")

        final_gap = train_scores[-1] - val_scores[-1]

        if len(train_scores) >= 3:
            train_trend = train_scores[-1] - train_scores[-3]
            val_trend = val_scores[-1] - val_scores[-3]
        else:
            train_trend = train_scores[-1] - train_scores[0]
            val_trend = val_scores[-1] - val_scores[0]

        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  - æœ€ç»ˆè®­ç»ƒR^2: {train_scores[-1]:.4f}")
        print(f"  - æœ€ç»ˆéªŒè¯R^2: {val_scores[-1]:.4f}")
        print(f"  - R^2å·®è·: {final_gap:.4f}")
        print(f"  - è®­ç»ƒè¶‹åŠ¿: {train_trend:+.4f}")
        print(f"  - éªŒè¯è¶‹åŠ¿: {val_trend:+.4f}")

        print(f"\nğŸ” è¯Šæ–­ç»“è®º:")

        if final_gap > 0.2:
            print(f"  âš ï¸  ä¸¥é‡è¿‡æ‹Ÿåˆ (å·®è·>0.2)")
            diagnosis = "ä¸¥é‡è¿‡æ‹Ÿåˆ"
        elif final_gap > 0.1:
            print(f"  âš ï¸  ä¸­åº¦è¿‡æ‹Ÿåˆ (å·®è·>0.1)")
            diagnosis = "ä¸­åº¦è¿‡æ‹Ÿåˆ"
        elif final_gap > 0.05:
            print(f"  âš¡ è½»å¾®è¿‡æ‹Ÿåˆ (å·®è·>0.05)")
            diagnosis = "è½»å¾®è¿‡æ‹Ÿåˆ"
        else:
            print(f"  âœ… æ‹Ÿåˆè‰¯å¥½ (å·®è·<0.05)")
            diagnosis = "æ‹Ÿåˆè‰¯å¥½"

        if val_scores[-1] < 0.5:
            print(f"  âš ï¸  å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ (éªŒè¯R^2<0.5)")
            diagnosis += " + æ¬ æ‹Ÿåˆ"

        if abs(val_trend) < 0.01:
            print(f"  âœ… æ¨¡å‹å·²æ”¶æ•›")
        else:
            print(f"  âš¡ æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šæ•°æ®")

        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if final_gap > 0.1:
            print(f"  - å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦")
            print(f"  - å‡å°‘æ¨¡å‹å¤æ‚åº¦")
            print(f"  - å¢åŠ Dropoutæ¯”ä¾‹")
            print(f"  - ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®")
        elif val_scores[-1] < 0.5:
            print(f"  - å¢åŠ æ¨¡å‹å¤æ‚åº¦")
            print(f"  - å¢åŠ è®­ç»ƒè½®æ•°")
            print(f"  - ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–")
        else:
            print(f"  - æ¨¡å‹çŠ¶æ€è‰¯å¥½ï¼Œå¯æŠ•å…¥ä½¿ç”¨")

        self.results[model_name]['diagnosis'] = diagnosis
        self.results[model_name]['final_gap'] = final_gap

    def plot_all_learning_curves(self, figsize=(20, 8)):
        """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„å­¦ä¹ æ›²çº¿å¯¹æ¯”"""
        n_models = len(self.results)
        if n_models == 0:
            print("æ²¡æœ‰å¯ç»˜åˆ¶çš„ç»“æœï¼")
            return

        fig, axes = plt.subplots(1, 2, figsize=figsize)

        for idx, (model_name, result) in enumerate(self.results.items()):
            if idx >= 2:
                break

            ax = axes[idx]

            sample_counts = result['sample_counts']
            train_scores = result['train_scores']
            val_scores = result['val_scores']
            diagnosis = result.get('diagnosis', 'Unknown')
            final_gap = result.get('final_gap', 0)

            ax.plot(sample_counts, train_scores, 'o-',
                    linewidth=2.5, markersize=8,
                    label='è®­ç»ƒR^2', color='#2E86AB', alpha=0.8)
            ax.plot(sample_counts, val_scores, 's-',
                    linewidth=2.5, markersize=8,
                    label='éªŒè¯R^2', color='#A23B72', alpha=0.8)

            ax.axhline(y=train_scores[-1], color='#2E86AB',
                       linestyle='--', alpha=0.3, linewidth=1)
            ax.axhline(y=val_scores[-1], color='#A23B72',
                       linestyle='--', alpha=0.3, linewidth=1)

            ax.fill_between(sample_counts, train_scores, val_scores,
                            alpha=0.2, color='red' if final_gap > 0.1 else 'green')

            ax.set_title(f'{model_name}\n{diagnosis} (å·®è·={final_gap:.4f})',
                         fontsize=13, fontweight='bold')
            ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°', fontsize=11)
            ax.set_ylabel('R^2 Score', fontsize=11)
            ax.legend(loc='best', fontsize=10)
            ax.grid(True, alpha=0.3)

            ax.text(sample_counts[-1], train_scores[-1],
                    f'{train_scores[-1]:.3f}',
                    fontsize=9, ha='right', va='bottom', color='#2E86AB')
            ax.text(sample_counts[-1], val_scores[-1],
                    f'{val_scores[-1]:.3f}',
                    fontsize=9, ha='right', va='top', color='#A23B72')

        plt.suptitle('åŸºæ¨¡å‹å­¦ä¹ æ›²çº¿åˆ†æ - è¿‡æ‹Ÿåˆè¯Šæ–­',
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}learning_curves_base_models.png',
                    dpi=300, bbox_inches='tight')
        plt.show()

        print(f"\nâœ… å­¦ä¹ æ›²çº¿å¯¹æ¯”å›¾å·²ä¿å­˜: {self.output_dir}learning_curves_base_models.png")

    def plot_loss_curves(self, figsize=(20, 6)):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        n_models = len(self.results)
        if n_models == 0:
            return

        fig, axes = plt.subplots(1, 2, figsize=figsize)

        for idx, (model_name, result) in enumerate(self.results.items()):
            if idx >= 2:
                break

            ax = axes[idx]

            sample_counts = result['sample_counts']
            train_losses = result['train_losses']
            val_losses = result['val_losses']

            ax.plot(sample_counts, train_losses, 'o-',
                    linewidth=2.5, label='è®­ç»ƒæŸå¤±', color='#F18F01')
            ax.plot(sample_counts, val_losses, 's-',
                    linewidth=2.5, label='éªŒè¯æŸå¤±', color='#C73E1D')

            ax.set_title(f'{model_name} - MSEæŸå¤±',
                         fontsize=13, fontweight='bold')
            ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°', fontsize=11)
            ax.set_ylabel('MSE Loss', fontsize=11)
            ax.legend(loc='best', fontsize=10)
            ax.grid(True, alpha=0.3)
            ax.set_yscale('log')

        plt.suptitle('åŸºæ¨¡å‹è®­ç»ƒä¸éªŒè¯æŸå¤±æ›²çº¿',
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}loss_curves_base_models.png',
                    dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… æŸå¤±æ›²çº¿å›¾å·²ä¿å­˜: {self.output_dir}loss_curves_base_models.png")

    def generate_report(self):
        """ç”Ÿæˆè¿‡æ‹Ÿåˆè¯Šæ–­æŠ¥å‘Š"""
        print(f"\n{'=' * 80}")
        print("åŸºæ¨¡å‹è¿‡æ‹Ÿåˆè¯Šæ–­æ€»ç»“æŠ¥å‘Š".center(80))
        print(f"{'=' * 80}\n")

        report_data = []
        for model_name, result in self.results.items():
            report_data.append({
                'æ¨¡å‹': model_name,
                'æœ€ç»ˆè®­ç»ƒR^2': result['train_scores'][-1],
                'æœ€ç»ˆéªŒè¯R^2': result['val_scores'][-1],
                'R^2å·®è·': result['final_gap'],
                'è¯Šæ–­ç»“æœ': result['diagnosis']
            })

        df = pd.DataFrame(report_data)
        df = df.sort_values('R^2å·®è·', ascending=False)

        print(df.to_string(index=False))

        df.to_csv(f'{self.output_dir}overfitting_report.csv', index=False)
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {self.output_dir}overfitting_report.csv")

        print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - åˆ†ææ¨¡å‹æ•°: {len(self.results)}")
        print(f"  - æ‹Ÿåˆè‰¯å¥½: {len([r for r in report_data if r['R^2å·®è·'] < 0.05])}")
        print(f"  - è½»å¾®è¿‡æ‹Ÿåˆ: {len([r for r in report_data if 0.05 <= r['R^2å·®è·'] < 0.1])}")
        print(f"  - ä¸­åº¦è¿‡æ‹Ÿåˆ: {len([r for r in report_data if 0.1 <= r['R^2å·®è·'] < 0.2])}")
        print(f"  - ä¸¥é‡è¿‡æ‹Ÿåˆ: {len([r for r in report_data if r['R^2å·®è·'] >= 0.2])}")

        print(f"\nğŸ’¡ å…³é”®ç»“è®º:")
        print(f"  - ç­–ç•¥æ¨¡å‹ï¼ˆXGBoostï¼‰ä»…å­¦ä¹ æ®‹å·®ï¼Œå½±å“æœ‰é™")
        print(f"  - åŸºæ¨¡å‹è¿‡æ‹Ÿåˆæ˜¯ä¸»è¦é£é™©ï¼Œéœ€é‡ç‚¹å…³æ³¨")
        print(f"  - å¦‚åŸºæ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°åé‡æ–°è®­ç»ƒ")


# =====================================================================================
# SECTION 3: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================================================================================
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# =====================================================================================
# SECTION 4: ç‰¹å¾å·¥ç¨‹
# =====================================================================================
def add_features(X, y):
    """æ·»åŠ æ»åç‰¹å¾"""
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# =====================================================================================
# âœ¨ NEW SECTION: Optunaè¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µ
# =====================================================================================

if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print("\n" + "=" * 100)
    print("ã€Optunaè¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µã€‘".center(100))
    print("=" * 100)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = OptunaOptimizer(
        X_train=X_train_seq,
        y_train=y_train_seq,
        output_dir='./optuna_results/',
        seed=GLOBAL_SEED,
        verbose=OPTUNA_CONFIG['verbose']
    )

    # ä¼˜åŒ–LSTM
    print(f"\n[1/3] ä¼˜åŒ–LSTMè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['lstm_trials']})...")
    best_lstm_params = optimizer.optimize_lstm(
        n_trials=OPTUNA_CONFIG['lstm_trials'],
        max_epochs=200,
        batch_size=32,
        enable_pruning=OPTUNA_CONFIG['enable_pruning']
    )

    # ä¼˜åŒ–GRU
    print(f"\n[2/3] ä¼˜åŒ–GRUè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['gru_trials']})...")
    best_gru_params = optimizer.optimize_gru(
        n_trials=OPTUNA_CONFIG['gru_trials'],
        max_epochs=200,
        batch_size=32,
        enable_pruning=OPTUNA_CONFIG['enable_pruning']
    )

    # ä¼˜åŒ–XGBoost
    print(f"\n[3/3] ä¼˜åŒ–XGBoostè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['xgb_trials']})...")
    best_xgb_params = optimizer.optimize_xgboost(
        n_trials=OPTUNA_CONFIG['xgb_trials'],
        cv_splits=5
    )

    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå’Œå¯è§†åŒ–
    try:
        optimizer.visualize('lstm', show=False)
        optimizer.visualize('gru', show=False)
        optimizer.visualize('xgboost', show=False)
    except Exception as e:
        print(f"[WARNING] å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

    optimizer.generate_report()

    print("\n" + "=" * 100)
    print("âœ… Optunaä¼˜åŒ–å®Œæˆï¼".center(100))
    print(f"ä¼˜åŒ–ç»“æœä¿å­˜åœ¨: ./optuna_results/".center(100))
    print("=" * 100)

else:
    # ä½¿ç”¨åŸæœ‰æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°
    print("\n" + "=" * 100)
    print("ã€ä½¿ç”¨æ‰‹åŠ¨è®¾ç½®å‚æ•°ã€‘".center(100))
    print("=" * 100)

    best_lstm_params = {
        'units': 80,
        'dropout': 0.3,
        'recurrent_dropout': 0.0,
        'l2_reg': 0.01,
        'learning_rate': 0.001
    }

    best_gru_params = {
        'units': 100,
        'dropout': 0.3,
        'recurrent_dropout': 0.0,
        'learning_rate': 0.001
    }

    best_xgb_params = {
        'n_estimators': 100,
        'learning_rate': 0.01,
        'max_depth': 3,
        'min_child_weight': 5,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0
    }

    print("\nå½“å‰ä½¿ç”¨çš„è¶…å‚æ•°:")
    print(f"LSTM: {best_lstm_params}")
    print(f"GRU: {best_gru_params}")
    print(f"XGBoost: {best_xgb_params}")


# =====================================================================================
# SECTION 5: æ¨¡å‹æ„å»ºå™¨ï¼ˆç”¨äºè¿‡æ‹Ÿåˆæ£€æµ‹ï¼‰
# =====================================================================================
def build_final_lstm(input_shape):
    """æœ€ç»ˆLSTMæ¨¡å‹"""
    tf.random.set_seed(GLOBAL_SEED)
    return Sequential([
        layers.LSTM(
            units=best_lstm_params['units'],
            input_shape=input_shape,
            kernel_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
            recurrent_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
            recurrent_dropout=best_lstm_params.get('recurrent_dropout', 0.0),
            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
            recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
        ),
        layers.Dropout(best_lstm_params['dropout'], seed=GLOBAL_SEED),
        layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
    ])


def build_final_gru(input_shape):
    """æœ€ç»ˆGRUæ¨¡å‹"""
    tf.random.set_seed(GLOBAL_SEED)
    return Sequential([
        layers.GRU(
            units=best_gru_params['units'],
            input_shape=input_shape,
            recurrent_dropout=best_gru_params.get('recurrent_dropout', 0.0),
            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
            recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
        ),
        layers.Dropout(best_gru_params['dropout'], seed=GLOBAL_SEED),
        layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
    ])


# =====================================================================================
# SECTION 6: OOFé¢„æµ‹ç”Ÿæˆ
# =====================================================================================
def get_oof_predictions(X_seq, y_seq, params, model_type='lstm', n_splits=5):
    """ç”ŸæˆOOFé¢„æµ‹"""
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")

    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        tf.random.set_seed(GLOBAL_SEED + fold)
        np.random.seed(GLOBAL_SEED + fold)

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = Sequential([
                layers.LSTM(
                    units=params['units'],
                    input_shape=(X_seq.shape[1], X_seq.shape[2]),
                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                    recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
                ),
                layers.Dropout(params['dropout'], seed=GLOBAL_SEED),
                layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
            ])
        else:
            model = Sequential([
                layers.GRU(
                    units=params['units'],
                    input_shape=(X_seq.shape[1], X_seq.shape[2]),
                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                    recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
                ),
                layers.Dropout(params['dropout'], seed=GLOBAL_SEED),
                layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
            ])

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            shuffle=False,
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()

        fold += 1

    return oof_preds


print("\n" + "=" * 100)
print("ã€é˜¶æ®µ1ã€‘ç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(100))
print("=" * 100)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, best_lstm_params, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, best_gru_params, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF R^2: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF R^2: {r2_score(y_train_seq, gru_oof_preds):.4f}")


# =====================================================================================
# SECTION 7: è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ2ã€‘è®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä¼˜å‚æ•°ï¼‰".center(100))
print("=" * 100)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

lstm_final = Sequential([
    layers.LSTM(
        units=best_lstm_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
        recurrent_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
        recurrent_dropout=best_lstm_params.get('recurrent_dropout', 0.0),
        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
        recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
    ),
    layers.Dropout(best_lstm_params['dropout'], seed=GLOBAL_SEED),
    layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_lstm_params['learning_rate']))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    shuffle=False,
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

gru_final = Sequential([
    layers.GRU(
        units=best_gru_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        recurrent_dropout=best_gru_params.get('recurrent_dropout', 0.0),
        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
        recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
    ),
    layers.Dropout(best_gru_params['dropout'], seed=GLOBAL_SEED),
    layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_gru_params['learning_rate']))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    shuffle=False,
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

# è·å–é¢„æµ‹
lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

lstm_train_pred = lstm_final.predict(X_train_seq, verbose=0).flatten()
gru_train_pred = gru_final.predict(X_train_seq, verbose=0).flatten()

# è¿‡æ‹Ÿåˆè¯Šæ–­ï¼ˆåŸæ–¹æ³•ï¼‰
lstm_train_r2 = r2_score(y_train_seq, lstm_train_pred)
lstm_test_r2 = r2_score(y_test_seq, lstm_test_pred)
gru_train_r2 = r2_score(y_train_seq, gru_train_pred)
gru_test_r2 = r2_score(y_test_seq, gru_test_pred)

print(f"\nã€ä¼ ç»Ÿè¿‡æ‹Ÿåˆè¯Šæ–­ã€‘")
print(f"LSTM - è®­ç»ƒR^2: {lstm_train_r2:.4f}, æµ‹è¯•R^2: {lstm_test_r2:.4f}, å·®è·: {lstm_train_r2 - lstm_test_r2:.4f}")
print(f"GRU  - è®­ç»ƒR^2: {gru_train_r2:.4f}, æµ‹è¯•R^2: {gru_test_r2:.4f}, å·®è·: {gru_train_r2 - gru_test_r2:.4f}")

'''
overfitting_detected = max(lstm_train_r2 - lstm_test_r2, gru_train_r2 - gru_test_r2) > 0.15
if overfitting_detected:
    print(f"âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œå°†é‡‡ç”¨ä¿å®ˆæ®‹å·®å­¦ä¹ ç­–ç•¥ï¼")
else:
    print(f"âœ“ è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½ï¼Œå¯å°è¯•å¤šç§æ®‹å·®ç­–ç•¥")
'''

# =====================================================================================
# SECTION 8: å­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ3ã€‘å¯¹åŸºæ¨¡å‹ï¼ˆLSTM/GRUï¼‰è¿›è¡Œå­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹".center(100))
print("ç†ç”±ï¼šåŸºæ¨¡å‹æ˜¯ä¸»è¦é¢„æµ‹æ¥æºï¼Œè¿‡æ‹Ÿåˆä¼šç›´æ¥å½±å“æœ€ç»ˆç»“æœ".center(100))
print("=" * 100)

detector = OverfittingDetector(output_dir='./overfitting_analysis/')

print("\nğŸ” æ£€æµ‹LSTMæœ€ç»ˆæ¨¡å‹...")
detector.sequential_learning_curve(
    model_builder=build_final_lstm,
    X_seq=X_train_seq,
    y_seq=y_train_seq,
    model_name='LSTMæœ€ç»ˆæ¨¡å‹',
    train_sizes=np.linspace(0.2, 1.0, 8),
    epochs=100,
    batch_size=32
)

print("\nğŸ” æ£€æµ‹GRUæœ€ç»ˆæ¨¡å‹...")
detector.sequential_learning_curve(
    model_builder=build_final_gru,
    X_seq=X_train_seq,
    y_seq=y_train_seq,
    model_name='GRUæœ€ç»ˆæ¨¡å‹',
    train_sizes=np.linspace(0.2, 1.0, 8),
    epochs=100,
    batch_size=32
)

detector.plot_all_learning_curves(figsize=(16, 6))
detector.plot_loss_curves(figsize=(16, 5))
detector.generate_report()


# =====================================================================================
# SECTION 9: ç‰¹å¾å·¥ç¨‹ä¸XGBoostè®­ç»ƒ
# =====================================================================================
def create_simplified_features(X_flat, lstm_preds, gru_preds):
    """ç®€åŒ–ç‰¹å¾ï¼šåŸå§‹ + é¢„æµ‹ + å¹³å‡ + å·®å¼‚"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


def train_xgboost(X_train, y_train):
    """è®­ç»ƒXGBoost"""
    np.random.seed(GLOBAL_SEED)
    model = XGBRegressor(
        **best_xgb_params,
        random_state=GLOBAL_SEED,
        seed=GLOBAL_SEED,
        n_jobs=1,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def clip_residual(residual_pred, threshold=2.0):
    """å‰ªè£æç«¯æ®‹å·®å€¼"""
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    """åŠ æƒæ®‹å·®ä¿®æ­£"""
    return base_pred + weight * residual_pred


# åŸºå‡†ï¼šç®€å•å¹³å‡
print("\n" + "=" * 100)
print("ã€åŸºå‡†ã€‘ç®€å•å¹³å‡èåˆ".center(100))
print("=" * 100)

avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"ç®€å•å¹³å‡æµ‹è¯•é›†R^2: {avg_r2:.4f}")

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds
avg_oof_preds = (lstm_oof_preds + gru_oof_preds) / 2
avg_oof_residual = y_train_seq - avg_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# å‡†å¤‡ç‰¹å¾
strategies_results = {}

basic_features_train = X_train_flat[:len(gru_oof_preds)]
basic_features_test = X_test_flat

simplified_train = create_simplified_features(
    basic_features_train, lstm_oof_preds, gru_oof_preds
)
simplified_test = create_simplified_features(
    basic_features_test, lstm_test_pred, gru_test_pred
)

print(f"\nç‰¹å¾ç»´åº¦:")
print(f"  ç®€åŒ–ç‰¹å¾: {simplified_train.shape[1]} ç»´")

# ç­–ç•¥1ï¼šLSTMæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥1ã€‘LSTMæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + XGBoost".center(100))
print("=" * 100)

xgb_lstm = train_xgboost(simplified_train, lstm_oof_residual)
lstm_residual = xgb_lstm.predict(simplified_test)
pred_lstm_residual = lstm_test_pred + lstm_residual

r2_lstm_residual = r2_score(y_test_seq, pred_lstm_residual)
print(f"âœ“ R^2: {r2_lstm_residual:.4f} (vsç®€å•å¹³å‡: {r2_lstm_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '] = pred_lstm_residual

# ç­–ç•¥2ï¼šGRUæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥2ã€‘GRUæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + XGBoost".center(100))
print("=" * 100)

xgb_gru = train_xgboost(simplified_train, gru_oof_residual)
gru_residual = xgb_gru.predict(simplified_test)
pred_gru_residual = gru_test_pred + gru_residual

r2_gru_residual = r2_score(y_test_seq, pred_gru_residual)
print(f"âœ“ R^2: {r2_gru_residual:.4f} (vsç®€å•å¹³å‡: {r2_gru_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '] = pred_gru_residual

# ç­–ç•¥3ï¼šåŒæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥3ã€‘åŒæ®‹å·®å­¦ä¹ ï¼š(LSTM+GRU)/2 + XGBoost".center(100))
print("=" * 100)

xgb_dual = train_xgboost(simplified_train, avg_oof_residual)
dual_residual = xgb_dual.predict(simplified_test)
pred_dual = avg_test_pred + dual_residual

r2_dual = r2_score(y_test_seq, pred_dual)
print(f"âœ“ R^2: {r2_dual:.4f} (vsç®€å•å¹³å‡: {r2_dual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '] = pred_dual

# ç­–ç•¥4ï¼šæ®‹å·®å‰ªè£
print("\n" + "=" * 100)
print("ã€ç­–ç•¥4ã€‘GRUæ®‹å·®å­¦ä¹  + æ®‹å·®å‰ªè£".center(100))
print("=" * 100)

gru_residual_clipped = clip_residual(gru_residual, threshold=2.0)
pred_gru_clipped = gru_test_pred + gru_residual_clipped

r2_gru_clipped = r2_score(y_test_seq, pred_gru_clipped)
print(f"âœ“ R^2: {r2_gru_clipped:.4f} (vsç®€å•å¹³å‡: {r2_gru_clipped - avg_r2:+.4f})")
print(f"  æ®‹å·®å‰ªè£å‰std: {np.std(gru_residual):.6f}, å‰ªè£åstd: {np.std(gru_residual_clipped):.6f}")
strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'] = pred_gru_clipped

# ç­–ç•¥5ï¼šåŠ æƒèåˆï¼ˆ30%ï¼‰
print("\n" + "=" * 100)
print("ã€ç­–ç•¥5ã€‘GRUæ®‹å·®å­¦ä¹  + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_gru_weighted = weighted_residual_correction(gru_test_pred, gru_residual, weight=0.3)

r2_gru_weighted = r2_score(y_test_seq, pred_gru_weighted)
print(f"âœ“ R^2: {r2_gru_weighted:.4f} (vsç®€å•å¹³å‡: {r2_gru_weighted - avg_r2:+.4f})")
strategies_results['ç­–ç•¥5-åŠ æƒèåˆ30%'] = pred_gru_weighted

# ç­–ç•¥6ï¼šç»ˆæç»„åˆ
print("\n" + "=" * 100)
print("ã€ç­–ç•¥6ã€‘ç»ˆæç»„åˆï¼šæ®‹å·®å‰ªè£ + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_ultimate = weighted_residual_correction(gru_test_pred, gru_residual_clipped, weight=0.3)

r2_ultimate = r2_score(y_test_seq, pred_ultimate)
print(f"âœ“ R^2: {r2_ultimate:.4f} (vsç®€å•å¹³å‡: {r2_ultimate - avg_r2:+.4f})")
strategies_results['ç­–ç•¥6-ç»ˆæç»„åˆ'] = pred_ultimate


# =====================================================================================
# SECTION 10: ç»¼åˆå¯¹æ¯”
# =====================================================================================
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

all_strategies = {
    'LSTMå•æ¨¡å‹': lstm_test_pred,
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç®€å•å¹³å‡(åŸºçº¿)': avg_test_pred,
    **strategies_results
}

print(f"\n{'ç­–ç•¥':<30} {'R^2':>10} {'vsåŸºçº¿':>10} {'MAE':>12} {'RMSE':>12}")
print("-" * 75)

results_list = []
for name, pred in all_strategies.items():
    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    improvement = r2 - avg_r2
    results_list.append((name, r2, improvement, mae, rmse, pred))
    print(f"{name:<30} {r2:>10.4f} {improvement:>10.4f} {mae:>12.6f} {rmse:>12.6f}")

# æ’åº
results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’åï¼ˆæŒ‰R^2é™åºï¼‰".center(100))
print("=" * 100)

best_r2 = results_list[0][1]
best_name = results_list[0][0]

for rank, (name, r2, improvement, mae, rmse, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<30} R^2={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_name} (R^2 = {best_r2:.4f})")

# åŸå§‹å°ºåº¦è¯„ä¼°
print("\n" + "=" * 100)
print("åŸå§‹å°ºåº¦æ€§èƒ½å¯¹æ¯”".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))
strategies_original = {}

print(f"\n{'ç­–ç•¥':<30} {'R^2':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 75)

for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    strategies_original[name] = pred_original

    r2 = r2_score(y_test_original, pred_original)
    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))
    mape = np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))

    print(f"{name:<30} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")


# =====================================================================================
# SECTION 11: å®Œæ•´å¯è§†åŒ–ï¼ˆä¿ç•™åŸä»£ç æ‰€æœ‰å›¾è¡¨ï¼‰
# =====================================================================================
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

print("\n" + "=" * 100)
print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨".center(100))
print("=" * 100)

# å›¾1: è®­ç»ƒè¿‡ç¨‹
fig = plt.figure(figsize=(16, 5))

plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(lstm_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('LSTMæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(gru_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('GRUæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '01_training_process.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾1: è®­ç»ƒè¿‡ç¨‹æ›²çº¿")

# å›¾2: æ€§èƒ½æ’åæŸ±çŠ¶å›¾
fig, ax = plt.subplots(figsize=(16, 8))

strategy_names = [name for name, _, _, _, _, _ in results_list]
r2_scores = [r2 for _, r2, _, _, _, _ in results_list]
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(strategy_names)))

bars = ax.barh(range(len(strategy_names)), r2_scores, color=colors, alpha=0.8)

ax.axvline(x=avg_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

for i, (bar, r2, improvement) in enumerate(zip(bars, r2_scores, [imp for _, _, imp, _, _, _ in results_list])):
    label = f'{r2:.4f}'
    if improvement > 0:
        label += f' (+{improvement:.4f})'
        color = 'green'
    elif improvement < 0:
        label += f' ({improvement:.4f})'
        color = 'red'
    else:
        color = 'black'

    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(strategy_names)))
ax.set_yticklabels(strategy_names, fontsize=10)
ax.set_xlabel('R^2 Score', fontsize=12, fontweight='bold')
ax.set_title('æ‰€æœ‰ç­–ç•¥æ€§èƒ½æ’åå¯¹æ¯”', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(results_directory + '02_performance_ranking.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾2: æ€§èƒ½æ’åå¯¹æ¯”")

# å›¾3: Top6ç­–ç•¥é¢„æµ‹å¯¹æ¯”
fig, axes = plt.subplots(3, 2, figsize=(18, 15))
axes = axes.flatten()

top6_strategies = results_list[:6]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top6_strategies):
    ax = axes[idx]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    r2_original = r2_score(y_test_original, pred_original)

    ax.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax.plot(pred_original, label=name, linewidth=2, alpha=0.8)
    ax.set_title(f'{name}\nR^2={r2_original:.4f} (vsåŸºçº¿: {improvement:+.4f})',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('æ ·æœ¬åºå·', fontsize=9)
    ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '03_top6_strategies_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾3: Top6ç­–ç•¥é¢„æµ‹å¯¹æ¯”")

# å›¾4: æ®‹å·®åˆ†æå¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

residuals_dict = {
    'ç®€å•å¹³å‡': y_test_seq - avg_test_pred,
    'ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ': y_test_seq - strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ': y_test_seq - strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥4-æ®‹å·®å‰ªè£': y_test_seq - strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'],
}

for idx, (name, residual) in enumerate(residuals_dict.items()):
    ax = axes[idx // 2, idx % 2]
    ax.hist(residual, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
    ax.axvline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax.set_xlabel('æ®‹å·®')
    ax.set_ylabel('é¢‘æ•°')
    ax.text(0.05, 0.95, f'å‡å€¼={np.mean(residual):.5f}\nstd={np.std(residual):.5f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + '04_residual_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾4: æ®‹å·®åˆ†å¸ƒåˆ†æ")

# å›¾5: æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”
fig, ax = plt.subplots(figsize=(16, 6))

for name, residual in residuals_dict.items():
    ax.plot(residual, label=name, linewidth=2, alpha=0.7)

ax.axhline(0, color='black', linestyle='--', linewidth=1)
ax.set_title('ä¸åŒç­–ç•¥æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xlabel('æ ·æœ¬åºå·', fontsize=12)
ax.set_ylabel('æ®‹å·®', fontsize=12)
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '05_residual_timeseries.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾5: æ®‹å·®æ—¶é—´åºåˆ—")

# å›¾6: ç»¼åˆå¯¹æ¯”å›¾
plt.figure(figsize=(18, 8))
plt.plot(y_test_original, label='çœŸå®å€¼', linewidth=3, color='black', alpha=0.9, zorder=10)

key_strategies_for_plot = [
    ('ç®€å•å¹³å‡(åŸºçº¿)', strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'], 'gray'),
    ('ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '], 'red'),
    ('ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '], 'blue'),
    ('ç­–ç•¥6-ç»ˆæç»„åˆ', strategies_original['ç­–ç•¥6-ç»ˆæç»„åˆ'], 'green'),
]

for name, pred, color in key_strategies_for_plot:
    plt.plot(pred, label=name, linewidth=1.8, alpha=0.7, color=color)

plt.title('å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”', fontsize=16, fontweight='bold')
plt.xlabel('æ ·æœ¬åºå·', fontsize=12)
plt.ylabel('ç‰ç±³ä»·æ ¼', fontsize=12)
plt.legend(fontsize=11, loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(results_directory + '06_key_strategies_comprehensive.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾6: å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”")

# å›¾7: æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

metrics_data = {}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    metrics_data[name] = {
        'R^2': r2_score(y_test_original, pred_original),
        'MAE': mean_absolute_error(y_test_original, pred_original),
        'RMSE': sqrt(mean_squared_error(y_test_original, pred_original)),
        'MAPE': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))
    }

top8_names = [name for name, _, _, _, _, _ in results_list[:min(8, len(results_list))]]
colors_bar = plt.cm.viridis(np.linspace(0, 1, len(top8_names)))

axes[0, 0].bar(range(len(top8_names)), [metrics_data[m]['R^2'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[0, 0].set_title('R^2 åˆ†æ•°å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 0].set_ylabel('R^2 Score')
axes[0, 0].set_xticks(range(len(top8_names)))
axes[0, 0].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[0, 0].grid(True, alpha=0.3, axis='y')

axes[0, 1].bar(range(len(top8_names)), [metrics_data[m]['MAE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[0, 1].set_title('MAE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].set_xticks(range(len(top8_names)))
axes[0, 1].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[0, 1].grid(True, alpha=0.3, axis='y')

axes[1, 0].bar(range(len(top8_names)), [metrics_data[m]['RMSE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[1, 0].set_title('RMSE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 0].set_ylabel('RMSE')
axes[1, 0].set_xticks(range(len(top8_names)))
axes[1, 0].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[1, 0].grid(True, alpha=0.3, axis='y')

axes[1, 1].bar(range(len(top8_names)), [metrics_data[m]['MAPE'] for m in top8_names],
               color=colors_bar, alpha=0.7)
axes[1, 1].set_title('MAPE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 1].set_ylabel('MAPE')
axes[1, 1].set_xticks(range(len(top8_names)))
axes[1, 1].set_xticklabels([n[:20] for n in top8_names], rotation=45, ha='right', fontsize=8)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + '07_metrics_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾7: å¤šæŒ‡æ ‡å¯¹æ¯”")

# å›¾8: é¢„æµ‹è¯¯å·®åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

top4_for_error = results_list[:4]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top4_for_error):
    ax = axes[idx // 2, idx % 2]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    errors = pred_original - y_test_original.flatten()

    ax.scatter(y_test_original, errors, alpha=0.5, s=30)
    ax.axhline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} - é¢„æµ‹è¯¯å·®åˆ†æ', fontsize=11, fontweight='bold')
    ax.set_xlabel('çœŸå®å€¼')
    ax.set_ylabel('é¢„æµ‹è¯¯å·®')
    ax.grid(True, alpha=0.3)

    ax.text(0.05, 0.95, f'MAE={mae:.2f}\nRMSE={rmse:.2f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))

plt.tight_layout()
plt.savefig(results_directory + '08_prediction_error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾8: é¢„æµ‹è¯¯å·®åˆ†æ")

# å›¾9: æ¨¡å‹æ”¹è¿›æ•ˆæœé›·è¾¾å›¾
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='polar')

radar_strategies = ['ç®€å•å¹³å‡(åŸºçº¿)', 'ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ', 'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', 'ç­–ç•¥6-ç»ˆæç»„åˆ']
categories = ['R^2', 'MAE', 'RMSE', 'MAPE']
N = len(categories)

angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

for strategy in radar_strategies:
    pred_original = strategies_original[strategy]

    r2_val = metrics_data[strategy]['R^2']
    mae_val = 1 / (1 + metrics_data[strategy]['MAE'] / 100)
    rmse_val = 1 / (1 + metrics_data[strategy]['RMSE'] / 100)
    mape_val = 1 / (1 + metrics_data[strategy]['MAPE'] * 100)

    values = [r2_val, mae_val, rmse_val, mape_val]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=strategy)
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories)
ax.set_ylim(0, 1)
ax.set_title('å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax.grid(True)

plt.tight_layout()
plt.savefig(results_directory + '09_performance_radar.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾9: æ€§èƒ½é›·è¾¾å›¾")

# å›¾10: æ®‹å·®ç®±çº¿å›¾å¯¹æ¯”
fig, ax = plt.subplots(figsize=(14, 6))

residual_data = []
residual_labels = []

for name, residual in residuals_dict.items():
    residual_data.append(residual)
    residual_labels.append(name)

bp = ax.boxplot(residual_data, labels=residual_labels, patch_artist=True)

for patch, color in zip(bp['boxes'], plt.cm.Set3(np.linspace(0, 1, len(residual_data)))):
    patch.set_facecolor(color)

ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)
ax.set_title('æ®‹å·®åˆ†å¸ƒç®±çº¿å›¾å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_ylabel('æ®‹å·®å€¼')
ax.grid(True, alpha=0.3, axis='y')
plt.xticks(rotation=15, ha='right')

plt.tight_layout()
plt.savefig(results_directory + '10_residual_boxplot.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾10: æ®‹å·®ç®±çº¿å›¾")

print("\nâœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶ä¿å­˜ï¼")


# =====================================================================================
# SECTION 12: ä¿å­˜æ¨¡å‹å’Œç»“æœ
# =====================================================================================
print("\n" + "=" * 100)
print("ä¿å­˜æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_final.h5')
gru_final.save(results_directory + 'gru_final.h5')

# ä¿å­˜XGBoostæ¨¡å‹
with open(results_directory + 'xgb_lstm.pkl', 'wb') as f:
    pickle.dump(xgb_lstm, f)

with open(results_directory + 'xgb_gru.pkl', 'wb') as f:
    pickle.dump(xgb_gru, f)

with open(results_directory + 'xgb_dual.pkl', 'wb') as f:
    pickle.dump(xgb_dual, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    predictions_dict[name] = pred_original.flatten()

predictions_df = pd.DataFrame(predictions_dict)
predictions_df.to_csv(results_directory + 'all_predictions.csv', index=False)

# ä¿å­˜è¶…å‚æ•°
import json
hyperparams_dict = {
    'lstm_params': best_lstm_params,
    'gru_params': best_gru_params,
    'xgb_params': best_xgb_params,
    'optimization_method': 'Optuna' if (USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE) else 'Manual',
    'global_seed': GLOBAL_SEED
}

with open(results_directory + 'hyperparameters.json', 'w') as f:
    json.dump(hyperparams_dict, f, indent=4)

print(f"\nâœ“ æ‰€æœ‰æ¨¡å‹å’Œç»“æœå·²ä¿å­˜è‡³: {results_directory}")


# =====================================================================================
# SECTION 13: DMæ£€éªŒï¼ˆDiebold-Mariano Testï¼‰
# =====================================================================================
print("\n" + "=" * 100)
print("ã€DMæ£€éªŒã€‘Diebold-Marianoç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ".center(100))
print("=" * 100)

try:
    from dm_test import quick_dm_analysis, pairwise_dm_analysis

    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹å°ºåº¦çš„é¢„æµ‹ç»“æœï¼‰
    all_predictions = {
        'LSTMå•æ¨¡å‹': strategies_original['LSTMå•æ¨¡å‹'],
        'GRUå•æ¨¡å‹': strategies_original['GRUå•æ¨¡å‹'],
        'ç®€å•å¹³å‡(åŸºçº¿)': strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'],
        **{k: v for k, v in strategies_original.items() if k.startswith('ç­–ç•¥')}
    }

    # 1. åŸºå‡†å¯¹æ¯”åˆ†æ
    print("\nç¬¬1éƒ¨åˆ†: æ‰€æœ‰æ¨¡å‹ vs åŸºå‡†æ¨¡å‹")
    print("-" * 100)

    dm_results = quick_dm_analysis(
        y_true=y_test_original,
        predictions=all_predictions,
        baseline='ç®€å•å¹³å‡(åŸºçº¿)',
        save_dir=results_directory,
        plot=True,
        verbose=True
    )

    print("\nâœ… DMæ£€éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³:", results_directory)

except ImportError:
    print("[WARNING] âš ï¸ æœªæ‰¾åˆ°dm_test.pyæ¨¡å—ï¼Œè·³è¿‡DMæ£€éªŒ")
    print("[INFO] å¦‚éœ€DMæ£€éªŒï¼Œè¯·ç¡®ä¿dm_test.pyåœ¨åŒç›®å½•ä¸‹")


# =====================================================================================
# SECTION 14: æœ€ç»ˆæ€»ç»“æŠ¥å‘Š
# =====================================================================================
print("\n" + "=" * 100)
print("æœ€ç»ˆæ€»ç»“æŠ¥å‘Š".center(100))
print("=" * 100)

print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"  - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(y_train_seq)}")
print(f"  - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_test_seq)}")
print(f"  - ç‰¹å¾ç»´åº¦: {X_train_feat.shape[1]}")
print(f"  - åºåˆ—é•¿åº¦: {seq_len}")

print(f"\nğŸ† æœ€ä½³æ€§èƒ½ç­–ç•¥:")
print(f"  - ç­–ç•¥åç§°: {best_name}")
print(f"  - æµ‹è¯•é›†R^2: {best_r2:.4f}")
print(f"  - æå‡å¹…åº¦: {best_r2 - avg_r2:+.4f}")

best_pred = strategies_original[best_name]
best_mae = mean_absolute_error(y_test_original, best_pred)
best_rmse = sqrt(mean_squared_error(y_test_original, best_pred))
best_mape = np.mean(np.abs((best_pred - y_test_original) / (y_test_original + 1e-8)))

print(f"\nğŸ“ˆ æœ€ä½³ç­–ç•¥åŸå§‹å°ºåº¦æŒ‡æ ‡:")
print(f"  - MAE:  {best_mae:.2f}")
print(f"  - RMSE: {best_rmse:.2f}")
print(f"  - MAPE: {best_mape:.4%}")

print(f"\nâš™ï¸ è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•:")
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"  âœ… Optunaè‡ªåŠ¨ä¼˜åŒ–")
    print(f"  - LSTM trials: {OPTUNA_CONFIG['lstm_trials']}")
    print(f"  - GRU trials: {OPTUNA_CONFIG['gru_trials']}")
    print(f"  - XGBoost trials: {OPTUNA_CONFIG['xgb_trials']}")
    print(f"  - ä¼˜åŒ–æŠ¥å‘Š: ./optuna_results/")
else:
    print(f"  âš ï¸  æ‰‹åŠ¨è®¾ç½®å‚æ•°")

print(f"\nâš ï¸ è¿‡æ‹Ÿåˆè¯Šæ–­æ€»ç»“:")
lstm_gap = lstm_train_r2 - lstm_test_r2
gru_gap = gru_train_r2 - gru_test_r2
print(f"  ã€ä¼ ç»Ÿæ–¹æ³•ã€‘")
print(f"  - LSTM: è®­ç»ƒR^2={lstm_train_r2:.4f}, æµ‹è¯•R^2={lstm_test_r2:.4f}, å·®è·={lstm_gap:.4f}")
print(f"  - GRU:  è®­ç»ƒR^2={gru_train_r2:.4f}, æµ‹è¯•R^2={gru_test_r2:.4f}, å·®è·={gru_gap:.4f}")

print(f"\n  ã€å­¦ä¹ æ›²çº¿æ–¹æ³•ã€‘")
for model_name, result in detector.results.items():
    diagnosis = result.get('diagnosis', 'Unknown')
    final_gap = result.get('final_gap', 0)
    print(f"  - {model_name}: {diagnosis} (å·®è·={final_gap:.4f})")

print(f"\nğŸ’¡ å…³é”®ç»“è®º:")
if best_r2 > avg_r2:
    print(f"  âœ… æ®‹å·®å­¦ä¹ ç­–ç•¥æˆåŠŸæå‡äº†æ¨¡å‹æ€§èƒ½")
    print(f"  âœ… ç›¸æ¯”ç®€å•å¹³å‡æå‡äº† {(best_r2 - avg_r2) * 100:.2f}%")
else:
    print(f"  âš ï¸ æ®‹å·®å­¦ä¹ ç­–ç•¥æœªèƒ½æ”¹å–„æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨ç®€å•å¹³å‡")

if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"\nğŸ¯ Optunaä¼˜åŒ–æˆæœ:")
    print(f"  âœ… è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°ç»„åˆ")
    print(f"  âœ… èŠ‚çœå¤§é‡æ‰‹åŠ¨è°ƒå‚æ—¶é—´")
    print(f"  âœ… è¯¦ç»†ä¼˜åŒ–æŠ¥å‘Šå¯ä¾›åˆ†æ")

print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜ä½ç½®:")
print(f"  - æ¨¡å‹å’Œé¢„æµ‹ç»“æœ: {results_directory}")
print(f"  - è¿‡æ‹Ÿåˆåˆ†æ: {detector.output_dir}")
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"  - Optunaä¼˜åŒ–æŠ¥å‘Š: ./optuna_results/")

print("\n" + "=" * 100)
print("ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼".center(100))
print("=" * 100)