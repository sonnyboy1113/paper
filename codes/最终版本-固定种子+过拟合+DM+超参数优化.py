import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import random
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from xgboost import XGBRegressor
import warnings
import pickle

warnings.filterwarnings('ignore')
try:
    from feature_ablation import FeatureAblationAnalyzer
    FEATURE_ABLATION_AVAILABLE = True
    print("[INFO] âœ… ç‰¹å¾æ¶ˆèæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError:
    FEATURE_ABLATION_AVAILABLE = False
    print("[WARNING] âš ï¸ æœªæ‰¾åˆ°feature_ablation.pyï¼Œå°†è·³è¿‡ç‰¹å¾æ¶ˆèå®éªŒ")
# =====================================================================================
# âœ¨ NEW: å¯¼å…¥ç‹¬ç«‹Optunaä¼˜åŒ–æ¨¡å—
# =====================================================================================
try:
    from optuna_optimizer import OptunaOptimizer
    OPTUNA_AVAILABLE = True
    print("[INFO] âœ… Optunaä¼˜åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError:
    OPTUNA_AVAILABLE = False
    print("[WARNING] âš ï¸ æœªæ‰¾åˆ°optuna_optimizer.pyï¼Œå°†ä½¿ç”¨æ‰‹åŠ¨å‚æ•°")
    print("[INFO] ä¸‹è½½åœ°å€: è¯·å°†optuna_optimizer.pyæ”¾åœ¨åŒç›®å½•ä¸‹")

# =====================================================================================
# âœ¨ NEW: Optunaä¼˜åŒ–é…ç½®ï¼ˆå…¨å±€å¼€å…³ï¼‰
# =====================================================================================
USE_OPTUNA_OPTIMIZATION = False  # ğŸ”„ è®¾ç½®ä¸ºFalseåˆ™ä½¿ç”¨åŸæœ‰æ‰‹åŠ¨å‚æ•°
OPTUNA_CONFIG = {
    'lstm_trials': 20,      # å‡å°‘åˆ°20æ¬¡
    'gru_trials': 20,
    'xgb_trials': 10,
    'timeout_hours': 2,     # æœ€å¤š2å°æ—¶
    'enable_pruning': True,
    'verbose': True,
}
FEATURE_ABLATION_CONFIG = {
    'enabled': False,              # æ˜¯å¦å¯ç”¨ç‰¹å¾æ¶ˆè
    'output_dir': './feature_ablation/',
    'show_plots': True,
    'save_latex': True
}

# ä¸­æ–‡æ˜¾ç¤ºè®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - Optunaè‡ªåŠ¨ä¼˜åŒ–ç‰ˆï¼ˆæ•°æ®æ³„éœ²å·²ä¿®å¤ï¼‰".center(100))
else:
    print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - æ‰‹åŠ¨å‚æ•°ç‰ˆï¼ˆæ•°æ®æ³„éœ²å·²ä¿®å¤ï¼‰".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šæ»šåŠ¨çª—å£è¯„ä¼° + ä»…å¯¹åŸºæ¨¡å‹è¿›è¡Œå­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹ + å®Œæ•´è®­ç»ƒæµç¨‹".center(100))
print("=" * 100)


# =====================================================================================
# SECTION 1: å…¨å±€ç§å­å›ºå®š
# =====================================================================================
def set_global_seed(seed=12):
    """å…¨å±€å›ºå®šéšæœºç§å­ - ç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

    try:
        tf.config.experimental.enable_op_determinism()
    except:
        pass

    print(f"âœ… å…¨å±€éšæœºç§å­å·²å›ºå®š: {seed}")


GLOBAL_SEED = 12
set_global_seed(GLOBAL_SEED)

# =====================================================================================
# SECTION 2: è¿‡æ‹Ÿåˆæ£€æµ‹æ¨¡å—ï¼ˆä¿æŒåŸæ ·ï¼‰
# =====================================================================================
class OverfittingDetector:
    """è¿‡æ‹Ÿåˆæ£€æµ‹å™¨ - åŸºäºå­¦ä¹ æ›²çº¿åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, output_dir='./overfitting_analysis/'):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.results = {}

    def sequential_learning_curve(self, model_builder, X_seq, y_seq,
                                  model_name='Model',
                                  train_sizes=None,
                                  epochs=100,
                                  batch_size=32):
        """ä¸ºåºåˆ—æ¨¡å‹ï¼ˆLSTM/GRUï¼‰ç”Ÿæˆå­¦ä¹ æ›²çº¿"""
        print(f"\n{'=' * 80}")
        print(f"å­¦ä¹ æ›²çº¿åˆ†æ: {model_name}".center(80))
        print(f"{'=' * 80}")

        if train_sizes is None:
            train_sizes = np.linspace(0.1, 1.0, 10)

        val_split_idx = int(len(X_seq) * 0.8)
        X_train_pool = X_seq[:val_split_idx]
        y_train_pool = y_seq[:val_split_idx]
        X_val = X_seq[val_split_idx:]
        y_val = y_seq[val_split_idx:]

        train_scores = []
        val_scores = []
        train_losses = []
        val_losses = []
        sample_counts = []

        for idx, train_size in enumerate(train_sizes):
            tf.random.set_seed(GLOBAL_SEED + idx)
            np.random.seed(GLOBAL_SEED + idx)

            if train_size <= 1.0:
                n_samples = int(len(X_train_pool) * train_size)
            else:
                n_samples = int(train_size)

            n_samples = max(n_samples, batch_size)

            print(f"è®­ç»ƒæ ·æœ¬æ•°: {n_samples}/{len(X_train_pool)}", end=' ')

            X_train_subset = X_train_pool[:n_samples]
            y_train_subset = y_train_pool[:n_samples]

            model = model_builder((X_seq.shape[1], X_seq.shape[2]))
            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

            early_stop = EarlyStopping(monitor='val_loss', patience=15,
                                       restore_best_weights=True, verbose=0)

            history = model.fit(
                X_train_subset, y_train_subset,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[early_stop],
                shuffle=False,
                verbose=0
            )

            train_pred = model.predict(X_train_subset, verbose=0).flatten()
            val_pred = model.predict(X_val, verbose=0).flatten()

            train_mse = mean_squared_error(y_train_subset, train_pred)
            val_mse = mean_squared_error(y_val, val_pred)
            train_r2 = r2_score(y_train_subset, train_pred)
            val_r2 = r2_score(y_val, val_pred)

            train_scores.append(train_r2)
            val_scores.append(val_r2)
            train_losses.append(train_mse)
            val_losses.append(val_mse)
            sample_counts.append(n_samples)

            print(f"â†’ è®­ç»ƒR^2={train_r2:.4f}, éªŒè¯R^2={val_r2:.4f}, å·®è·={train_r2 - val_r2:.4f}")

        self.results[model_name] = {
            'sample_counts': sample_counts,
            'train_scores': train_scores,
            'val_scores': val_scores,
            'train_losses': train_losses,
            'val_losses': val_losses
        }

        self._diagnose_overfitting(model_name, train_scores, val_scores)
        return sample_counts, train_scores, val_scores, train_losses, val_losses

    def _diagnose_overfitting(self, model_name, train_scores, val_scores):
        """è¯Šæ–­è¿‡æ‹ŸåˆçŠ¶æ€"""
        print(f"\n{'=' * 80}")
        print(f"è¿‡æ‹Ÿåˆè¯Šæ–­: {model_name}".center(80))
        print(f"{'=' * 80}")

        final_gap = train_scores[-1] - val_scores[-1]

        if len(train_scores) >= 3:
            train_trend = train_scores[-1] - train_scores[-3]
            val_trend = val_scores[-1] - val_scores[-3]
        else:
            train_trend = train_scores[-1] - train_scores[0]
            val_trend = val_scores[-1] - val_scores[0]

        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  - æœ€ç»ˆè®­ç»ƒR^2: {train_scores[-1]:.4f}")
        print(f"  - æœ€ç»ˆéªŒè¯R^2: {val_scores[-1]:.4f}")
        print(f"  - R^2å·®è·: {final_gap:.4f}")
        print(f"  - è®­ç»ƒè¶‹åŠ¿: {train_trend:+.4f}")
        print(f"  - éªŒè¯è¶‹åŠ¿: {val_trend:+.4f}")

        print(f"\nğŸ” è¯Šæ–­ç»“è®º:")

        if final_gap > 0.2:
            print(f"  âš ï¸  ä¸¥é‡è¿‡æ‹Ÿåˆ (å·®è·>0.2)")
            diagnosis = "ä¸¥é‡è¿‡æ‹Ÿåˆ"
        elif final_gap > 0.1:
            print(f"  âš ï¸  ä¸­åº¦è¿‡æ‹Ÿåˆ (å·®è·>0.1)")
            diagnosis = "ä¸­åº¦è¿‡æ‹Ÿåˆ"
        elif final_gap > 0.05:
            print(f"  âš¡ è½»å¾®è¿‡æ‹Ÿåˆ (å·®è·>0.05)")
            diagnosis = "è½»å¾®è¿‡æ‹Ÿåˆ"
        else:
            print(f"  âœ… æ‹Ÿåˆè‰¯å¥½ (å·®è·<0.05)")
            diagnosis = "æ‹Ÿåˆè‰¯å¥½"

        if val_scores[-1] < 0.5:
            print(f"  âš ï¸  å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ (éªŒè¯R^2<0.5)")
            diagnosis += " + æ¬ æ‹Ÿåˆ"

        if abs(val_trend) < 0.01:
            print(f"  âœ… æ¨¡å‹å·²æ”¶æ•›")
        else:
            print(f"  âš¡ æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šæ•°æ®")

        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if final_gap > 0.1:
            print(f"  - å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦")
            print(f"  - å‡å°‘æ¨¡å‹å¤æ‚åº¦")
            print(f"  - å¢åŠ Dropoutæ¯”ä¾‹")
            print(f"  - ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®")
        elif val_scores[-1] < 0.5:
            print(f"  - å¢åŠ æ¨¡å‹å¤æ‚åº¦")
            print(f"  - å¢åŠ è®­ç»ƒè½®æ•°")
            print(f"  - ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–")
        else:
            print(f"  - æ¨¡å‹çŠ¶æ€è‰¯å¥½ï¼Œå¯æŠ•å…¥ä½¿ç”¨")

        self.results[model_name]['diagnosis'] = diagnosis
        self.results[model_name]['final_gap'] = final_gap

    def plot_all_learning_curves(self, figsize=(20, 8)):
        """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„å­¦ä¹ æ›²çº¿å¯¹æ¯”"""
        n_models = len(self.results)
        if n_models == 0:
            print("æ²¡æœ‰å¯ç»˜åˆ¶çš„ç»“æœï¼")
            return

        fig, axes = plt.subplots(1, 2, figsize=figsize)

        for idx, (model_name, result) in enumerate(self.results.items()):
            if idx >= 2:
                break

            ax = axes[idx]

            sample_counts = result['sample_counts']
            train_scores = result['train_scores']
            val_scores = result['val_scores']
            diagnosis = result.get('diagnosis', 'Unknown')
            final_gap = result.get('final_gap', 0)

            ax.plot(sample_counts, train_scores, 'o-',
                    linewidth=2.5, markersize=8,
                    label='è®­ç»ƒR^2', color='#2E86AB', alpha=0.8)
            ax.plot(sample_counts, val_scores, 's-',
                    linewidth=2.5, markersize=8,
                    label='éªŒè¯R^2', color='#A23B72', alpha=0.8)

            ax.axhline(y=train_scores[-1], color='#2E86AB',
                       linestyle='--', alpha=0.3, linewidth=1)
            ax.axhline(y=val_scores[-1], color='#A23B72',
                       linestyle='--', alpha=0.3, linewidth=1)

            ax.fill_between(sample_counts, train_scores, val_scores,
                            alpha=0.2, color='red' if final_gap > 0.1 else 'green')

            ax.set_title(f'{model_name}\n{diagnosis} (å·®è·={final_gap:.4f})',
                         fontsize=13, fontweight='bold')
            ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°', fontsize=11)
            ax.set_ylabel('R^2 Score', fontsize=11)
            ax.legend(loc='best', fontsize=10)
            ax.grid(True, alpha=0.3)

            ax.text(sample_counts[-1], train_scores[-1],
                    f'{train_scores[-1]:.3f}',
                    fontsize=9, ha='right', va='bottom', color='#2E86AB')
            ax.text(sample_counts[-1], val_scores[-1],
                    f'{val_scores[-1]:.3f}',
                    fontsize=9, ha='right', va='top', color='#A23B72')

        plt.suptitle('åŸºæ¨¡å‹å­¦ä¹ æ›²çº¿åˆ†æ - è¿‡æ‹Ÿåˆè¯Šæ–­',
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}learning_curves_base_models.png',
                    dpi=300, bbox_inches='tight')
        plt.show()

        print(f"\nâœ… å­¦ä¹ æ›²çº¿å¯¹æ¯”å›¾å·²ä¿å­˜: {self.output_dir}learning_curves_base_models.png")

    def plot_loss_curves(self, figsize=(20, 6)):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        n_models = len(self.results)
        if n_models == 0:
            return

        fig, axes = plt.subplots(1, 2, figsize=figsize)

        for idx, (model_name, result) in enumerate(self.results.items()):
            if idx >= 2:
                break

            ax = axes[idx]

            sample_counts = result['sample_counts']
            train_losses = result['train_losses']
            val_losses = result['val_losses']

            ax.plot(sample_counts, train_losses, 'o-',
                    linewidth=2.5, label='è®­ç»ƒæŸå¤±', color='#F18F01')
            ax.plot(sample_counts, val_losses, 's-',
                    linewidth=2.5, label='éªŒè¯æŸå¤±', color='#C73E1D')

            ax.set_title(f'{model_name} - MSEæŸå¤±',
                         fontsize=13, fontweight='bold')
            ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°', fontsize=11)
            ax.set_ylabel('MSE Loss', fontsize=11)
            ax.legend(loc='best', fontsize=10)
            ax.grid(True, alpha=0.3)
            ax.set_yscale('log')

        plt.suptitle('åŸºæ¨¡å‹è®­ç»ƒä¸éªŒè¯æŸå¤±æ›²çº¿',
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}loss_curves_base_models.png',
                    dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… æŸå¤±æ›²çº¿å›¾å·²ä¿å­˜: {self.output_dir}loss_curves_base_models.png")

    def generate_report(self):
        """ç”Ÿæˆè¿‡æ‹Ÿåˆè¯Šæ–­æŠ¥å‘Š"""
        print(f"\n{'=' * 80}")
        print("åŸºæ¨¡å‹è¿‡æ‹Ÿåˆè¯Šæ–­æ€»ç»“æŠ¥å‘Š".center(80))
        print(f"{'=' * 80}\n")

        report_data = []
        for model_name, result in self.results.items():
            report_data.append({
                'æ¨¡å‹': model_name,
                'æœ€ç»ˆè®­ç»ƒR^2': result['train_scores'][-1],
                'æœ€ç»ˆéªŒè¯R^2': result['val_scores'][-1],
                'R^2å·®è·': result['final_gap'],
                'è¯Šæ–­ç»“æœ': result['diagnosis']
            })

        df = pd.DataFrame(report_data)
        df = df.sort_values('R^2å·®è·', ascending=False)

        print(df.to_string(index=False))

        df.to_csv(f'{self.output_dir}overfitting_report.csv', index=False)
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {self.output_dir}overfitting_report.csv")

        print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - åˆ†ææ¨¡å‹æ•°: {len(self.results)}")
        print(f"  - æ‹Ÿåˆè‰¯å¥½: {len([r for r in report_data if r['R^2å·®è·'] < 0.05])}")
        print(f"  - è½»å¾®è¿‡æ‹Ÿåˆ: {len([r for r in report_data if 0.05 <= r['R^2å·®è·'] < 0.1])}")
        print(f"  - ä¸­åº¦è¿‡æ‹Ÿåˆ: {len([r for r in report_data if 0.1 <= r['R^2å·®è·'] < 0.2])}")
        print(f"  - ä¸¥é‡è¿‡æ‹Ÿåˆ: {len([r for r in report_data if r['R^2å·®è·'] >= 0.2])}")

        print(f"\nğŸ’¡ å…³é”®ç»“è®º:")
        print(f"  - ç­–ç•¥æ¨¡å‹ï¼ˆXGBoostï¼‰ä»…å­¦ä¹ æ®‹å·®ï¼Œå½±å“æœ‰é™")
        print(f"  - åŸºæ¨¡å‹è¿‡æ‹Ÿåˆæ˜¯ä¸»è¦é£é™©ï¼Œéœ€é‡ç‚¹å…³æ³¨")
        print(f"  - å¦‚åŸºæ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°åé‡æ–°è®­ç»ƒ")


# =====================================================================================
# SECTION 3: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================================================================================
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# =====================================================================================
# âœ¨ FIXED SECTION 4: ç‰¹å¾å·¥ç¨‹ï¼ˆä»…å¤„ç†è®­ç»ƒé›†ï¼‰
# =====================================================================================
def add_features(X, y):
    """æ·»åŠ æ»åç‰¹å¾ï¼ˆä»…ç”¨äºè®­ç»ƒé›†ï¼‰"""
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


print(f"\n{'=' * 80}")
print("ã€ç‰¹å¾å·¥ç¨‹ - ä»…å¤„ç†è®­ç»ƒé›†ã€‘".center(80))
print(f"{'=' * 80}")

X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

print(f"\nâœ“ è®­ç»ƒé›†ç‰¹å¾æ„é€ :")
print(f"  - åŸå§‹æ ·æœ¬æ•°: {len(X_train)}")
print(f"  - æ·»åŠ æ»åç‰¹å¾å: {len(X_train_feat)}")
print(f"  - ç‰¹å¾ç»´åº¦: {X_train_feat.shape[1]}")

print(f"\nâœ“ æµ‹è¯•é›†å¤„ç†ç­–ç•¥:")
print(f"  - ä¸é¢„å…ˆæ„é€ ç‰¹å¾")
print(f"  - å°†åœ¨æ»šåŠ¨çª—å£é¢„æµ‹æ—¶åŠ¨æ€æ„é€ ")
print(f"  - å®Œå…¨é¿å…æ•°æ®æ³„éœ²")


# æ„é€ è®­ç»ƒé›†åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)

print(f"\nâœ“ è®­ç»ƒé›†åºåˆ—æ„é€ :")
print(f"  - åºåˆ—å½¢çŠ¶: {X_train_seq.shape}")
print(f"  - ç›®æ ‡å½¢çŠ¶: {y_train_seq.shape}")


# =====================================================================================
# âœ¨ NEW: æ»šåŠ¨çª—å£é¢„æµ‹å‡½æ•°
# =====================================================================================
def rolling_window_predict(model, X_train_feat, y_train, X_test, y_test, seq_len=5):
    """
    æ»šåŠ¨çª—å£é¢„æµ‹å‡½æ•°ï¼ˆç”¨äºLSTM/GRUï¼‰

    å…³é”®é€»è¾‘ï¼š
    1. åˆå§‹åŒ–å†å²çª—å£ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
    2. é€ä¸ªé¢„æµ‹æµ‹è¯•æ ·æœ¬
    3. æ¯æ¬¡é¢„æµ‹åç”¨çœŸå®å€¼æ›´æ–°å†å²
    """
    predictions = []

    # åˆå§‹åŒ–ï¼šä»è®­ç»ƒé›†è·å–å†å²çª—å£
    history_X_feat = X_train_feat.tail(seq_len).copy()
    history_y = y_train.tail(5).copy()

    # é€ä¸ªé¢„æµ‹æµ‹è¯•æ ·æœ¬
    for idx in range(len(X_test)):
        # 1. è·å–å½“å‰æµ‹è¯•æ ·æœ¬çš„åŸå§‹ç‰¹å¾ï¼ˆä¸å¸¦lagï¼‰
        current_X_raw = X_test.iloc[idx:idx+1].copy()

        # 2. åŠ¨æ€æ„é€ lagç‰¹å¾ï¼ˆåªç”¨å·²çŸ¥å†å²ï¼‰
        for i in range(1, 6):
            if len(history_y) >= i:
                current_X_raw[f'Corn_lag_{i}'] = history_y.iloc[-i]
            else:
                current_X_raw[f'Corn_lag_{i}'] = 0

        # 3. æ„é€ åºåˆ—çª—å£
        current_window = pd.concat([history_X_feat.tail(seq_len-1), current_X_raw])
        X_seq = current_window.values.reshape(1, seq_len, -1)

        # 4. é¢„æµ‹
        pred = model.predict(X_seq, verbose=0)[0, 0]
        predictions.append(pred)

        # 5. âœ… ç”¨çœŸå®å€¼æ›´æ–°å†å²ï¼ˆå…³é”®ï¼šè¿™æ˜¯é¢„æµ‹åæ‰çŸ¥é“çš„ï¼‰
        history_X_feat = pd.concat([history_X_feat.iloc[1:], current_X_raw])
        history_y = pd.concat([history_y.iloc[1:], pd.Series([y_test.iloc[idx]])])

        # è¿›åº¦æ˜¾ç¤º
        if (idx + 1) % 50 == 0 or idx == len(X_test) - 1:
            print(f"  è¿›åº¦: {idx+1}/{len(X_test)} æ ·æœ¬", end='\r')

    print()  # æ¢è¡Œ
    return np.array(predictions)


def rolling_window_predict_ensemble(lstm_model, gru_model, xgb_model,
                                     X_train_feat, y_train, X_test, y_test,
                                     X_train_flat, seq_len=5, strategy='gru_residual'):
    """
    é›†æˆæ¨¡å‹æ»šåŠ¨çª—å£é¢„æµ‹
    """
    predictions = []
    lstm_preds_list = []
    gru_preds_list = []

    # åˆå§‹åŒ–å†å²çª—å£
    history_X_feat = X_train_feat.tail(seq_len).copy()
    history_y = y_train.tail(5).copy()

    # é€ä¸ªé¢„æµ‹
    for idx in range(len(X_test)):
        # æ„é€ å½“å‰æ ·æœ¬ç‰¹å¾
        current_X_raw = X_test.iloc[idx:idx+1].copy()

        for i in range(1, 6):
            if len(history_y) >= i:
                current_X_raw[f'Corn_lag_{i}'] = history_y.iloc[-i]
            else:
                current_X_raw[f'Corn_lag_{i}'] = 0

        # æ„é€ åºåˆ—çª—å£
        current_window = pd.concat([history_X_feat.tail(seq_len-1), current_X_raw])
        X_seq = current_window.values.reshape(1, seq_len, -1)

        # è·å–LSTMå’ŒGRUé¢„æµ‹
        lstm_pred = lstm_model.predict(X_seq, verbose=0)[0, 0]
        gru_pred = gru_model.predict(X_seq, verbose=0)[0, 0]

        lstm_preds_list.append(lstm_pred)
        gru_preds_list.append(gru_pred)

        # æ ¹æ®ç­–ç•¥è®¡ç®—æœ€ç»ˆé¢„æµ‹
        if strategy == 'average':
            final_pred = (lstm_pred + gru_pred) / 2

        elif strategy == 'lstm_residual':
            # æ„é€ XGBoostç‰¹å¾
            X_flat = X_seq.reshape(1, -1)
            X_xgb = np.hstack([
                X_flat,
                np.array([[lstm_pred, gru_pred, (lstm_pred + gru_pred) / 2,
                          abs(lstm_pred - gru_pred)]])
            ])
            residual = xgb_model.predict(X_xgb)[0]
            final_pred = lstm_pred + residual

        elif strategy == 'gru_residual':
            X_flat = X_seq.reshape(1, -1)
            X_xgb = np.hstack([
                X_flat,
                np.array([[lstm_pred, gru_pred, (lstm_pred + gru_pred) / 2,
                          abs(lstm_pred - gru_pred)]])
            ])
            residual = xgb_model.predict(X_xgb)[0]
            final_pred = gru_pred + residual

        else:
            final_pred = (lstm_pred + gru_pred) / 2

        predictions.append(final_pred)

        # æ›´æ–°å†å²
        history_X_feat = pd.concat([history_X_feat.iloc[1:], current_X_raw])
        history_y = pd.concat([history_y.iloc[1:], pd.Series([y_test.iloc[idx]])])

        if (idx + 1) % 50 == 0 or idx == len(X_test) - 1:
            print(f"  è¿›åº¦: {idx+1}/{len(X_test)} æ ·æœ¬", end='\r')

    print()
    return np.array(predictions), np.array(lstm_preds_list), np.array(gru_preds_list)


# =====================================================================================
# âœ¨ NEW SECTION: Optunaè¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µ
# =====================================================================================

if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print("\n" + "=" * 100)
    print("ã€Optunaè¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µã€‘".center(100))
    print("=" * 100)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = OptunaOptimizer(
        X_train=X_train_seq,
        y_train=y_train_seq,
        output_dir='./optuna_results/',
        seed=GLOBAL_SEED,
        verbose=OPTUNA_CONFIG['verbose']
    )

    # ä¼˜åŒ–LSTM
    print(f"\n[1/3] ä¼˜åŒ–LSTMè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['lstm_trials']})...")
    best_lstm_params = optimizer.optimize_lstm(
        n_trials=OPTUNA_CONFIG['lstm_trials'],
        max_epochs=200,
        batch_size=32,
        enable_pruning=OPTUNA_CONFIG['enable_pruning']
    )

    # ä¼˜åŒ–GRU
    print(f"\n[2/3] ä¼˜åŒ–GRUè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['gru_trials']})...")
    best_gru_params = optimizer.optimize_gru(
        n_trials=OPTUNA_CONFIG['gru_trials'],
        max_epochs=200,
        batch_size=32,
        enable_pruning=OPTUNA_CONFIG['enable_pruning']
    )

    # ä¼˜åŒ–XGBoost
    print(f"\n[3/3] ä¼˜åŒ–XGBoostè¶…å‚æ•° (n_trials={OPTUNA_CONFIG['xgb_trials']})...")
    best_xgb_params = optimizer.optimize_xgboost(
        n_trials=OPTUNA_CONFIG['xgb_trials'],
        cv_splits=5
    )

    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå’Œå¯è§†åŒ–
    try:
        optimizer.visualize('lstm', show=False)
        optimizer.visualize('gru', show=False)
        optimizer.visualize('xgboost', show=False)
    except Exception as e:
        print(f"[WARNING] å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

    optimizer.generate_report()

    print("\n" + "=" * 100)
    print("âœ… Optunaä¼˜åŒ–å®Œæˆï¼".center(100))
    print(f"ä¼˜åŒ–ç»“æœä¿å­˜åœ¨: ./optuna_results/".center(100))
    print("=" * 100)

else:
    # ä½¿ç”¨åŸæœ‰æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°
    print("\n" + "=" * 100)
    print("ã€ä½¿ç”¨æ‰‹åŠ¨è®¾ç½®å‚æ•°ã€‘".center(100))
    print("=" * 100)

    best_lstm_params = {
        'units': 150,
        'dropout': 0.2462968579989427,
        'recurrent_dropout': 0.040227669708937194,
        'l2_reg': 0.0006181065539453701,
        'learning_rate': 0.0002295600141013699
    }

    best_gru_params = {
        'units': 150,
        'dropout': 0.41579437412891573,
        'recurrent_dropout': 0.066600650933163,
        'learning_rate': 0.0002070950616082725
    }

    best_xgb_params = {
        'n_estimators': 91,
        'learning_rate': 0.0487307213710181,
        'max_depth': 5,
        'min_child_weight': 10,
        'subsample': 0.5784746899524377,
        'colsample_bytree': 0.7083156395264648,
        'reg_alpha': 0.02939013756116536,
        'reg_lambda': 0.010506850510122958
    }

    print("\nå½“å‰ä½¿ç”¨çš„è¶…å‚æ•°:")
    print(f"LSTM: {best_lstm_params}")
    print(f"GRU: {best_gru_params}")
    print(f"XGBoost: {best_xgb_params}")


# =====================================================================================
# SECTION 5: æ¨¡å‹æ„å»ºå™¨ï¼ˆç”¨äºè¿‡æ‹Ÿåˆæ£€æµ‹ï¼‰
# =====================================================================================
def build_final_lstm(input_shape):
    """æœ€ç»ˆLSTMæ¨¡å‹"""
    tf.random.set_seed(GLOBAL_SEED)
    return Sequential([
        layers.LSTM(
            units=best_lstm_params['units'],
            input_shape=input_shape,
            kernel_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
            recurrent_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
            recurrent_dropout=best_lstm_params.get('recurrent_dropout', 0.0),
            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
            recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
        ),
        layers.Dropout(best_lstm_params['dropout'], seed=GLOBAL_SEED),
        layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
    ])


def build_final_gru(input_shape):
    """æœ€ç»ˆGRUæ¨¡å‹"""
    tf.random.set_seed(GLOBAL_SEED)
    return Sequential([
        layers.GRU(
            units=best_gru_params['units'],
            input_shape=input_shape,
            recurrent_dropout=best_gru_params.get('recurrent_dropout', 0.0),
            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
            recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
        ),
        layers.Dropout(best_gru_params['dropout'], seed=GLOBAL_SEED),
        layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
    ])


# =====================================================================================
# SECTION 6: OOFé¢„æµ‹ç”Ÿæˆï¼ˆè®­ç»ƒé›†å†…éƒ¨ï¼Œæ— æ•°æ®æ³„éœ²ï¼‰
# =====================================================================================
def get_oof_predictions(X_seq, y_seq, params, model_type='lstm', n_splits=5):
    """ç”ŸæˆOOFé¢„æµ‹"""
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")

    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        tf.random.set_seed(GLOBAL_SEED + fold)
        np.random.seed(GLOBAL_SEED + fold)

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = Sequential([
                layers.LSTM(
                    units=params['units'],
                    input_shape=(X_seq.shape[1], X_seq.shape[2]),
                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                    recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
                ),
                layers.Dropout(params['dropout'], seed=GLOBAL_SEED),
                layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
            ])
        else:
            model = Sequential([
                layers.GRU(
                    units=params['units'],
                    input_shape=(X_seq.shape[1], X_seq.shape[2]),
                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                    recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
                ),
                layers.Dropout(params['dropout'], seed=GLOBAL_SEED),
                layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
            ])

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            shuffle=False,
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()

        fold += 1

    return oof_preds


print("\n" + "=" * 100)
print("ã€é˜¶æ®µ1ã€‘ç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(100))
print("=" * 100)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, best_lstm_params, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, best_gru_params, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF R^2: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF R^2: {r2_score(y_train_seq, gru_oof_preds):.4f}")


# =====================================================================================
# SECTION 7: è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ2ã€‘è®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä¼˜å‚æ•°ï¼‰".center(100))
print("=" * 100)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

lstm_final = Sequential([
    layers.LSTM(
        units=best_lstm_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
        recurrent_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
        recurrent_dropout=best_lstm_params.get('recurrent_dropout', 0.0),
        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
        recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
    ),
    layers.Dropout(best_lstm_params['dropout'], seed=GLOBAL_SEED),
    layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_lstm_params['learning_rate']))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    shuffle=False,
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

gru_final = Sequential([
    layers.GRU(
        units=best_gru_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        recurrent_dropout=best_gru_params.get('recurrent_dropout', 0.0),
        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
        recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
    ),
    layers.Dropout(best_gru_params['dropout'], seed=GLOBAL_SEED),
    layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_gru_params['learning_rate']))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    shuffle=False,
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")


# =====================================================================================
# âœ¨ FIXED SECTION 8: ä½¿ç”¨æ»šåŠ¨çª—å£é¢„æµ‹æµ‹è¯•é›†
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ3ã€‘æ»šåŠ¨çª—å£é¢„æµ‹æµ‹è¯•é›†ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰".center(100))
print("=" * 100)

print("\næ»šåŠ¨çª—å£é¢„æµ‹ LSTM...")
lstm_test_pred = rolling_window_predict(
    lstm_final, X_train_feat, y_train, X_test, y_test, seq_len=seq_len
)

print("\næ»šåŠ¨çª—å£é¢„æµ‹ GRU...")
gru_test_pred = rolling_window_predict(
    gru_final, X_train_feat, y_train, X_test, y_test, seq_len=seq_len
)

# ç®€å•å¹³å‡
avg_test_pred = (lstm_test_pred + gru_test_pred) / 2

# è¯„ä¼°åŸºç¡€æ¨¡å‹
lstm_test_r2 = r2_score(y_test, lstm_test_pred)
gru_test_r2 = r2_score(y_test, gru_test_pred)
avg_r2 = r2_score(y_test, avg_test_pred)

print(f"\nã€åŸºç¡€æ¨¡å‹æ€§èƒ½ï¼ˆæ»šåŠ¨çª—å£è¯„ä¼°ï¼‰ã€‘")
print(f"LSTMå•æ¨¡å‹ - æµ‹è¯•R^2: {lstm_test_r2:.4f}")
print(f"GRUå•æ¨¡å‹  - æµ‹è¯•R^2: {gru_test_r2:.4f}")
print(f"ç®€å•å¹³å‡   - æµ‹è¯•R^2: {avg_r2:.4f}")


# =====================================================================================
# SECTION 9: å­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ4ã€‘å¯¹åŸºæ¨¡å‹ï¼ˆLSTM/GRUï¼‰è¿›è¡Œå­¦ä¹ æ›²çº¿è¿‡æ‹Ÿåˆæ£€æµ‹".center(100))
print("ç†ç”±ï¼šåŸºæ¨¡å‹æ˜¯ä¸»è¦é¢„æµ‹æ¥æºï¼Œè¿‡æ‹Ÿåˆä¼šç›´æ¥å½±å“æœ€ç»ˆç»“æœ".center(100))
print("=" * 100)

detector = OverfittingDetector(output_dir='./overfitting_analysis/')

print("\nğŸ” æ£€æµ‹LSTMæœ€ç»ˆæ¨¡å‹...")
detector.sequential_learning_curve(
    model_builder=build_final_lstm,
    X_seq=X_train_seq,
    y_seq=y_train_seq,
    model_name='LSTMæœ€ç»ˆæ¨¡å‹',
    train_sizes=np.linspace(0.2, 1.0, 8),
    epochs=100,
    batch_size=32
)

print("\nğŸ” æ£€æµ‹GRUæœ€ç»ˆæ¨¡å‹...")
detector.sequential_learning_curve(
    model_builder=build_final_gru,
    X_seq=X_train_seq,
    y_seq=y_train_seq,
    model_name='GRUæœ€ç»ˆæ¨¡å‹',
    train_sizes=np.linspace(0.2, 1.0, 8),
    epochs=100,
    batch_size=32
)

detector.plot_all_learning_curves(figsize=(16, 6))
detector.plot_loss_curves(figsize=(16, 5))
detector.generate_report()


# =====================================================================================
# SECTION 10: XGBoostè®­ç»ƒ
# =====================================================================================
def create_simplified_features(X_flat, lstm_preds, gru_preds):
    """ç®€åŒ–ç‰¹å¾ï¼šåŸå§‹ + é¢„æµ‹ + å¹³å‡ + å·®å¼‚"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


def train_xgboost(X_train, y_train):
    """è®­ç»ƒXGBoost"""
    np.random.seed(GLOBAL_SEED)
    model = XGBRegressor(
        **best_xgb_params,
        random_state=GLOBAL_SEED,
        seed=GLOBAL_SEED,
        n_jobs=1,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


print("\n" + "=" * 100)
print("ã€é˜¶æ®µ5ã€‘XGBoostæ®‹å·®å­¦ä¹ ".center(100))
print("=" * 100)

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds
avg_oof_preds = (lstm_oof_preds + gru_oof_preds) / 2
avg_oof_residual = y_train_seq - avg_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# å‡†å¤‡ç‰¹å¾ï¼ˆè®­ç»ƒé›†ï¼‰
X_train_flat = X_train_seq.reshape(len(X_train_seq), -1)
simplified_train = create_simplified_features(X_train_flat, lstm_oof_preds, gru_oof_preds)

print(f"\nç‰¹å¾ç»´åº¦: {simplified_train.shape[1]} ç»´")

# è®­ç»ƒXGBoostæ¨¡å‹
print("\nè®­ç»ƒXGBoost (LSTMæ®‹å·®)...")
xgb_lstm = train_xgboost(simplified_train, lstm_oof_residual)

print("è®­ç»ƒXGBoost (GRUæ®‹å·®)...")
xgb_gru = train_xgboost(simplified_train, gru_oof_residual)

print("è®­ç»ƒXGBoost (åŒæ®‹å·®)...")
xgb_dual = train_xgboost(simplified_train, avg_oof_residual)


# =====================================================================================
# SECTION 11: ç‰¹å¾å·¥ç¨‹ä¸XGBoostè®­ç»ƒï¼ˆè¡¥å……å‡½æ•°ï¼‰
# =====================================================================================
def clip_residual(residual_pred, threshold=2.0):
    """å‰ªè£æç«¯æ®‹å·®å€¼"""
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    """åŠ æƒæ®‹å·®ä¿®æ­£"""
    return base_pred + weight * residual_pred


# =====================================================================================
# âœ¨ FIXED SECTION 12: é›†æˆç­–ç•¥æ»šåŠ¨çª—å£è¯„ä¼°ï¼ˆå…¨éƒ¨6ä¸ªç­–ç•¥ï¼‰
# =====================================================================================
print("\n" + "=" * 100)
print("ã€é˜¶æ®µ6ã€‘é›†æˆç­–ç•¥æ»šåŠ¨çª—å£è¯„ä¼°".center(100))
print("=" * 100)

strategies_results = {}

# ç­–ç•¥1: LSTMæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥1ã€‘LSTMæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + XGBoost".center(100))
print("=" * 100)

print("\næ»šåŠ¨çª—å£é¢„æµ‹...")
pred_lstm_residual, _, _ = rolling_window_predict_ensemble(
    lstm_final, gru_final, xgb_lstm,
    X_train_feat, y_train, X_test, y_test,
    X_train_flat, seq_len=seq_len, strategy='lstm_residual'
)
strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '] = pred_lstm_residual
r2_lstm_residual = r2_score(y_test, pred_lstm_residual)
print(f"âœ“ R^2: {r2_lstm_residual:.4f} (vsç®€å•å¹³å‡: {r2_lstm_residual - avg_r2:+.4f})")

# ç­–ç•¥2: GRUæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥2ã€‘GRUæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + XGBoost".center(100))
print("=" * 100)

print("\næ»šåŠ¨çª—å£é¢„æµ‹...")
pred_gru_residual, _, _ = rolling_window_predict_ensemble(
    lstm_final, gru_final, xgb_gru,
    X_train_feat, y_train, X_test, y_test,
    X_train_flat, seq_len=seq_len, strategy='gru_residual'
)
strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '] = pred_gru_residual
r2_gru_residual = r2_score(y_test, pred_gru_residual)
print(f"âœ“ R^2: {r2_gru_residual:.4f} (vsç®€å•å¹³å‡: {r2_gru_residual - avg_r2:+.4f})")

# ç­–ç•¥3: åŒæ®‹å·®å­¦ä¹ 
print("\n" + "=" * 100)
print("ã€ç­–ç•¥3ã€‘åŒæ®‹å·®å­¦ä¹ ï¼š(LSTM+GRU)/2 + XGBoost".center(100))
print("=" * 100)

print("\næ»šåŠ¨çª—å£é¢„æµ‹...")
# åŒæ®‹å·®é¢„æµ‹éœ€è¦å•ç‹¬å®ç°ï¼Œå› ä¸ºåŸºå‡†æ˜¯å¹³å‡å€¼
pred_dual_list = []
history_X_feat = X_train_feat.tail(seq_len).copy()
history_y = y_train.tail(5).copy()

for idx in range(len(X_test)):
    current_X_raw = X_test.iloc[idx:idx+1].copy()
    for i in range(1, 6):
        current_X_raw[f'Corn_lag_{i}'] = history_y.iloc[-i] if len(history_y) >= i else 0

    current_window = pd.concat([history_X_feat.tail(seq_len-1), current_X_raw])
    X_seq = current_window.values.reshape(1, seq_len, -1)

    # è·å–LSTMå’ŒGRUé¢„æµ‹
    lstm_pred = lstm_final.predict(X_seq, verbose=0)[0, 0]
    gru_pred = gru_final.predict(X_seq, verbose=0)[0, 0]
    avg_pred = (lstm_pred + gru_pred) / 2

    # æ„é€ XGBoostç‰¹å¾å¹¶é¢„æµ‹æ®‹å·®
    X_flat = X_seq.reshape(1, -1)
    X_xgb = np.hstack([X_flat, np.array([[lstm_pred, gru_pred, avg_pred, abs(lstm_pred - gru_pred)]])])

    residual = xgb_dual.predict(X_xgb)[0]
    final_pred = avg_pred + residual
    pred_dual_list.append(final_pred)

    history_X_feat = pd.concat([history_X_feat.iloc[1:], current_X_raw])
    history_y = pd.concat([history_y.iloc[1:], pd.Series([y_test.iloc[idx]])])

    if (idx + 1) % 50 == 0 or idx == len(X_test) - 1:
        print(f"  è¿›åº¦: {idx+1}/{len(X_test)} æ ·æœ¬", end='\r')

print()
pred_dual = np.array(pred_dual_list)
strategies_results['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '] = pred_dual
r2_dual = r2_score(y_test, pred_dual)
print(f"âœ“ R^2: {r2_dual:.4f} (vsç®€å•å¹³å‡: {r2_dual - avg_r2:+.4f})")

# ç­–ç•¥4: æ®‹å·®å‰ªè£
print("\n" + "=" * 100)
print("ã€ç­–ç•¥4ã€‘GRUæ®‹å·®å­¦ä¹  + æ®‹å·®å‰ªè£".center(100))
print("=" * 100)

# ä½¿ç”¨ç­–ç•¥2çš„é¢„æµ‹ï¼Œè®¡ç®—æ®‹å·®å¹¶å‰ªè£
gru_residual_test = pred_gru_residual - gru_test_pred
gru_residual_clipped = clip_residual(gru_residual_test, threshold=2.0)
pred_gru_clipped = gru_test_pred + gru_residual_clipped

r2_gru_clipped = r2_score(y_test, pred_gru_clipped)
print(f"âœ“ R^2: {r2_gru_clipped:.4f} (vsç®€å•å¹³å‡: {r2_gru_clipped - avg_r2:+.4f})")
print(f"  æ®‹å·®å‰ªè£å‰std: {np.std(gru_residual_test):.6f}, å‰ªè£åstd: {np.std(gru_residual_clipped):.6f}")
strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'] = pred_gru_clipped

# ç­–ç•¥5: åŠ æƒèåˆï¼ˆ30%ï¼‰
print("\n" + "=" * 100)
print("ã€ç­–ç•¥5ã€‘GRUæ®‹å·®å­¦ä¹  + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_gru_weighted = weighted_residual_correction(gru_test_pred, gru_residual_test, weight=0.3)

r2_gru_weighted = r2_score(y_test, pred_gru_weighted)
print(f"âœ“ R^2: {r2_gru_weighted:.4f} (vsç®€å•å¹³å‡: {r2_gru_weighted - avg_r2:+.4f})")
strategies_results['ç­–ç•¥5-åŠ æƒèåˆ30%'] = pred_gru_weighted

# ç­–ç•¥6: ç»ˆæç»„åˆ
print("\n" + "=" * 100)
print("ã€ç­–ç•¥6ã€‘ç»ˆæç»„åˆï¼šæ®‹å·®å‰ªè£ + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_ultimate = weighted_residual_correction(gru_test_pred, gru_residual_clipped, weight=0.3)

r2_ultimate = r2_score(y_test, pred_ultimate)
print(f"âœ“ R^2: {r2_ultimate:.4f} (vsç®€å•å¹³å‡: {r2_ultimate - avg_r2:+.4f})")
strategies_results['ç­–ç•¥6-ç»ˆæç»„åˆ'] = pred_ultimate


# =====================================================================================
# SECTION 13: ç»¼åˆå¯¹æ¯”
# =====================================================================================
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

all_strategies = {
    'LSTMå•æ¨¡å‹': lstm_test_pred,
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç®€å•å¹³å‡(åŸºçº¿)': avg_test_pred,
    **strategies_results
}

print(f"\n{'ç­–ç•¥':<30} {'R^2':>10} {'vsåŸºçº¿':>10} {'MAE':>12} {'RMSE':>12}")
print("-" * 75)

results_list = []
for name, pred in all_strategies.items():
    r2 = r2_score(y_test, pred)
    mae = mean_absolute_error(y_test, pred)
    rmse = sqrt(mean_squared_error(y_test, pred))
    improvement = r2 - avg_r2
    results_list.append((name, r2, improvement, mae, rmse, pred))
    print(f"{name:<30} {r2:>10.4f} {improvement:>10.4f} {mae:>12.6f} {rmse:>12.6f}")

# æ’åº
results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’åï¼ˆæŒ‰R^2é™åºï¼‰".center(100))
print("=" * 100)

best_r2 = results_list[0][1]
best_name = results_list[0][0]

for rank, (name, r2, improvement, mae, rmse, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<30} R^2={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_name} (R^2 = {best_r2:.4f})")

# åŸå§‹å°ºåº¦è¯„ä¼°
print("\n" + "=" * 100)
print("åŸå§‹å°ºåº¦æ€§èƒ½å¯¹æ¯”".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test.values.reshape(-1, 1))
strategies_original = {}

print(f"\n{'ç­–ç•¥':<30} {'R^2':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 75)

for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    strategies_original[name] = pred_original

    r2 = r2_score(y_test_original, pred_original)
    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))
    mape = np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))

    print(f"{name:<30} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")


# =====================================================================================
# SECTION 14: å®Œæ•´å¯è§†åŒ–ï¼ˆä¿ç•™åŸä»£ç æ‰€æœ‰å›¾è¡¨ï¼‰
# =====================================================================================
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

print("\n" + "=" * 100)
print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨".center(100))
print("=" * 100)

# å›¾1: è®­ç»ƒè¿‡ç¨‹
fig = plt.figure(figsize=(16, 5))

plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(lstm_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('LSTMæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.plot(gru_history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
plt.title('GRUæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '01_training_process.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾1: è®­ç»ƒè¿‡ç¨‹æ›²çº¿")

# å›¾2: æ€§èƒ½æ’åæŸ±çŠ¶å›¾
fig, ax = plt.subplots(figsize=(16, 8))

strategy_names = [name for name, _, _, _, _, _ in results_list]
r2_scores = [r2 for _, r2, _, _, _, _ in results_list]
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(strategy_names)))

bars = ax.barh(range(len(strategy_names)), r2_scores, color=colors, alpha=0.8)

ax.axvline(x=avg_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

for i, (bar, r2, improvement) in enumerate(zip(bars, r2_scores, [imp for _, _, imp, _, _, _ in results_list])):
    label = f'{r2:.4f}'
    if improvement > 0:
        label += f' (+{improvement:.4f})'
        color = 'green'
    elif improvement < 0:
        label += f' ({improvement:.4f})'
        color = 'red'
    else:
        color = 'black'

    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(strategy_names)))
ax.set_yticklabels(strategy_names, fontsize=10)
ax.set_xlabel('R^2 Score', fontsize=12, fontweight='bold')
ax.set_title('æ‰€æœ‰ç­–ç•¥æ€§èƒ½æ’åå¯¹æ¯”ï¼ˆæ»šåŠ¨çª—å£è¯„ä¼°ï¼‰', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(results_directory + '02_performance_ranking.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾2: æ€§èƒ½æ’åå¯¹æ¯”")

# å›¾3: Top6ç­–ç•¥é¢„æµ‹å¯¹æ¯”
fig, axes = plt.subplots(3, 2, figsize=(18, 15))
axes = axes.flatten()

top6_strategies = results_list[:6]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top6_strategies):
    ax = axes[idx]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    r2_original = r2_score(y_test_original, pred_original)

    ax.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax.plot(pred_original, label=name, linewidth=2, alpha=0.8)
    ax.set_title(f'{name}\nR^2={r2_original:.4f} (vsåŸºçº¿: {improvement:+.4f})',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('æ ·æœ¬åºå·', fontsize=9)
    ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '03_top6_strategies_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾3: Top6ç­–ç•¥é¢„æµ‹å¯¹æ¯”")

# å›¾4: æ®‹å·®åˆ†æå¯¹æ¯”
fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # ğŸ”§ æ”¹ä¸º 2x4 å¸ƒå±€
axes = axes.flatten()  # ğŸ”§ å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ï¼Œæ–¹ä¾¿ç´¢å¼•

residuals_dict = {
    'ç®€å•å¹³å‡': y_test - avg_test_pred,
    'ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ': y_test - strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ': y_test - strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ ': y_test - strategies_results['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '],
    'ç­–ç•¥4-æ®‹å·®å‰ªè£': y_test - strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'],
    'ç­–ç•¥5-åŠ æƒèåˆ30%': y_test - strategies_results['ç­–ç•¥5-åŠ æƒèåˆ30%'],
    'ç­–ç•¥6-ç»ˆæç»„åˆ': y_test - strategies_results['ç­–ç•¥6-ç»ˆæç»„åˆ'],
}

for idx, (name, residual) in enumerate(residuals_dict.items()):
    ax = axes[idx]  # ğŸ”§ ç›´æ¥ç”¨ä¸€ç»´ç´¢å¼•
    ax.hist(residual, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
    ax.axvline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax.set_xlabel('æ®‹å·®')
    ax.set_ylabel('é¢‘æ•°')
    ax.text(0.05, 0.95, f'å‡å€¼={np.mean(residual):.5f}\nstd={np.std(residual):.5f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax.grid(True, alpha=0.3, axis='y')

# ğŸ”§ éšè—å¤šä½™çš„å­å›¾
for idx in range(len(residuals_dict), len(axes)):
    axes[idx].axis('off')

plt.tight_layout()
plt.savefig(results_directory + '04_residual_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾4: æ®‹å·®åˆ†å¸ƒåˆ†æ")

# å›¾5: æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”
fig, ax = plt.subplots(figsize=(16, 6))

for name, residual in residuals_dict.items():
    ax.plot(residual, label=name, linewidth=2, alpha=0.7)

ax.axhline(0, color='black', linestyle='--', linewidth=1)
ax.set_title('ä¸åŒç­–ç•¥æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_xlabel('æ ·æœ¬åºå·', fontsize=12)
ax.set_ylabel('æ®‹å·®', fontsize=12)
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + '05_residual_timeseries.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾5: æ®‹å·®æ—¶é—´åºåˆ—")

# å›¾6: ç»¼åˆå¯¹æ¯”å›¾
plt.figure(figsize=(18, 8))
plt.plot(y_test_original, label='çœŸå®å€¼', linewidth=3, color='black', alpha=0.9, zorder=10)

key_strategies_for_plot = [
    ('ç®€å•å¹³å‡(åŸºçº¿)', strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'], 'gray'),
    ('ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '], '#A09FA4'),
    ('ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '], '#FEA0A0'),
    ('ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ ', strategies_original['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '], '#AD07E3'),
    ('ç­–ç•¥4-æ®‹å·®å‰ªè£', strategies_original['ç­–ç•¥4-æ®‹å·®å‰ªè£'], '#55A0FB'),
    ('ç­–ç•¥5-åŠ æƒèåˆ30%', strategies_original['ç­–ç•¥5-åŠ æƒèåˆ30%'], '#099A63'),
    ('ç­–ç•¥6-ç»ˆæç»„åˆ', strategies_original['ç­–ç•¥6-ç»ˆæç»„åˆ'], '#FFB90F'),
]

for name, pred, color in key_strategies_for_plot:
    plt.plot(pred, label=name, linewidth=1.8, alpha=0.7, color=color)

plt.title('å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”', fontsize=16, fontweight='bold')
plt.xlabel('æ ·æœ¬åºå·', fontsize=12)
plt.ylabel('ç‰ç±³ä»·æ ¼', fontsize=12)
plt.legend(fontsize=11, loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(results_directory + '06_key_strategies_comprehensive.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾6: å…³é”®ç­–ç•¥ç»¼åˆå¯¹æ¯”")

# å›¾7: æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

metrics_data = {}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    metrics_data[name] = {
        'R^2': r2_score(y_test_original, pred_original),
        'MAE': mean_absolute_error(y_test_original, pred_original),
        'RMSE': sqrt(mean_squared_error(y_test_original, pred_original)),
        'MAPE': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))
    }

top9_names = [name for name, _, _, _, _, _ in results_list[:min(9, len(results_list))]]
colors_bar = plt.cm.viridis(np.linspace(0, 1, len(top9_names)))

axes[0, 0].bar(range(len(top9_names)), [metrics_data[m]['R^2'] for m in top9_names],
               color=colors_bar, alpha=0.7)
axes[0, 0].set_title('R^2 åˆ†æ•°å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 0].set_ylabel('R^2 Score')
axes[0, 0].set_xticks(range(len(top9_names)))
axes[0, 0].set_xticklabels([n[:20] for n in top9_names], rotation=45, ha='right', fontsize=8)
axes[0, 0].grid(True, alpha=0.3, axis='y')

axes[0, 1].bar(range(len(top9_names)), [metrics_data[m]['MAE'] for m in top9_names],
               color=colors_bar, alpha=0.7)
axes[0, 1].set_title('MAE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].set_xticks(range(len(top9_names)))
axes[0, 1].set_xticklabels([n[:20] for n in top9_names], rotation=45, ha='right', fontsize=8)
axes[0, 1].grid(True, alpha=0.3, axis='y')

axes[1, 0].bar(range(len(top9_names)), [metrics_data[m]['RMSE'] for m in top9_names],
               color=colors_bar, alpha=0.7)
axes[1, 0].set_title('RMSE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 0].set_ylabel('RMSE')
axes[1, 0].set_xticks(range(len(top9_names)))
axes[1, 0].set_xticklabels([n[:20] for n in top9_names], rotation=45, ha='right', fontsize=8)
axes[1, 0].grid(True, alpha=0.3, axis='y')

axes[1, 1].bar(range(len(top9_names)), [metrics_data[m]['MAPE'] for m in top9_names],
               color=colors_bar, alpha=0.7)
axes[1, 1].set_title('MAPE å¯¹æ¯”', fontsize=13, fontweight='bold')
axes[1, 1].set_ylabel('MAPE')
axes[1, 1].set_xticks(range(len(top9_names)))
axes[1, 1].set_xticklabels([n[:20] for n in top9_names], rotation=45, ha='right', fontsize=8)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(results_directory + '07_metrics_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾7: å¤šæŒ‡æ ‡å¯¹æ¯”")

# å›¾8: é¢„æµ‹è¯¯å·®åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

top4_for_error = results_list[:4]

for idx, (name, r2, improvement, mae, rmse, pred) in enumerate(top4_for_error):
    ax = axes[idx // 2, idx % 2]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    errors = pred_original - y_test_original.flatten()

    ax.scatter(y_test_original, errors, alpha=0.5, s=30)
    ax.axhline(0, color='red', linestyle='--', linewidth=2)
    ax.set_title(f'{name} - é¢„æµ‹è¯¯å·®åˆ†æ', fontsize=11, fontweight='bold')
    ax.set_xlabel('çœŸå®å€¼')
    ax.set_ylabel('é¢„æµ‹è¯¯å·®')
    ax.grid(True, alpha=0.3)

    ax.text(0.05, 0.95, f'MAE={mae:.2f}\nRMSE={rmse:.2f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))

plt.tight_layout()
plt.savefig(results_directory + '08_prediction_error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾8: é¢„æµ‹è¯¯å·®åˆ†æ")

# å›¾9: æ¨¡å‹æ”¹è¿›æ•ˆæœé›·è¾¾å›¾
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='polar')

radar_strategies = ['ç®€å•å¹³å‡(åŸºçº¿)', 'ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ ', 'ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ', 'ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ ','ç­–ç•¥4-æ®‹å·®å‰ªè£','ç­–ç•¥5-åŠ æƒèåˆ30%','ç­–ç•¥6-ç»ˆæç»„åˆ']
categories = ['R^2', 'MAE', 'RMSE', 'MAPE']
N = len(categories)

angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

for strategy in radar_strategies:
    pred_original = strategies_original[strategy]

    r2_val = metrics_data[strategy]['R^2']
    mae_val = 1 / (1 + metrics_data[strategy]['MAE'] / 100)
    rmse_val = 1 / (1 + metrics_data[strategy]['RMSE'] / 100)
    mape_val = 1 / (1 + metrics_data[strategy]['MAPE'] * 100)

    values = [r2_val, mae_val, rmse_val, mape_val]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=strategy)
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories)
ax.set_ylim(0, 1)
ax.set_title('å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax.grid(True)

plt.tight_layout()
plt.savefig(results_directory + '09_performance_radar.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾9: æ€§èƒ½é›·è¾¾å›¾")

# å›¾10: æ®‹å·®ç®±çº¿å›¾å¯¹æ¯”
fig, ax = plt.subplots(figsize=(14, 6))

residual_data = []
residual_labels = []

for name, residual in residuals_dict.items():
    residual_data.append(residual)
    residual_labels.append(name)

bp = ax.boxplot(residual_data, labels=residual_labels, patch_artist=True)

for patch, color in zip(bp['boxes'], plt.cm.Set3(np.linspace(0, 1, len(residual_data)))):
    patch.set_facecolor(color)

ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)
ax.set_title('æ®‹å·®åˆ†å¸ƒç®±çº¿å›¾å¯¹æ¯”', fontsize=14, fontweight='bold')
ax.set_ylabel('æ®‹å·®å€¼')
ax.grid(True, alpha=0.3, axis='y')
plt.xticks(rotation=15, ha='right')

plt.tight_layout()
plt.savefig(results_directory + '10_residual_boxplot.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾10: æ®‹å·®ç®±çº¿å›¾")

print("\nâœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå¹¶ä¿å­˜ï¼")


# =====================================================================================
# SECTION 15: ä¿å­˜æ¨¡å‹å’Œç»“æœ
# =====================================================================================
print("\n" + "=" * 100)
print("ä¿å­˜æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_final_rolling.h5')
gru_final.save(results_directory + 'gru_final_rolling.h5')

# ä¿å­˜XGBoostæ¨¡å‹
with open(results_directory + 'xgb_lstm_rolling.pkl', 'wb') as f:
    pickle.dump(xgb_lstm, f)

with open(results_directory + 'xgb_gru_rolling.pkl', 'wb') as f:
    pickle.dump(xgb_gru, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    predictions_dict[name] = pred_original.flatten()

predictions_df = pd.DataFrame(predictions_dict)
predictions_df.to_csv(results_directory + 'all_predictions_rolling.csv', index=False)

# ä¿å­˜è¶…å‚æ•°
import json
hyperparams_dict = {
    'lstm_params': best_lstm_params,
    'gru_params': best_gru_params,
    'xgb_params': best_xgb_params,
    'optimization_method': 'Optuna' if (USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE) else 'Manual',
    'global_seed': GLOBAL_SEED,
    'evaluation_method': 'Rolling Window (No Data Leakage)'
}

with open(results_directory + 'hyperparameters.json', 'w') as f:
    json.dump(hyperparams_dict, f, indent=4)

print(f"\nâœ“ æ‰€æœ‰æ¨¡å‹å’Œç»“æœå·²ä¿å­˜è‡³: {results_directory}")


# =====================================================================================
# SECTION 16: DMæ£€éªŒï¼ˆDiebold-Mariano Testï¼‰
# =====================================================================================
print("\n" + "=" * 100)
print("ã€DMæ£€éªŒã€‘Diebold-Marianoç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ".center(100))
print("=" * 100)

try:
    from dm_test import quick_dm_analysis, pairwise_dm_analysis

    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹å°ºåº¦çš„é¢„æµ‹ç»“æœï¼‰
    all_predictions = {
        'LSTMå•æ¨¡å‹': strategies_original['LSTMå•æ¨¡å‹'],
        'GRUå•æ¨¡å‹': strategies_original['GRUå•æ¨¡å‹'],
        'ç®€å•å¹³å‡(åŸºçº¿)': strategies_original['ç®€å•å¹³å‡(åŸºçº¿)'],
        **{k: v for k, v in strategies_original.items() if k.startswith('ç­–ç•¥')}
    }

    # åŸºå‡†å¯¹æ¯”åˆ†æ
    print("\næ‰€æœ‰æ¨¡å‹ vs åŸºå‡†æ¨¡å‹")
    print("-" * 100)

    dm_results = quick_dm_analysis(
        y_true=y_test_original,
        predictions=all_predictions,
        baseline='ç®€å•å¹³å‡(åŸºçº¿)',
        save_dir=results_directory,
        plot=True,
        verbose=True
    )

    print("\nâœ… DMæ£€éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³:", results_directory)

except ImportError:
    print("[WARNING] âš ï¸ æœªæ‰¾åˆ°dm_test.pyæ¨¡å—ï¼Œè·³è¿‡DMæ£€éªŒ")
    print("[INFO] å¦‚éœ€DMæ£€éªŒï¼Œè¯·ç¡®ä¿dm_test.pyåœ¨åŒç›®å½•ä¸‹")


# =====================================================================================
# SECTION 17: æœ€ç»ˆæ€»ç»“æŠ¥å‘Š
# =====================================================================================
print("\n" + "=" * 100)
print("æœ€ç»ˆæ€»ç»“æŠ¥å‘Š".center(100))
print("=" * 100)

print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"  - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(y_train_seq)}")
print(f"  - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_test)}")
print(f"  - ç‰¹å¾ç»´åº¦: {X_train_feat.shape[1]}")
print(f"  - åºåˆ—é•¿åº¦: {seq_len}")

print(f"\nâœ… æ•°æ®æ³„éœ²ä¿®å¤éªŒè¯:")
print(f"  - è¯„ä¼°æ–¹æ³•: æ»šåŠ¨çª—å£é¢„æµ‹")
print(f"  - æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹é¢„æµ‹")
print(f"  - ä½¿ç”¨çœŸå®å†å²å€¼æ›´æ–°çª—å£")
print(f"  - å®Œå…¨é¿å…æ•°æ®æ³„éœ²")

print(f"\nğŸ† æœ€ä½³æ€§èƒ½ç­–ç•¥:")
print(f"  - ç­–ç•¥åç§°: {best_name}")
print(f"  - æµ‹è¯•é›†R^2: {best_r2:.4f}")
print(f"  - æå‡å¹…åº¦: {best_r2 - avg_r2:+.4f}")

# âœ… ä¿®å¤ï¼šç´¢å¼•ä»4æ”¹ä¸º5
best_pred = strategies_original[best_name]
best_mae = mean_absolute_error(y_test_original, best_pred)
best_rmse = sqrt(mean_squared_error(y_test_original, best_pred))
best_mape = np.mean(np.abs((best_pred - y_test_original) / (y_test_original + 1e-8)))

print(f"\nğŸ“ˆ æœ€ä½³ç­–ç•¥åŸå§‹å°ºåº¦æŒ‡æ ‡:")
print(f"  - MAE:  {best_mae:.2f}")
print(f"  - RMSE: {best_rmse:.2f}")
print(f"  - MAPE: {best_mape:.4%}")

print(f"\nâš™ï¸ è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•:")
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"  âœ… Optunaè‡ªåŠ¨ä¼˜åŒ–")
    print(f"  - LSTM trials: {OPTUNA_CONFIG['lstm_trials']}")
    print(f"  - GRU trials: {OPTUNA_CONFIG['gru_trials']}")
    print(f"  - XGBoost trials: {OPTUNA_CONFIG['xgb_trials']}")
    print(f"  - ä¼˜åŒ–æŠ¥å‘Š: ./optuna_results/")
else:
    print(f"  âš ï¸  æ‰‹åŠ¨è®¾ç½®å‚æ•°")

print(f"\nâš ï¸ è¿‡æ‹Ÿåˆè¯Šæ–­æ€»ç»“:")
print(f"  ã€å­¦ä¹ æ›²çº¿æ–¹æ³•ã€‘")
for model_name, result in detector.results.items():
    diagnosis = result.get('diagnosis', 'Unknown')
    final_gap = result.get('final_gap', 0)
    print(f"  - {model_name}: {diagnosis} (å·®è·={final_gap:.4f})")

print(f"\nğŸ’¡ å…³é”®ç»“è®º:")
if best_r2 > avg_r2:
    print(f"  âœ… æ®‹å·®å­¦ä¹ ç­–ç•¥æˆåŠŸæå‡äº†æ¨¡å‹æ€§èƒ½")
    print(f"  âœ… ç›¸æ¯”ç®€å•å¹³å‡æå‡äº† {(best_r2 - avg_r2) * 100:.2f}%")
else:
    print(f"  âš ï¸ æ®‹å·®å­¦ä¹ ç­–ç•¥æœªèƒ½æ”¹å–„æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨ç®€å•å¹³å‡")

if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"\nğŸ¯ Optunaä¼˜åŒ–æˆæœ:")
    print(f"  âœ… è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•°ç»„åˆ")
    print(f"  âœ… èŠ‚çœå¤§é‡æ‰‹åŠ¨è°ƒå‚æ—¶é—´")
    print(f"  âœ… è¯¦ç»†ä¼˜åŒ–æŠ¥å‘Šå¯ä¾›åˆ†æ")

print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜ä½ç½®:")
print(f"  - æ¨¡å‹å’Œé¢„æµ‹ç»“æœ: {results_directory}")
print(f"  - è¿‡æ‹Ÿåˆåˆ†æ: {detector.output_dir}")
if USE_OPTUNA_OPTIMIZATION and OPTUNA_AVAILABLE:
    print(f"  - Optunaä¼˜åŒ–æŠ¥å‘Š: ./optuna_results/")

print("\n" + "=" * 100)
print("ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼æ•°æ®æ³„éœ²é—®é¢˜å·²å½»åº•ä¿®å¤ï¼".center(100))
print("=" * 100)

# =====================================================================================
# åœ¨ SECTION 18 æ·»åŠ ç‰¹å¾æ¶ˆèå®éªŒä»£ç 
# =====================================================================================

if FEATURE_ABLATION_CONFIG['enabled'] and FEATURE_ABLATION_AVAILABLE:
    print("\n" + "=" * 100)
    print("ã€ç‰¹å¾æ¶ˆèå®éªŒã€‘é‡åŒ–æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®åº¦".center(100))
    print("=" * 100)

    # è·å–åŸå§‹ç‰¹å¾åˆ—è¡¨ï¼ˆä¸åŒ…æ‹¬æ»åç‰¹å¾ï¼‰
    original_features = [col for col in X_train.columns if not col.startswith('Corn_lag_')]

    print(f"\nåˆ†æç‰¹å¾åˆ—è¡¨: {original_features}")
    print(f"ç‰¹å¾æ•°é‡: {len(original_features)}")

    # åˆ›å»ºç‰¹å¾æ¶ˆèåˆ†æå™¨
    feature_analyzer = FeatureAblationAnalyzer(
        y_test_true=y_test.values,
        feature_names=original_features,
        output_dir=FEATURE_ABLATION_CONFIG['output_dir']
    )

    # =========================================================================
    # æ­¥éª¤1: æ·»åŠ åŸºçº¿ï¼ˆä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
    # =========================================================================
    print("\n[æ­¥éª¤1] æ·»åŠ åŸºçº¿å®éªŒï¼ˆä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰...")

    # ä½¿ç”¨å½“å‰æœ€ä½³ç­–ç•¥çš„é¢„æµ‹ä½œä¸ºåŸºçº¿
    baseline_pred = strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ ']  # æ ¹æ®æ‚¨çš„æœ€ä½³ç­–ç•¥è°ƒæ•´

    feature_analyzer.add_baseline(
        predictions=baseline_pred,
        description=f'å®Œæ•´æ¨¡å‹ï¼šLSTM+GRU+XGBoostï¼Œä½¿ç”¨å…¨éƒ¨{len(original_features)}ä¸ªç‰¹å¾'
    )

    # =========================================================================
    # æ­¥éª¤2: é€ä¸ªç§»é™¤ç‰¹å¾å¹¶é‡æ–°è®­ç»ƒ
    # =========================================================================
    print("\n[æ­¥éª¤2] é€ä¸ªç§»é™¤ç‰¹å¾å¹¶é‡æ–°è®­ç»ƒ...")
    print("æ³¨æ„ï¼šè¿™å°†èŠ±è´¹è¾ƒé•¿æ—¶é—´ï¼ˆæ¯ä¸ªç‰¹å¾éœ€è¦é‡æ–°è®­ç»ƒå®Œæ•´æ¨¡å‹ï¼‰")

    for idx, feature_to_remove in enumerate(original_features, 1):
        print(f"\n{'=' * 100}")
        print(f"[{idx}/{len(original_features)}] ç§»é™¤ç‰¹å¾: {feature_to_remove}".center(100))
        print(f"{'=' * 100}")

        # é€‰æ‹©å‰©ä½™ç‰¹å¾
        remaining_features = [f for f in original_features if f != feature_to_remove]

        print(f"å‰©ä½™ç‰¹å¾: {remaining_features}")

        # é‡æ–°æ„é€ è®­ç»ƒæ•°æ®ï¼ˆä»…ä½¿ç”¨å‰©ä½™ç‰¹å¾ï¼‰
        X_train_ablation = X_train[remaining_features].copy()
        X_test_ablation = X_test[remaining_features].copy()

        # æ·»åŠ æ»åç‰¹å¾
        X_train_feat_ablation = add_features(X_train_ablation, y_train)
        y_train_ablation = y_train.loc[X_train_feat_ablation.index]

        # æ„é€ åºåˆ—
        X_train_seq_ablation, y_train_seq_ablation = create_sequences(
            X_train_feat_ablation, y_train_ablation, seq_len
        )

        print(f"è®­ç»ƒé›†åºåˆ—å½¢çŠ¶: {X_train_seq_ablation.shape}")

        # è®­ç»ƒLSTM
        print("  [1/3] è®­ç»ƒLSTM...", end='')
        tf.random.set_seed(GLOBAL_SEED)
        lstm_ablation = Sequential([
            layers.LSTM(
                best_lstm_params['units'],
                input_shape=(X_train_seq_ablation.shape[1], X_train_seq_ablation.shape[2]),
                kernel_regularizer=l2(best_lstm_params.get('l2_reg', 0.01)),
                recurrent_dropout=best_lstm_params.get('recurrent_dropout', 0.0),
                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
            ),
            layers.Dropout(best_lstm_params['dropout'], seed=GLOBAL_SEED),
            layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
        ])
        lstm_ablation.compile(
            loss='mse',
            optimizer=tf.keras.optimizers.Adam(learning_rate=best_lstm_params['learning_rate'])
        )
        lstm_ablation.fit(
            X_train_seq_ablation, y_train_seq_ablation,
            validation_split=0.2, epochs=200, batch_size=32,
            verbose=0, shuffle=False,
            callbacks=[EarlyStopping(monitor='val_loss', patience=15,
                                     restore_best_weights=True, verbose=0)]
        )
        print(" å®Œæˆ")

        # è®­ç»ƒGRU
        print("  [2/3] è®­ç»ƒGRU...", end='')
        tf.random.set_seed(GLOBAL_SEED)
        gru_ablation = Sequential([
            layers.GRU(
                best_gru_params['units'],
                input_shape=(X_train_seq_ablation.shape[1], X_train_seq_ablation.shape[2]),
                recurrent_dropout=best_gru_params.get('recurrent_dropout', 0.0),
                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED),
                recurrent_initializer=tf.keras.initializers.Orthogonal(seed=GLOBAL_SEED)
            ),
            layers.Dropout(best_gru_params['dropout'], seed=GLOBAL_SEED),
            layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=GLOBAL_SEED))
        ])
        gru_ablation.compile(
            loss='mse',
            optimizer=tf.keras.optimizers.Adam(learning_rate=best_gru_params['learning_rate'])
        )
        gru_ablation.fit(
            X_train_seq_ablation, y_train_seq_ablation,
            validation_split=0.2, epochs=200, batch_size=32,
            verbose=0, shuffle=False,
            callbacks=[EarlyStopping(monitor='val_loss', patience=20,
                                     restore_best_weights=True, verbose=0)]
        )
        print(" å®Œæˆ")

        # è®­ç»ƒXGBoost
        print("  [3/3] è®­ç»ƒXGBoost...", end='')
        lstm_oof_ablation = get_oof_predictions(
            X_train_seq_ablation, y_train_seq_ablation,
            best_lstm_params, 'lstm', n_splits=5
        )
        gru_oof_ablation = get_oof_predictions(
            X_train_seq_ablation, y_train_seq_ablation,
            best_gru_params, 'gru', n_splits=5
        )

        gru_residual_ablation = y_train_seq_ablation - gru_oof_ablation

        X_flat_ablation = X_train_seq_ablation.reshape(len(X_train_seq_ablation), -1)
        X_xgb_train_ablation = create_simplified_features(
            X_flat_ablation, lstm_oof_ablation, gru_oof_ablation
        )

        xgb_ablation = XGBRegressor(
            **best_xgb_params,
            random_state=GLOBAL_SEED,
            seed=GLOBAL_SEED,
            n_jobs=1,
            verbosity=0
        )
        xgb_ablation.fit(X_xgb_train_ablation, gru_residual_ablation)
        print(" å®Œæˆ")

        # æ»šåŠ¨çª—å£é¢„æµ‹
        print("  [é¢„æµ‹] æ»šåŠ¨çª—å£é¢„æµ‹æµ‹è¯•é›†...", end='')
        predictions_ablation = []
        history_X_feat = X_train_feat_ablation.tail(seq_len).copy()
        history_y = y_train_ablation.tail(5).copy()

        for test_idx in range(len(X_test)):
            # æ„é€ å½“å‰æ ·æœ¬ç‰¹å¾ï¼ˆä»…åŒ…å«å‰©ä½™ç‰¹å¾ï¼‰
            current_X_raw = X_test_ablation.iloc[test_idx:test_idx + 1].copy()

            for i in range(1, 6):
                if len(history_y) >= i:
                    current_X_raw[f'Corn_lag_{i}'] = history_y.iloc[-i]
                else:
                    current_X_raw[f'Corn_lag_{i}'] = 0

            # æ„é€ åºåˆ—çª—å£
            current_window = pd.concat([history_X_feat.tail(seq_len - 1), current_X_raw])
            X_seq_test = current_window.values.reshape(1, seq_len, -1)

            # è·å–LSTMå’ŒGRUé¢„æµ‹
            lstm_pred = lstm_ablation.predict(X_seq_test, verbose=0)[0, 0]
            gru_pred = gru_ablation.predict(X_seq_test, verbose=0)[0, 0]

            # æ„é€ XGBoostç‰¹å¾å¹¶é¢„æµ‹æ®‹å·®
            X_flat_test = X_seq_test.reshape(1, -1)
            X_xgb_test = np.hstack([
                X_flat_test,
                np.array([[lstm_pred, gru_pred, (lstm_pred + gru_pred) / 2,
                           abs(lstm_pred - gru_pred)]])
            ])

            residual = xgb_ablation.predict(X_xgb_test)[0]
            final_pred = gru_pred + residual
            predictions_ablation.append(final_pred)

            # æ›´æ–°å†å²
            history_X_feat = pd.concat([history_X_feat.iloc[1:], current_X_raw])
            history_y = pd.concat([history_y.iloc[1:], pd.Series([y_test.iloc[test_idx]])])

        predictions_ablation = np.array(predictions_ablation)
        print(" å®Œæˆ")

        # æ·»åŠ åˆ°ç‰¹å¾åˆ†æå™¨
        feature_analyzer.add_single_feature_removal(
            feature_name=feature_to_remove,
            predictions=predictions_ablation,
            description=f'ä½¿ç”¨{len(remaining_features)}ä¸ªç‰¹å¾ï¼ˆç§»é™¤{feature_to_remove}ï¼‰'
        )

        print(f"\nâœ… ç‰¹å¾ {feature_to_remove} çš„æ¶ˆèå®éªŒå®Œæˆ")

    # =========================================================================
    # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
    # =========================================================================
    print("\n" + "=" * 100)
    print("ç”Ÿæˆç‰¹å¾é‡è¦æ€§æŠ¥å‘Šå’Œå¯è§†åŒ–".center(100))
    print("=" * 100)

    # ç”ŸæˆæŠ¥å‘Š
    feature_report = feature_analyzer.generate_feature_importance_report()

    # ç”Ÿæˆå¯è§†åŒ–
    feature_analyzer.visualize_feature_importance(
        show=FEATURE_ABLATION_CONFIG['show_plots']
    )

    # å¯¼å‡ºLaTeXï¼ˆå¯é€‰ï¼‰
    if FEATURE_ABLATION_CONFIG['save_latex']:
        feature_analyzer.export_to_latex()

    print("\n" + "=" * 100)
    print("âœ… ç‰¹å¾æ¶ˆèå®éªŒå®Œæˆï¼".center(100))
    print(f"ç»“æœä¿å­˜åœ¨: {FEATURE_ABLATION_CONFIG['output_dir']}".center(100))
    print("=" * 100)

else:
    if not FEATURE_ABLATION_CONFIG['enabled']:
        print("\nâ­ï¸ ç‰¹å¾æ¶ˆèå®éªŒå·²ç¦ç”¨ï¼ˆFEATURE_ABLATION_CONFIG['enabled']=Falseï¼‰")
    else:
        print("\nâš ï¸ ç‰¹å¾æ¶ˆèæ¨¡å—æœªå¯¼å…¥ï¼Œè·³è¿‡å®éªŒ")