import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - æ”¹è¿›ç‰ˆæ®‹å·®å­¦ä¹ ".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ + ç®€åŒ–ç‰¹å¾ + ä¿å®ˆå‚æ•° + å¤šç§æ®‹å·®ç­–ç•¥".center(100))
print("=" * 100)

# ========== åŠ è½½æ•°æ® ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ ==========
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# æ·»åŠ æ»åç‰¹å¾
def add_features(X, y):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# å®šä¹‰æ¨¡å‹
def build_simple_lstm(input_shape):
    return Sequential([
        layers.LSTM(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


def build_simple_gru(input_shape):
    return Sequential([
        layers.GRU(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


# OOFé¢„æµ‹ç”Ÿæˆ
def get_oof_predictions(X_seq, y_seq, model_type='lstm', n_splits=5):
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")
    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = build_simple_lstm((X_seq.shape[1], X_seq.shape[2]))
        else:
            model = build_simple_gru((X_seq.shape[1], X_seq.shape[2]))

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()
        fold += 1

    return oof_preds


print("\n" + "=" * 100)
print("ç¬¬ä¸€æ­¥ï¼šç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹".center(100))
print("=" * 100)

lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF R^2: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF R^2: {r2_score(y_train_seq, gru_oof_preds):.4f}")

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
print("\n" + "=" * 100)
print("ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆçš„LSTMå’ŒGRUæ¨¡å‹".center(100))
print("=" * 100)

print("\nè®­ç»ƒæœ€ç»ˆLSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=80,
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    ),
    layers.Dropout(0.3),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒæœ€ç»ˆGRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(units=100, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    layers.Dropout(0.3),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

# è¯Šæ–­è¿‡æ‹Ÿåˆ
lstm_train_pred = lstm_final.predict(X_train_seq, verbose=0).flatten()
gru_train_pred = gru_final.predict(X_train_seq, verbose=0).flatten()

lstm_train_r2 = r2_score(y_train_seq, lstm_train_pred)
lstm_test_r2 = r2_score(y_test_seq, lstm_test_pred)
gru_train_r2 = r2_score(y_train_seq, gru_train_pred)
gru_test_r2 = r2_score(y_test_seq, gru_test_pred)

print(f"\nè¿‡æ‹Ÿåˆè¯Šæ–­:")
print(f"LSTM - è®­ç»ƒR^2: {lstm_train_r2:.4f}, æµ‹è¯•R^2: {lstm_test_r2:.4f}, å·®è·: {lstm_train_r2 - lstm_test_r2:.4f}")
print(f"GRU  - è®­ç»ƒR^2: {gru_train_r2:.4f}, æµ‹è¯•R^2: {gru_test_r2:.4f}, å·®è·: {gru_train_r2 - gru_test_r2:.4f}")

if max(lstm_train_r2 - lstm_test_r2, gru_train_r2 - gru_test_r2) > 0.15:
    print(f"âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œæ®‹å·®å­¦ä¹ éœ€è¦ä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼")


# ========== ã€æ ¸å¿ƒæ”¹è¿›ã€‘ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========

def create_original_features(X_flat, lstm_preds, gru_preds):
    """åŸå§‹å¢å¼ºç‰¹å¾ï¼ˆ80ç»´ï¼‰- å®¹æ˜“è¿‡æ‹Ÿåˆ"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append((lstm_preds + gru_preds).reshape(-1, 1))
    features_list.append((lstm_preds - gru_preds).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    features_list.append((lstm_preds * gru_preds).reshape(-1, 1))
    features_list.append(np.maximum(lstm_preds, gru_preds).reshape(-1, 1))
    features_list.append(np.minimum(lstm_preds, gru_preds).reshape(-1, 1))
    disagreement = np.abs(lstm_preds - gru_preds)
    confidence = 1 / (1 + disagreement)
    features_list.append(confidence.reshape(-1, 1))
    weighted_avg = 0.5 * lstm_preds + 0.5 * gru_preds
    features_list.append(weighted_avg.reshape(-1, 1))
    return np.hstack(features_list)


def create_simplified_features(X_flat, lstm_preds, gru_preds):
    """ç®€åŒ–ç‰¹å¾ï¼ˆ74ç»´ï¼‰- åªä¿ç•™4ä¸ªæœ€å…³é”®çš„å¢å¼ºç‰¹å¾"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


def create_minimal_features(X_flat, lstm_preds, gru_preds):
    """æœ€å°åŒ–ç‰¹å¾ï¼ˆ72ç»´ï¼‰- åªæ·»åŠ é¢„æµ‹å€¼"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    return np.hstack(features_list)


# ========== ã€æ ¸å¿ƒæ”¹è¿›ã€‘æ®‹å·®å­¦ä¹ è®­ç»ƒå‡½æ•° ==========

def train_original_xgboost(X_train, y_train):
    """åŸå§‹XGBoostå‚æ•°ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰"""
    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.03,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_conservative_xgboost(X_train, y_train):
    """ä¿å®ˆXGBoostå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰"""
    model = XGBRegressor(
        n_estimators=100,  # 500 â†’ 100
        learning_rate=0.01,  # 0.03 â†’ 0.01
        max_depth=3,  # 4 â†’ 3
        min_child_weight=5,  # æ–°å¢
        subsample=0.7,  # 0.8 â†’ 0.7
        colsample_bytree=0.7,  # 0.8 â†’ 0.7
        reg_alpha=0.1,  # æ–°å¢ L1
        reg_lambda=1.0,  # æ–°å¢ L2
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_xgboost_with_early_stopping(X_train, y_train):
    """å¸¦æ—©åœçš„XGBoostï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰"""
    split_point = int(len(X_train) * 0.85)
    X_tr, X_val = X_train[:split_point], X_train[split_point:]
    y_tr, y_val = y_train[:split_point], y_train[split_point:]

    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.01,
        max_depth=3,
        min_child_weight=5,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        verbosity=0,
        early_stopping_rounds=20  # æ–°ç‰ˆæœ¬ï¼šä½œä¸ºåˆå§‹åŒ–å‚æ•°
    )

    try:
        # æ–°ç‰ˆæœ¬XGBoost (>=2.0.0)
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
    except TypeError:
        # æ—§ç‰ˆæœ¬XGBoost (<2.0.0) - å¦‚æœæ–°ç‰ˆæœ¬å¤±è´¥ï¼Œå°è¯•æ—§ç‰ˆæœ¬æ–¹å¼
        model = XGBRegressor(
            n_estimators=500,
            learning_rate=0.01,
            max_depth=3,
            min_child_weight=5,
            subsample=0.7,
            colsample_bytree=0.7,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=42,
            verbosity=0
        )
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=20,
            verbose=False
        )
    return model


def train_ridge_model(X_train, y_train, alpha=10.0):
    """Ridgeçº¿æ€§å›å½’ï¼ˆæœ€ä¿å®ˆï¼‰"""
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    return model


# ========== ã€æ ¸å¿ƒæ”¹è¿›ã€‘æ®‹å·®å¤„ç†å‡½æ•° ==========

def clip_residual(residual_pred, threshold=2.0):
    """å‰ªè£æç«¯æ®‹å·®å€¼"""
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    """åŠ æƒæ®‹å·®ä¿®æ­£"""
    return base_pred + weight * residual_pred


# ========== åŸºå‡†ç­–ç•¥ ==========
print("\n" + "=" * 100)
print("åŸºå‡†ç­–ç•¥ï¼šç®€å•å¹³å‡".center(100))
print("=" * 100)

avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"ç®€å•å¹³å‡æµ‹è¯•R^2: {avg_r2:.4f}")

# è®¡ç®—æ®‹å·®
lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# ========== ç­–ç•¥å¯¹æ¯”å®éªŒ ==========
print("\n" + "=" * 100)
print("æ”¹è¿›ç‰ˆæ®‹å·®å­¦ä¹ ç­–ç•¥å¯¹æ¯”å®éªŒ".center(100))
print("=" * 100)

strategies_results = {}

# å‡†å¤‡åŸºç¡€ç‰¹å¾
basic_features_train = X_train_flat[:len(gru_oof_preds)]
basic_features_test = X_test_flat

# åŸå§‹ç‰¹å¾
original_train = create_original_features(basic_features_train, lstm_oof_preds, gru_oof_preds)
original_test = create_original_features(basic_features_test, lstm_test_pred, gru_test_pred)

simplified_train = create_simplified_features(basic_features_train, lstm_oof_preds, gru_oof_preds)
simplified_test = create_simplified_features(basic_features_test, lstm_test_pred, gru_test_pred)

minimal_train = create_minimal_features(basic_features_train, lstm_oof_preds, gru_oof_preds)
minimal_test = create_minimal_features(basic_features_test, lstm_test_pred, gru_test_pred)

print(f"\nç‰¹å¾ç»´åº¦å¯¹æ¯”:")
print(f"  åŸå§‹å¢å¼ºç‰¹å¾: {original_train.shape[1]} ç»´")
print(f"  ç®€åŒ–ç‰¹å¾: {simplified_train.shape[1]} ç»´")
print(f"  æœ€å°åŒ–ç‰¹å¾: {minimal_train.shape[1]} ç»´")

# ========== å®éªŒ1ï¼šåŸå§‹æ–¹æ³•ï¼ˆé‡ç°é—®é¢˜ï¼‰==========
print("\n" + "-" * 100)
print("ã€å®éªŒ1ã€‘åŸå§‹æ–¹æ³•ï¼šåŸå§‹ç‰¹å¾ + æ¿€è¿›XGBoostï¼ˆé‡ç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼‰")
print("-" * 100)

model_exp1 = train_original_xgboost(original_train, gru_oof_residual)
residual_exp1 = model_exp1.predict(original_test)
pred_exp1 = gru_test_pred + residual_exp1
r2_exp1 = r2_score(y_test_seq, pred_exp1)

print(f"âœ“ R^2: {r2_exp1:.4f} (vsç®€å•å¹³å‡: {r2_exp1 - avg_r2:+.4f})")
strategies_results['å®éªŒ1-åŸå§‹æ–¹æ³•'] = pred_exp1

# ========== å®éªŒ2ï¼šç®€åŒ–ç‰¹å¾ + æ¿€è¿›XGBoost ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ2ã€‘æ”¹è¿›Aï¼šç®€åŒ–ç‰¹å¾ + æ¿€è¿›XGBoost")
print("-" * 100)

model_exp2 = train_original_xgboost(simplified_train, gru_oof_residual)
residual_exp2 = model_exp2.predict(simplified_test)
pred_exp2 = gru_test_pred + residual_exp2
r2_exp2 = r2_score(y_test_seq, pred_exp2)

print(f"âœ“ R^2: {r2_exp2:.4f} (vsç®€å•å¹³å‡: {r2_exp2 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp2 - r2_exp1:+.4f})")
strategies_results['å®éªŒ2-ç®€åŒ–ç‰¹å¾'] = pred_exp2

# ========== å®éªŒ3ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ3ã€‘æ”¹è¿›Bï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost")
print("-" * 100)

model_exp3 = train_conservative_xgboost(simplified_train, gru_oof_residual)
residual_exp3 = model_exp3.predict(simplified_test)
pred_exp3 = gru_test_pred + residual_exp3
r2_exp3 = r2_score(y_test_seq, pred_exp3)

print(f"âœ“ R^2: {r2_exp3:.4f} (vsç®€å•å¹³å‡: {r2_exp3 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp3 - r2_exp1:+.4f})")
strategies_results['å®éªŒ3-ä¿å®ˆå‚æ•°'] = pred_exp3

# ========== å®éªŒ4ï¼šç®€åŒ–ç‰¹å¾ + æ—©åœXGBoost ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ4ã€‘æ”¹è¿›Cï¼šç®€åŒ–ç‰¹å¾ + æ—©åœXGBoost")
print("-" * 100)

model_exp4 = train_xgboost_with_early_stopping(simplified_train, gru_oof_residual)
residual_exp4 = model_exp4.predict(simplified_test)
pred_exp4 = gru_test_pred + residual_exp4
r2_exp4 = r2_score(y_test_seq, pred_exp4)

print(f"âœ“ R^2: {r2_exp4:.4f} (vsç®€å•å¹³å‡: {r2_exp4 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp4 - r2_exp1:+.4f})")
strategies_results['å®éªŒ4-æ—©åœæœºåˆ¶'] = pred_exp4

# ========== å®éªŒ5ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£ ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ5ã€‘æ”¹è¿›Dï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£")
print("-" * 100)

model_exp5 = train_conservative_xgboost(simplified_train, gru_oof_residual)
residual_exp5 = model_exp5.predict(simplified_test)
residual_exp5_clipped = clip_residual(residual_exp5, threshold=2.0)
pred_exp5 = gru_test_pred + residual_exp5_clipped
r2_exp5 = r2_score(y_test_seq, pred_exp5)

print(f"âœ“ R^2: {r2_exp5:.4f} (vsç®€å•å¹³å‡: {r2_exp5 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp5 - r2_exp1:+.4f})")
print(f"  æ®‹å·®å‰ªè£å‰std: {np.std(residual_exp5):.6f}, å‰ªè£åstd: {np.std(residual_exp5_clipped):.6f}")
strategies_results['å®éªŒ5-æ®‹å·®å‰ªè£'] = pred_exp5

# ========== å®éªŒ6ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + åŠ æƒèåˆ ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ6ã€‘æ”¹è¿›Eï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + åŠ æƒèåˆ(50%)")
print("-" * 100)

model_exp6 = train_conservative_xgboost(simplified_train, gru_oof_residual)
residual_exp6 = model_exp6.predict(simplified_test)
pred_exp6 = weighted_residual_correction(gru_test_pred, residual_exp6, weight=0.5)
r2_exp6 = r2_score(y_test_seq, pred_exp6)

print(f"âœ“ R^2: {r2_exp6:.4f} (vsç®€å•å¹³å‡: {r2_exp6 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp6 - r2_exp1:+.4f})")
strategies_results['å®éªŒ6-åŠ æƒèåˆ50%'] = pred_exp6

# ========== å®éªŒ7ï¼šæœ€å°åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ7ã€‘æ”¹è¿›Fï¼šæœ€å°åŒ–ç‰¹å¾(ä»…2ä¸ª) + ä¿å®ˆXGBoost")
print("-" * 100)

model_exp7 = train_conservative_xgboost(minimal_train, gru_oof_residual)
residual_exp7 = model_exp7.predict(minimal_test)
pred_exp7 = gru_test_pred + residual_exp7
r2_exp7 = r2_score(y_test_seq, pred_exp7)

print(f"âœ“ R^2: {r2_exp7:.4f} (vsç®€å•å¹³å‡: {r2_exp7 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp7 - r2_exp1:+.4f})")
strategies_results['å®éªŒ7-æœ€å°ç‰¹å¾'] = pred_exp7

# ========== å®éªŒ8ï¼šç®€åŒ–ç‰¹å¾ + Ridgeçº¿æ€§æ¨¡å‹ ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ8ã€‘æ”¹è¿›Gï¼šç®€åŒ–ç‰¹å¾ + Ridgeçº¿æ€§å›å½’(alpha=10)")
print("-" * 100)

model_exp8 = train_ridge_model(simplified_train, gru_oof_residual, alpha=10.0)
residual_exp8 = model_exp8.predict(simplified_test)
pred_exp8 = gru_test_pred + residual_exp8
r2_exp8 = r2_score(y_test_seq, pred_exp8)

print(f"âœ“ R^2: {r2_exp8:.4f} (vsç®€å•å¹³å‡: {r2_exp8 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp8 - r2_exp1:+.4f})")
strategies_results['å®éªŒ8-Ridgeå›å½’'] = pred_exp8

# ========== å®éªŒ9ï¼šç»„åˆæœ€ä¼˜ç­–ç•¥ ==========
print("\n" + "-" * 100)
print("ã€å®éªŒ9ã€‘ç»ˆæç»„åˆï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost + æ®‹å·®å‰ªè£ + åŠ æƒèåˆ(30%)")
print("-" * 100)

model_exp9 = train_conservative_xgboost(simplified_train, gru_oof_residual)
residual_exp9 = model_exp9.predict(simplified_test)
residual_exp9_clipped = clip_residual(residual_exp9, threshold=2.0)
pred_exp9 = weighted_residual_correction(gru_test_pred, residual_exp9_clipped, weight=0.3)
r2_exp9 = r2_score(y_test_seq, pred_exp9)

print(f"âœ“ R^2: {r2_exp9:.4f} (vsç®€å•å¹³å‡: {r2_exp9 - avg_r2:+.4f}, vså®éªŒ1: {r2_exp9 - r2_exp1:+.4f})")
strategies_results['å®éªŒ9-ç»ˆæç»„åˆ'] = pred_exp9

# ========== åŠ¨æ€æƒé‡èåˆï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰==========
print("\n" + "-" * 100)
print("ã€å¯¹æ¯”ã€‘åŠ¨æ€æƒé‡èåˆç­–ç•¥")
print("-" * 100)

# LSTMæ®‹å·®æ¨¡å‹
xgb_lstm_dynamic = train_conservative_xgboost(simplified_train, lstm_oof_residual)
lstm_res_train = xgb_lstm_dynamic.predict(simplified_train)
lstm_res_test = xgb_lstm_dynamic.predict(simplified_test)

# GRUæ®‹å·®æ¨¡å‹
xgb_gru_dynamic = train_conservative_xgboost(simplified_train, gru_oof_residual)
gru_res_train = xgb_gru_dynamic.predict(simplified_train)
gru_res_test = xgb_gru_dynamic.predict(simplified_test)

# Ridgeå­¦ä¹ æƒé‡
meta_features_train = np.column_stack([
    lstm_oof_preds + lstm_res_train,
    gru_oof_preds + gru_res_train
])
meta_features_test = np.column_stack([
    lstm_test_pred + lstm_res_test,
    gru_test_pred + gru_res_test
])

meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_features_train, y_train_seq)
pred_dynamic = meta_model.predict(meta_features_test)
r2_dynamic = r2_score(y_test_seq, pred_dynamic)

lstm_weight = meta_model.coef_[0]
gru_weight = meta_model.coef_[1]

print(f"å­¦ä¹ åˆ°çš„æƒé‡: LSTM={lstm_weight:.4f}, GRU={gru_weight:.4f}")
print(f"âœ“ R^2: {r2_dynamic:.4f} (vsç®€å•å¹³å‡: {r2_dynamic - avg_r2:+.4f}, vså®éªŒ1: {r2_dynamic - r2_exp1:+.4f})")
strategies_results['åŠ¨æ€æƒé‡èåˆ'] = pred_dynamic

# ========== ç»¼åˆç»“æœå¯¹æ¯” ==========
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥ç»¼åˆå¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

all_strategies = {
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç®€å•å¹³å‡(åŸºçº¿)': avg_test_pred,
    **strategies_results
}

print(f"\n{'ç­–ç•¥':<35} {'R^2':>10} {'vsåŸºçº¿':>10} {'MAE':>12} {'RMSE':>12}")
print("-" * 85)

results_list = []
for name, pred in all_strategies.items():
    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    improvement = r2 - avg_r2
    results_list.append((name, r2, improvement, mae, rmse, pred))
    print(f"{name:<35} {r2:>10.4f} {improvement:>10.4f} {mae:>12.6f} {rmse:>12.6f}")

# æ’åº
results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’åï¼ˆæŒ‰R^2é™åºï¼‰".center(100))
print("=" * 100)

best_r2 = results_list[0][1]
best_name = results_list[0][0]

for rank, (name, r2, improvement, mae, rmse, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<35} R^2={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_name} (R^2 = {best_r2:.4f})")

# ========== åŸå§‹å°ºåº¦è¯„ä¼° ==========
print("\n" + "=" * 100)
print("åŸå§‹å°ºåº¦æ€§èƒ½å¯¹æ¯”".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))
strategies_original = {}

print(f"\n{'ç­–ç•¥':<35} {'R^2':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 85)

for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    strategies_original[name] = pred_original

    r2 = r2_score(y_test_original, pred_original)
    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))
    mape = np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))

    print(f"{name:<35} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")

# ========== å¯è§†åŒ– ==========
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

# 1. æ”¹è¿›æ•ˆæœå¯¹æ¯”æŸ±çŠ¶å›¾
fig, ax = plt.subplots(figsize=(16, 8))

strategy_names = [name for name, _, _, _, _, _ in results_list]
r2_scores = [r2 for _, r2, _, _, _, _ in results_list]
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(strategy_names)))

bars = ax.barh(range(len(strategy_names)), r2_scores, color=colors, alpha=0.8)

# æ·»åŠ åŸºçº¿è™šçº¿
ax.axvline(x=avg_r2, color='red', linestyle='--', linewidth=2, label='ç®€å•å¹³å‡åŸºçº¿', alpha=0.7)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, r2, improvement) in enumerate(zip(bars, r2_scores, [imp for _, _, imp, _, _, _ in results_list])):
    label = f'{r2:.4f}'
    if improvement > 0:
        label += f' (+{improvement:.4f})'
        color = 'green'
    elif improvement < 0:
        label += f' ({improvement:.4f})'
        color = 'red'
    else:
        color = 'black'

    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            label, ha='left', va='center', fontweight='bold', fontsize=9, color=color)

ax.set_yticks(range(len(strategy_names)))
ax.set_yticklabels(strategy_names, fontsize=10)
ax.set_xlabel('R^2 Score', fontsize=12, fontweight='bold')
ax.set_title('æ”¹è¿›ç‰ˆæ®‹å·®å­¦ä¹ ç­–ç•¥æ•ˆæœå¯¹æ¯”', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(results_directory + 'improved_residual_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 2. å…³é”®æ”¹è¿›ç­–ç•¥å¯¹æ¯”
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

key_strategies = [
    ('å®éªŒ1-åŸå§‹æ–¹æ³•', strategies_results['å®éªŒ1-åŸå§‹æ–¹æ³•'], 'red'),
    ('å®éªŒ3-ä¿å®ˆå‚æ•°', strategies_results['å®éªŒ3-ä¿å®ˆå‚æ•°'], 'orange'),
    ('å®éªŒ4-æ—©åœæœºåˆ¶', strategies_results['å®éªŒ4-æ—©åœæœºåˆ¶'], 'yellow'),
    ('å®éªŒ6-åŠ æƒèåˆ50%', strategies_results['å®éªŒ6-åŠ æƒèåˆ50%'], 'cyan'),
    ('å®éªŒ9-ç»ˆæç»„åˆ', strategies_results['å®éªŒ9-ç»ˆæç»„åˆ'], 'green'),
    ('åŠ¨æ€æƒé‡èåˆ', strategies_results['åŠ¨æ€æƒé‡èåˆ'], 'purple'),
]

for idx, (name, pred, color) in enumerate(key_strategies):
    ax = axes[idx // 3, idx % 3]

    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    r2 = r2_score(y_test_original, pred_original)
    improvement = r2 - r2_score(y_test_original, y_scaler.inverse_transform(avg_test_pred.reshape(-1, 1)))

    ax.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax.plot(pred_original, label=name, linewidth=2, alpha=0.8, color=color)
    ax.set_title(f'{name}\nR^2={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})',
                 fontsize=11, fontweight='bold')
    ax.set_xlabel('æ ·æœ¬åºå·', fontsize=9)
    ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'key_strategies_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 3. æ®‹å·®åˆ†æå¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# åŸå§‹æ–¹æ³•æ®‹å·®
residual_original = y_test_seq - strategies_results['å®éªŒ1-åŸå§‹æ–¹æ³•']
axes[0, 0].hist(residual_original, bins=30, color='red', alpha=0.7, edgecolor='black')
axes[0, 0].axvline(0, color='black', linestyle='--', linewidth=2)
axes[0, 0].set_title('å®éªŒ1-åŸå§‹æ–¹æ³• æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('æ®‹å·®')
axes[0, 0].set_ylabel('é¢‘æ•°')
axes[0, 0].text(0.05, 0.95, f'std={np.std(residual_original):.5f}',
                transform=axes[0, 0].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# ä¿å®ˆå‚æ•°æ®‹å·®
residual_conservative = y_test_seq - strategies_results['å®éªŒ3-ä¿å®ˆå‚æ•°']
axes[0, 1].hist(residual_conservative, bins=30, color='orange', alpha=0.7, edgecolor='black')
axes[0, 1].axvline(0, color='black', linestyle='--', linewidth=2)
axes[0, 1].set_title('å®éªŒ3-ä¿å®ˆå‚æ•° æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('æ®‹å·®')
axes[0, 1].set_ylabel('é¢‘æ•°')
axes[0, 1].text(0.05, 0.95, f'std={np.std(residual_conservative):.5f}',
                transform=axes[0, 1].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# ç»ˆæç»„åˆæ®‹å·®
residual_ultimate = y_test_seq - strategies_results['å®éªŒ9-ç»ˆæç»„åˆ']
axes[1, 0].hist(residual_ultimate, bins=30, color='green', alpha=0.7, edgecolor='black')
axes[1, 0].axvline(0, color='black', linestyle='--', linewidth=2)
axes[1, 0].set_title('å®éªŒ9-ç»ˆæç»„åˆ æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('æ®‹å·®')
axes[1, 0].set_ylabel('é¢‘æ•°')
axes[1, 0].text(0.05, 0.95, f'std={np.std(residual_ultimate):.5f}',
                transform=axes[1, 0].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# æ®‹å·®å¯¹æ¯”ï¼ˆæ—¶é—´åºåˆ—ï¼‰
axes[1, 1].plot(residual_original, label='åŸå§‹æ–¹æ³•', linewidth=2, alpha=0.7, color='red')
axes[1, 1].plot(residual_conservative, label='ä¿å®ˆå‚æ•°', linewidth=2, alpha=0.7, color='orange')
axes[1, 1].plot(residual_ultimate, label='ç»ˆæç»„åˆ', linewidth=2, alpha=0.7, color='green')
axes[1, 1].axhline(0, color='black', linestyle='--', linewidth=1)
axes[1, 1].set_title('æ®‹å·®æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('æ ·æœ¬åºå·')
axes[1, 1].set_ylabel('æ®‹å·®')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'residual_analysis_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# 4. æ”¹è¿›æ•ˆæœé›·è¾¾å›¾
from math import pi

fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

# é€‰æ‹©å‡ ä¸ªå…³é”®ç­–ç•¥
radar_strategies = [
    'ç®€å•å¹³å‡(åŸºçº¿)',
    'å®éªŒ1-åŸå§‹æ–¹æ³•',
    'å®éªŒ3-ä¿å®ˆå‚æ•°',
    'å®éªŒ9-ç»ˆæç»„åˆ',
    'åŠ¨æ€æƒé‡èåˆ'
]

# è®¡ç®—å¤šä¸ªè¯„ä»·æŒ‡æ ‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
metrics = ['R^2', '1-MAE', '1-RMSE', 'ç¨³å®šæ€§', 'å¤æ‚åº¦']
n_metrics = len(metrics)

angles = [n / float(n_metrics) * 2 * pi for n in range(n_metrics)]
angles += angles[:1]

for strategy_name in radar_strategies:
    pred = all_strategies[strategy_name]

    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    stability = 1 - np.std(y_test_seq - pred) / np.std(y_test_seq)

    # å¤æ‚åº¦è¯„åˆ†ï¼ˆç®€å•=1ï¼Œå¤æ‚=0ï¼‰
    if 'ç®€å•å¹³å‡' in strategy_name:
        complexity = 1.0
    elif 'åŸå§‹æ–¹æ³•' in strategy_name:
        complexity = 0.3
    elif 'ç»ˆæç»„åˆ' in strategy_name:
        complexity = 0.5
    else:
        complexity = 0.7

    # å½’ä¸€åŒ–
    values = [
        r2,
        1 - (mae / 0.1),  # å‡è®¾æœ€å¤§MAE=0.1
        1 - (rmse / 0.1),  # å‡è®¾æœ€å¤§RMSE=0.1
        stability,
        complexity
    ]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=strategy_name, alpha=0.7)
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics, fontsize=11)
ax.set_ylim(0, 1)
ax.set_title('æ®‹å·®å­¦ä¹ ç­–ç•¥å¤šç»´è¯„ä¼°', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)
ax.grid(True)

plt.tight_layout()
plt.savefig(results_directory + 'strategies_radar_chart.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# ========== ä¿å­˜æ¨¡å‹å’Œç»“æœ ==========
import pickle

print("\n" + "=" * 100)
print("ä¿å­˜æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_final_model.h5')
gru_final.save(results_directory + 'gru_final_model.h5')

# ä¿å­˜æœ€ä½³XGBoostæ¨¡å‹ï¼ˆç»ˆæç»„åˆä½¿ç”¨çš„ï¼‰
with open(results_directory + 'best_xgboost_model.pkl', 'wb') as f:
    pickle.dump(model_exp9, f)

# ä¿å­˜åŠ¨æ€æƒé‡æ¨¡å‹
with open(results_directory + 'xgb_lstm_dynamic.pkl', 'wb') as f:
    pickle.dump(xgb_lstm_dynamic, f)

with open(results_directory + 'xgb_gru_dynamic.pkl', 'wb') as f:
    pickle.dump(xgb_gru_dynamic, f)

with open(results_directory + 'ridge_meta_model.pkl', 'wb') as f:
    pickle.dump(meta_model, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    predictions_dict[name.replace('/', '_')] = pred_original.flatten()

results_df = pd.DataFrame(predictions_dict)
results_df.to_csv(results_directory + 'all_predictions_improved.csv', index=False)

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
metrics_data = []
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    metrics_data.append({
        'strategy': name,
        'r2': r2_score(y_test_original, pred_original),
        'mae': mean_absolute_error(y_test_original, pred_original),
        'rmse': sqrt(mean_squared_error(y_test_original, pred_original)),
        'mape': np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))
    })

metrics_df = pd.DataFrame(metrics_data)
metrics_df = metrics_df.sort_values('r2', ascending=False)
metrics_df.to_csv(results_directory + 'performance_metrics.csv', index=False)

print("\nâœ“ ä¿å­˜å®Œæˆï¼")
print(f"  - lstm_final_model.h5")
print(f"  - gru_final_model.h5")
print(f"  - best_xgboost_model.pkl (ç»ˆæç»„åˆæ¨¡å‹)")
print(f"  - xgb_lstm_dynamic.pkl")
print(f"  - xgb_gru_dynamic.pkl")
print(f"  - ridge_meta_model.pkl")
print(f"  - scalers.pkl")
print(f"  - all_predictions_improved.csv")
print(f"  - performance_metrics.csv")

# ========== æœ€ç»ˆæ€»ç»“æŠ¥å‘Š ==========
print("\n" + "=" * 100)
print("ğŸ‰ æ”¹è¿›ç‰ˆæ®‹å·®å­¦ä¹ è®­ç»ƒå®Œæˆï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š å®éªŒç»“æœæ€»ç»“:")
print(f"  åŸºçº¿æ–¹æ³•ï¼ˆç®€å•å¹³å‡ï¼‰: R^2 = {avg_r2:.4f}")
print(f"  åŸå§‹æ®‹å·®å­¦ä¹ ï¼ˆè¿‡æ‹Ÿåˆï¼‰: R^2 = {r2_exp1:.4f} ({r2_exp1 - avg_r2:+.4f})")
print(f"  æœ€ä½³æ”¹è¿›æ–¹æ³•: {best_name}")
print(f"  æœ€ä½³æ€§èƒ½: R^2 = {best_r2:.4f} ({best_r2 - avg_r2:+.4f})")

print(f"\nğŸ’¡ å…³é”®å‘ç°:")

improvements = {
    'ç®€åŒ–ç‰¹å¾': r2_exp2 - r2_exp1,
    'ä¿å®ˆå‚æ•°': r2_exp3 - r2_exp1,
    'æ—©åœæœºåˆ¶': r2_exp4 - r2_exp1,
    'æ®‹å·®å‰ªè£': r2_exp5 - r2_exp3,
    'åŠ æƒèåˆ': r2_exp6 - r2_exp3,
    'ç»ˆæç»„åˆ': r2_exp9 - r2_exp1,
}

for improvement_name, improvement_value in improvements.items():
    status = "âœ“ æœ‰æ•ˆ" if improvement_value > 0 else "âœ— æ— æ•ˆ"
    print(f"  {status} {improvement_name}: {improvement_value:+.4f}")

print(f"\nğŸ” æŠ€æœ¯åˆ†æ:")
print(f"  1. ç‰¹å¾å·¥ç¨‹:")
print(f"     - åŸå§‹ç‰¹å¾(80ç»´) â†’ ç®€åŒ–ç‰¹å¾(74ç»´): æ”¹è¿› {r2_exp2 - r2_exp1:+.4f}")
print(f"     - æœ€å°åŒ–ç‰¹å¾(72ç»´) æ•ˆæœ: {r2_exp7:.4f}")
print(f"  2. æ¨¡å‹å‚æ•°:")
print(f"     - æ¿€è¿›å‚æ•° â†’ ä¿å®ˆå‚æ•°: æ”¹è¿› {r2_exp3 - r2_exp2:+.4f}")
print(f"     - åŠ å…¥æ—©åœæœºåˆ¶: {r2_exp4:.4f}")
print(f"  3. æ®‹å·®å¤„ç†:")
print(f"     - æ®‹å·®å‰ªè£æ•ˆæœ: {r2_exp5:.4f}")
print(f"     - åŠ æƒèåˆ(50%)æ•ˆæœ: {r2_exp6:.4f}")
print(f"     - åŠ æƒèåˆ(30%)æ•ˆæœ: {r2_exp9:.4f}")
print(f"  4. æ¨¡å‹é€‰æ‹©:")
print(f"     - XGBoost vs Ridge: Ridge R^2={r2_exp8:.4f}")
print(f"     - åŠ¨æ€æƒé‡èåˆ: R^2={r2_dynamic:.4f}")

print(f"\nğŸ“ˆ æœ€ä½³å®è·µå»ºè®®:")
if best_r2 > avg_r2:
    print(f"  âœ… æ®‹å·®å­¦ä¹ åœ¨æœ¬æ•°æ®é›†ä¸Šæœ‰æ•ˆï¼")
    print(f"  âœ… æ¨èä½¿ç”¨: {best_name}")
    print(f"  âœ… ç›¸æ¯”ç®€å•å¹³å‡æå‡: {best_r2 - avg_r2:.4f} ({(best_r2 - avg_r2) / avg_r2 * 100:.2f}%)")
else:
    print(f"  âš ï¸  æ®‹å·®å­¦ä¹ æœªè¶…è¿‡ç®€å•å¹³å‡åŸºçº¿")
    print(f"  âš ï¸  å»ºè®®:")
    print(f"     1. ç»§ç»­ä½¿ç”¨ç®€å•å¹³å‡ä½œä¸ºæœ€ç»ˆæ–¹æ¡ˆ")
    print(f"     2. æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®")
    print(f"     3. æ”¹è¿›åŸºç¡€æ¨¡å‹ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆï¼‰")
    print(f"     4. å°è¯•å…¶ä»–é›†æˆæ–¹æ³•ï¼ˆå¦‚Votingï¼‰")

print(f"\nğŸ¯ æ ¸å¿ƒç»“è®º:")
if lstm_train_r2 - lstm_test_r2 > 0.15 or gru_train_r2 - gru_test_r2 > 0.15:
    print(f"  âš ï¸  åŸºç¡€æ¨¡å‹å­˜åœ¨ä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒæµ‹è¯•å·®è·>{0.15:.2f}ï¼‰")
    print(f"  ğŸ’¡ è¿‡æ‹Ÿåˆæ˜¯æ®‹å·®å­¦ä¹ å¤±æ•ˆçš„ä¸»è¦åŸå› ")
    print(f"  ğŸ“Œ å»ºè®®ä¼˜å…ˆè§£å†³åŸºç¡€æ¨¡å‹çš„è¿‡æ‹Ÿåˆé—®é¢˜:")
    print(f"     - å¢åŠ è®­ç»ƒæ•°æ®")
    print(f"     - å¢å¼ºæ­£åˆ™åŒ–")
    print(f"     - ç®€åŒ–æ¨¡å‹ç»“æ„")
    print(f"     - ä½¿ç”¨æ•°æ®å¢å¼º")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_directory}")
print("=" * 100)