import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt, pi
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, Model, layers, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from xgboost import XGBRegressor
import pickle
import warnings

warnings.filterwarnings('ignore')

# ========== ã€å…³é”®ä¿®å¤ã€‘TensorFlowå†…å­˜ç®¡ç†é…ç½® ==========
# è®¾ç½®TensorFlowä½¿ç”¨GPUå†…å­˜å¢é•¿æ¨¡å¼ï¼ˆå³ä½¿æ²¡æœ‰GPUä¹Ÿæœ‰åŠ©äºç¨³å®šæ€§ï¼‰
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(f"GPUé…ç½®è­¦å‘Š: {e}")

# é™åˆ¶CPUçº¿ç¨‹æ•°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
tf.config.threading.set_intra_op_parallelism_threads(2)
tf.config.threading.set_inter_op_parallelism_threads(2)

# è®¾ç½®ä¸ºæ›´ç¨³å®šçš„æ‰§è¡Œæ¨¡å¼
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # å‡å°‘æ—¥å¿—è¾“å‡º
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ç¦ç”¨oneDNNä¼˜åŒ–ï¼ˆå¯èƒ½å¯¼è‡´å´©æºƒï¼‰

print("âœ“ TensorFlowé…ç½®ä¼˜åŒ–å®Œæˆ")

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("LSTM + GRU + Attention + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šæ·»åŠ Attentionæœºåˆ¶æå‡æ—¶é—´åºåˆ—å»ºæ¨¡èƒ½åŠ›".center(100))
print("=" * 100)


# ========== ã€æ ¸å¿ƒæ–°å¢ã€‘Attentionå±‚å®šä¹‰ ==========
class AttentionLayer(layers.Layer):
    """
    è‡ªå®šä¹‰Attentionå±‚
    å®ç°Bahdanaué£æ ¼çš„åŠ æ€§æ³¨æ„åŠ›æœºåˆ¶
    """

    def __init__(self, units=128, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        # W1: ç”¨äºæŸ¥è¯¢çš„æƒé‡çŸ©é˜µ
        self.W1 = self.add_weight(
            name='attention_W1',
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        # W2: ç”¨äºé”®çš„æƒé‡çŸ©é˜µ
        self.W2 = self.add_weight(
            name='attention_W2',
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        # V: ç”¨äºè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°çš„æƒé‡å‘é‡
        self.V = self.add_weight(
            name='attention_V',
            shape=(self.units, 1),
            initializer='glorot_uniform',
            trainable=True
        )
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        # inputs shape: (batch_size, time_steps, features)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # score = V^T * tanh(W1*h + W2*h)
        score = tf.nn.tanh(
            tf.matmul(inputs, self.W1) + tf.matmul(inputs, self.W2)
        )  # (batch_size, time_steps, units)

        attention_weights = tf.nn.softmax(
            tf.matmul(score, self.V), axis=1
        )  # (batch_size, time_steps, 1)

        # åŠ æƒæ±‚å’Œ
        context_vector = attention_weights * inputs
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

    def get_config(self):
        config = super(AttentionLayer, self).get_config()
        config.update({'units': self.units})
        return config


# ========== ã€æ”¹è¿›ã€‘æ¨¡å‹æ„å»ºå‡½æ•°ï¼ˆå¸¦Attentionï¼‰ ==========
def build_lstm_with_attention(input_shape, lstm_units=100, attention_units=128):
    """æ„å»ºå¸¦Attentionæœºåˆ¶çš„LSTMæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # LSTMå±‚ï¼ˆreturn_sequences=Trueä»¥ä¾¿ä½¿ç”¨Attentionï¼‰
    lstm_out = layers.LSTM(
        units=lstm_units,
        return_sequences=True,
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    )(inputs)

    # Attentionå±‚
    context_vector, attention_weights = AttentionLayer(units=attention_units)(lstm_out)

    # Dropout
    dropout = layers.Dropout(0.3)(context_vector)

    # è¾“å‡ºå±‚
    outputs = layers.Dense(1)(dropout)

    model = Model(inputs=inputs, outputs=outputs)
    return model


def build_gru_with_attention(input_shape, gru_units=100, attention_units=128):
    """æ„å»ºå¸¦Attentionæœºåˆ¶çš„GRUæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # GRUå±‚ï¼ˆreturn_sequences=Trueä»¥ä¾¿ä½¿ç”¨Attentionï¼‰
    gru_out = layers.GRU(
        units=gru_units,
        return_sequences=True,
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    )(inputs)

    # Attentionå±‚
    context_vector, attention_weights = AttentionLayer(units=attention_units)(gru_out)

    # Dropout
    dropout = layers.Dropout(0.3)(context_vector)

    # è¾“å‡ºå±‚
    outputs = layers.Dense(1)(dropout)

    model = Model(inputs=inputs, outputs=outputs)
    return model


def build_stacked_lstm_attention(input_shape, lstm_units=100, attention_units=128):
    """æ„å»ºå †å LSTM+Attentionæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # ç¬¬ä¸€å±‚LSTM
    lstm1 = layers.LSTM(
        units=lstm_units,
        return_sequences=True,
        kernel_regularizer=l2(0.01)
    )(inputs)
    lstm1 = layers.Dropout(0.2)(lstm1)

    # ç¬¬äºŒå±‚LSTM
    lstm2 = layers.LSTM(
        units=lstm_units // 2,
        return_sequences=True,
        kernel_regularizer=l2(0.01)
    )(lstm1)

    # Attentionå±‚
    context_vector, attention_weights = AttentionLayer(units=attention_units)(lstm2)

    # Dropout
    dropout = layers.Dropout(0.3)(context_vector)

    # è¾“å‡ºå±‚
    outputs = layers.Dense(1)(dropout)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰ ==========
def build_simple_lstm(input_shape):
    return Sequential([
        layers.LSTM(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


def build_simple_gru(input_shape):
    return Sequential([
        layers.GRU(units=100, input_shape=input_shape),
        layers.Dense(1)
    ])


# ========== æ•°æ®åŠ è½½å’Œå‡†å¤‡ ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# æ·»åŠ æ»åç‰¹å¾
def add_features(X, y):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")


# ========== OOFé¢„æµ‹ç”Ÿæˆï¼ˆæ·»åŠ é”™è¯¯å¤„ç†å’Œå†…å­˜æ¸…ç†ï¼‰==========
def get_oof_predictions(X_seq, y_seq, model_type='lstm_attention', n_splits=5):
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆTimeSeriesSplit with {n_splits} splitsï¼‰...")
    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„æ„å»ºå‡½æ•°
            if model_type == 'lstm':
                model = build_simple_lstm((X_seq.shape[1], X_seq.shape[2]))
            elif model_type == 'gru':
                model = build_simple_gru((X_seq.shape[1], X_seq.shape[2]))
            elif model_type == 'lstm_attention':
                model = build_lstm_with_attention((X_seq.shape[1], X_seq.shape[2]))
            elif model_type == 'gru_attention':
                model = build_gru_with_attention((X_seq.shape[1], X_seq.shape[2]))
            elif model_type == 'stacked_lstm_attention':
                model = build_stacked_lstm_attention((X_seq.shape[1], X_seq.shape[2]))
            else:
                raise ValueError(f"Unknown model type: {model_type}")

            model.compile(
                loss='mse',
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
            )

            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=20,
                restore_best_weights=True,
                verbose=0
            )

            # è®­ç»ƒæ¨¡å‹
            model.fit(
                X_fold_train, y_fold_train,
                validation_data=(X_fold_val, y_fold_val),
                epochs=200,
                batch_size=32,
                callbacks=[early_stop],
                verbose=0
            )

            # é¢„æµ‹
            val_pred = model.predict(X_fold_val, verbose=0)
            oof_preds[val_idx] = val_pred.flatten()

            # å…³é”®ï¼šæ¸…ç†å†…å­˜
            del model
            tf.keras.backend.clear_session()

            print(f"    âœ“ Fold {fold} å®Œæˆ")

        except Exception as e:
            print(f"    âœ— Fold {fold} å¤±è´¥: {str(e)}")
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ç®€å•é¢„æµ‹å¡«å……
            oof_preds[val_idx] = np.mean(y_fold_train)

        fold += 1

    return oof_preds


print("\n" + "=" * 100)
print("ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„OOFé¢„æµ‹".center(100))
print("=" * 100)

# ç”ŸæˆåŸºç¡€æ¨¡å‹çš„OOFé¢„æµ‹
print("\nã€åŸºç¡€æ¨¡å‹ã€‘")
lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

# ç”ŸæˆAttentionæ¨¡å‹çš„OOFé¢„æµ‹
print("\nã€Attentionå¢å¼ºæ¨¡å‹ã€‘")
lstm_attn_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm_attention', n_splits=5)
gru_attn_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru_attention', n_splits=5)
stacked_attn_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'stacked_lstm_attention', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"åŸºç¡€LSTM R^2: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"åŸºç¡€GRU R^2: {r2_score(y_train_seq, gru_oof_preds):.4f}")
print(f"LSTM+Attention R^2: {r2_score(y_train_seq, lstm_attn_oof_preds):.4f}")
print(f"GRU+Attention R^2: {r2_score(y_train_seq, gru_attn_oof_preds):.4f}")
print(f"Stacked LSTM+Attention R^2: {r2_score(y_train_seq, stacked_attn_oof_preds):.4f}")

# ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆæ¨¡å‹ ==========
print("\n" + "=" * 100)
print("ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœ€ç»ˆçš„æ‰€æœ‰æ¨¡å‹".center(100))
print("=" * 100)

# è®­ç»ƒåŸºç¡€LSTM
print("\nè®­ç»ƒåŸºç¡€LSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=80,
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(0.01),
        recurrent_regularizer=l2(0.01)
    ),
    layers.Dropout(0.3),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)
    ],
    verbose=0
)
print("âœ“ åŸºç¡€LSTMè®­ç»ƒå®Œæˆ")

# è®­ç»ƒåŸºç¡€GRU
print("\nè®­ç»ƒåŸºç¡€GRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(units=100, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)],
    verbose=0
)
print("âœ“ åŸºç¡€GRUè®­ç»ƒå®Œæˆ")

# è®­ç»ƒLSTM+Attention
print("\nè®­ç»ƒLSTM+Attentionæ¨¡å‹...")
lstm_attn_final = build_lstm_with_attention((X_train_seq.shape[1], X_train_seq.shape[2]))
lstm_attn_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
lstm_attn_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)
    ],
    verbose=0
)
print("âœ“ LSTM+Attentionè®­ç»ƒå®Œæˆ")

# è®­ç»ƒGRU+Attention
print("\nè®­ç»ƒGRU+Attentionæ¨¡å‹...")
gru_attn_final = build_gru_with_attention((X_train_seq.shape[1], X_train_seq.shape[2]))
gru_attn_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
gru_attn_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)],
    verbose=0
)
print("âœ“ GRU+Attentionè®­ç»ƒå®Œæˆ")

# è®­ç»ƒStacked LSTM+Attention
print("\nè®­ç»ƒStacked LSTM+Attentionæ¨¡å‹...")
stacked_attn_final = build_stacked_lstm_attention((X_train_seq.shape[1], X_train_seq.shape[2]))
stacked_attn_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
stacked_attn_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)],
    verbose=0
)
print("âœ“ Stacked LSTM+Attentionè®­ç»ƒå®Œæˆ")

# ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()
lstm_attn_test_pred = lstm_attn_final.predict(X_test_seq, verbose=0).flatten()
gru_attn_test_pred = gru_attn_final.predict(X_test_seq, verbose=0).flatten()
stacked_attn_test_pred = stacked_attn_final.predict(X_test_seq, verbose=0).flatten()

# è¯„ä¼°æ‰€æœ‰æ¨¡å‹
print(f"\næµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”:")
print(f"åŸºç¡€LSTM R^2: {r2_score(y_test_seq, lstm_test_pred):.4f}")
print(f"åŸºç¡€GRU R^2: {r2_score(y_test_seq, gru_test_pred):.4f}")
print(f"LSTM+Attention R^2: {r2_score(y_test_seq, lstm_attn_test_pred):.4f}")
print(f"GRU+Attention R^2: {r2_score(y_test_seq, gru_attn_test_pred):.4f}")
print(f"Stacked LSTM+Attention R^2: {r2_score(y_test_seq, stacked_attn_test_pred):.4f}")

# ========== ç¬¬ä¸‰æ­¥ï¼šèåˆç­–ç•¥ ==========
print("\n" + "=" * 100)
print("ç¬¬ä¸‰æ­¥ï¼šèåˆæ‰€æœ‰æ¨¡å‹".center(100))
print("=" * 100)

# ç®€å•å¹³å‡èåˆ
print("\nã€ç­–ç•¥1ã€‘ç®€å•å¹³å‡")
avg_all = (lstm_test_pred + gru_test_pred + lstm_attn_test_pred +
           gru_attn_test_pred + stacked_attn_test_pred) / 5
avg_attention_only = (lstm_attn_test_pred + gru_attn_test_pred + stacked_attn_test_pred) / 3

print(f"æ‰€æœ‰æ¨¡å‹å¹³å‡ R^2: {r2_score(y_test_seq, avg_all):.4f}")
print(f"ä»…Attentionæ¨¡å‹å¹³å‡ R^2: {r2_score(y_test_seq, avg_attention_only):.4f}")

# åŠ æƒå¹³å‡èåˆï¼ˆåŸºäºOOFæ€§èƒ½ï¼‰
print("\nã€ç­–ç•¥2ã€‘åŠ æƒå¹³å‡ï¼ˆåŸºäºOOF R^2ï¼‰")
oof_r2_scores = {
    'lstm': r2_score(y_train_seq, lstm_oof_preds),
    'gru': r2_score(y_train_seq, gru_oof_preds),
    'lstm_attn': r2_score(y_train_seq, lstm_attn_oof_preds),
    'gru_attn': r2_score(y_train_seq, gru_attn_oof_preds),
    'stacked_attn': r2_score(y_train_seq, stacked_attn_oof_preds)
}

# å½’ä¸€åŒ–æƒé‡ï¼ˆä½¿ç”¨softmaxï¼‰
oof_scores = np.array(list(oof_r2_scores.values()))
weights = np.exp(oof_scores * 5) / np.sum(np.exp(oof_scores * 5))

weighted_pred = (weights[0] * lstm_test_pred +
                 weights[1] * gru_test_pred +
                 weights[2] * lstm_attn_test_pred +
                 weights[3] * gru_attn_test_pred +
                 weights[4] * stacked_attn_test_pred)

print(f"æ¨¡å‹æƒé‡: LSTM={weights[0]:.3f}, GRU={weights[1]:.3f}, " +
      f"LSTM+Attn={weights[2]:.3f}, GRU+Attn={weights[3]:.3f}, Stacked={weights[4]:.3f}")
print(f"åŠ æƒèåˆ R^2: {r2_score(y_test_seq, weighted_pred):.4f}")

# Stackingèåˆï¼ˆä½¿ç”¨Ridgeï¼‰
print("\nã€ç­–ç•¥3ã€‘Stackingèåˆï¼ˆRidgeï¼‰")
meta_features_train = np.column_stack([
    lstm_oof_preds,
    gru_oof_preds,
    lstm_attn_oof_preds,
    gru_attn_oof_preds,
    stacked_attn_oof_preds
])

meta_features_test = np.column_stack([
    lstm_test_pred,
    gru_test_pred,
    lstm_attn_test_pred,
    gru_attn_test_pred,
    stacked_attn_test_pred
])

ridge_meta = Ridge(alpha=1.0)
ridge_meta.fit(meta_features_train, y_train_seq)
stacking_pred = ridge_meta.predict(meta_features_test)

print(f"Ridgeæƒé‡: {ridge_meta.coef_}")
print(f"Stackingèåˆ R^2: {r2_score(y_test_seq, stacking_pred):.4f}")

# ========== ç¬¬å››æ­¥ï¼šXGBoostæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ç¬¬å››æ­¥ï¼šXGBoostæ®‹å·®å­¦ä¹ ".center(100))
print("=" * 100)

# é€‰æ‹©æ€§èƒ½æœ€å¥½çš„Attentionæ¨¡å‹ä½œä¸ºåŸºç¡€
best_attention_model = max(
    [('LSTM+Attention', lstm_attn_oof_preds, lstm_attn_test_pred),
     ('GRU+Attention', gru_attn_oof_preds, gru_attn_test_pred),
     ('Stacked+Attention', stacked_attn_oof_preds, stacked_attn_test_pred)],
    key=lambda x: r2_score(y_train_seq, x[1])
)

print(f"\né€‰æ‹© {best_attention_model[0]} ä½œä¸ºæ®‹å·®å­¦ä¹ çš„åŸºç¡€æ¨¡å‹")

base_oof_pred = best_attention_model[1]
base_test_pred = best_attention_model[2]

# è®¡ç®—æ®‹å·®
residual_train = y_train_seq - base_oof_pred


# ç®€åŒ–ç‰¹å¾
def create_simplified_features(X_flat, base_pred):
    features_list = [X_flat]
    features_list.append(base_pred.reshape(-1, 1))
    return np.hstack(features_list)


basic_features_train = X_train_flat[:len(base_oof_pred)]
basic_features_test = X_test_flat

simplified_train = create_simplified_features(basic_features_train, base_oof_pred)
simplified_test = create_simplified_features(basic_features_test, base_test_pred)

print(f"\nç‰¹å¾ç»´åº¦: {simplified_train.shape[1]} ç»´")

# è®­ç»ƒä¿å®ˆXGBoost
print("\nè®­ç»ƒXGBoostæ®‹å·®æ¨¡å‹...")
xgb_residual = XGBRegressor(
    n_estimators=100,
    learning_rate=0.01,
    max_depth=3,
    min_child_weight=5,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    verbosity=0
)
xgb_residual.fit(simplified_train, residual_train)

# é¢„æµ‹æ®‹å·®
residual_pred = xgb_residual.predict(simplified_test)

# æœ€ç»ˆé¢„æµ‹
final_pred = base_test_pred + residual_pred
final_r2 = r2_score(y_test_seq, final_pred)

print(f"âœ“ XGBoostæ®‹å·®å­¦ä¹ å®Œæˆ")
print(f"åŸºç¡€æ¨¡å‹ RÂ²: {r2_score(y_test_seq, base_test_pred):.4f}")
print(f"æ®‹å·®å­¦ä¹ å RÂ²: {final_r2:.4f} (æå‡: {final_r2 - r2_score(y_test_seq, base_test_pred):+.4f})")

# ========== ç»“æœæ±‡æ€» ==========
print("\n" + "=" * 100)
print("æœ€ç»ˆç»“æœæ±‡æ€»ï¼ˆåŸå§‹å°ºåº¦ï¼‰".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()

all_predictions = {
    'åŸºç¡€LSTM': lstm_test_pred,
    'åŸºç¡€GRU': gru_test_pred,
    'LSTM+Attention': lstm_attn_test_pred,
    'GRU+Attention': gru_attn_test_pred,
    'Stacked LSTM+Attention': stacked_attn_test_pred,
    'æ‰€æœ‰æ¨¡å‹å¹³å‡': avg_all,
    'Attentionæ¨¡å‹å¹³å‡': avg_attention_only,
    'åŠ æƒèåˆ': weighted_pred,
    'Stackingèåˆ': stacking_pred,
    f'{best_attention_model[0]}+XGBoost': final_pred
}

print(f"\n{'æ¨¡å‹':<30} {'RÂ²':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 80)

results_list = []
for name, pred in all_predictions.items():
    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()

    r2 = r2_score(y_test_original, pred_orig)
    mae = mean_absolute_error(y_test_original, pred_orig)
    rmse = sqrt(mean_squared_error(y_test_original, pred_orig))
    mape = np.mean(np.abs((pred_orig - y_test_original) / (y_test_original + 1e-8)))

    results_list.append((name, r2, mae, rmse, mape, pred_orig))
    print(f"{name:<30} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")

# æ’åº
results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’å".center(100))
print("=" * 100)

for rank, (name, r2, mae, rmse, mape, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<30} RÂ²={r2:.4f}")

best_name = results_list[0][0]
best_r2 = results_list[0][1]
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_name} (RÂ² = {best_r2:.4f})")

# ========== å¯è§†åŒ– ==========
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

# 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
fig, ax = plt.subplots(figsize=(14, 8))

models = ['åŸºç¡€LSTM', 'åŸºç¡€GRU', 'LSTM+Attn', 'GRU+Attn', 'Stacked+Attn',
          'ç®€å•å¹³å‡', 'Attnå¹³å‡', 'åŠ æƒèåˆ', 'Stacking', best_attention_model[0] + '+XGB']
r2_scores_list = [
    r2_score(y_test_seq, lstm_test_pred),
    r2_score(y_test_seq, gru_test_pred),
    r2_score(y_test_seq, lstm_attn_test_pred),
    r2_score(y_test_seq, gru_attn_test_pred),
    r2_score(y_test_seq, stacked_attn_test_pred),
    r2_score(y_test_seq, avg_all),
    r2_score(y_test_seq, avg_attention_only),
    r2_score(y_test_seq, weighted_pred),
    r2_score(y_test_seq, stacking_pred),
    final_r2
]

colors = ['#FF6B6B', '#FF6B6B', '#4ECDC4', '#4ECDC4', '#4ECDC4',
          '#95E1D3', '#95E1D3', '#F38181', '#AA96DA', '#FFD93D']

bars = ax.barh(models, r2_scores_list, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)

for i, (bar, score) in enumerate(zip(bars, r2_scores_list)):
    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
            f'{score:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)

ax.set_xlabel('RÂ² Score', fontsize=13, fontweight='bold')
ax.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆå¸¦Attentionæœºåˆ¶ï¼‰', fontsize=15, fontweight='bold', pad=20)
ax.grid(True, alpha=0.3, axis='x')
ax.set_xlim([min(r2_scores_list) - 0.05, max(r2_scores_list) + 0.05])

plt.tight_layout()
plt.savefig(results_directory + 'attention_model_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: attention_model_comparison.png")
plt.close()

# 2. Attention vs åŸºç¡€æ¨¡å‹å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

comparisons = [
    ('LSTM', lstm_test_pred, 'LSTM+Attention', lstm_attn_test_pred),
    ('GRU', gru_test_pred, 'GRU+Attention', gru_attn_test_pred)
]

for idx, (name1, pred1, name2, pred2) in enumerate(comparisons):
    # å·¦å›¾ï¼šåŸºç¡€æ¨¡å‹
    ax1 = axes[idx, 0]
    pred1_orig = y_scaler.inverse_transform(pred1.reshape(-1, 1)).flatten()
    r2_1 = r2_score(y_test_original, pred1_orig)

    ax1.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax1.plot(pred1_orig, label=name1, linewidth=2, alpha=0.7, color='#FF6B6B')
    ax1.set_title(f'{name1}\nRÂ²={r2_1:.4f}', fontsize=12, fontweight='bold')
    ax1.set_xlabel('æ ·æœ¬åºå·', fontsize=10)
    ax1.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=10)
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3)

    # å³å›¾ï¼šAttentionæ¨¡å‹
    ax2 = axes[idx, 1]
    pred2_orig = y_scaler.inverse_transform(pred2.reshape(-1, 1)).flatten()
    r2_2 = r2_score(y_test_original, pred2_orig)
    improvement = r2_2 - r2_1

    ax2.plot(y_test_original, label='çœŸå®å€¼', linewidth=2.5, color='black', alpha=0.8)
    ax2.plot(pred2_orig, label=name2, linewidth=2, alpha=0.7, color='#4ECDC4')
    ax2.set_title(f'{name2}\nRÂ²={r2_2:.4f} (æå‡: {improvement:+.4f})',
                  fontsize=12, fontweight='bold')
    ax2.set_xlabel('æ ·æœ¬åºå·', fontsize=10)
    ax2.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=10)
    ax2.legend(fontsize=9)
    ax2.grid(True, alpha=0.3)

plt.suptitle('Attentionæœºåˆ¶å¯¹æ¯”åŸºç¡€æ¨¡å‹', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(results_directory + 'attention_vs_baseline.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: attention_vs_baseline.png")
plt.close()

# 3. èåˆç­–ç•¥å¯¹æ¯”
fig, ax = plt.subplots(figsize=(16, 6))

fusion_preds = {
    'çœŸå®å€¼': y_test_original,
    'æ‰€æœ‰æ¨¡å‹å¹³å‡': y_scaler.inverse_transform(avg_all.reshape(-1, 1)).flatten(),
    'Attentionå¹³å‡': y_scaler.inverse_transform(avg_attention_only.reshape(-1, 1)).flatten(),
    'åŠ æƒèåˆ': y_scaler.inverse_transform(weighted_pred.reshape(-1, 1)).flatten(),
    'Stackingèåˆ': y_scaler.inverse_transform(stacking_pred.reshape(-1, 1)).flatten(),
    best_attention_model[0] + '+XGBoost': y_scaler.inverse_transform(final_pred.reshape(-1, 1)).flatten()
}

colors_fusion = ['black', '#95E1D3', '#4ECDC4', '#F38181', '#AA96DA', '#FFD93D']
line_styles = ['-', '--', '--', '--', '--', '--']
line_widths = [3, 2, 2, 2, 2, 2.5]

for (name, pred), color, style, width in zip(fusion_preds.items(), colors_fusion, line_styles, line_widths):
    if name == 'çœŸå®å€¼':
        ax.plot(pred, label=name, linewidth=width, alpha=0.9, color=color, linestyle=style)
    else:
        ax.plot(pred, label=name, linewidth=width, alpha=0.7, color=color, linestyle=style)

ax.set_xlabel('æ ·æœ¬åºå·', fontsize=12, fontweight='bold')
ax.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=12, fontweight='bold')
ax.set_title('ä¸åŒèåˆç­–ç•¥é¢„æµ‹æ•ˆæœå¯¹æ¯”', fontsize=15, fontweight='bold', pad=15)
ax.legend(fontsize=10, loc='best')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'fusion_strategies_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: fusion_strategies_comparison.png")
plt.close()

# 4. æ®‹å·®åˆ†æ
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

residual_preds = [
    ('åŸºç¡€LSTM', lstm_test_pred, '#FF6B6B'),
    ('LSTM+Attention', lstm_attn_test_pred, '#4ECDC4'),
    ('åŸºç¡€GRU', gru_test_pred, '#FF6B6B'),
    ('GRU+Attention', gru_attn_test_pred, '#4ECDC4'),
    ('Stacked+Attention', stacked_attn_test_pred, '#4ECDC4'),
    ('Stackingèåˆ', stacking_pred, '#AA96DA')
]

for idx, (name, pred, color) in enumerate(residual_preds):
    ax = axes[idx // 3, idx % 3]
    residual = y_test_seq - pred

    ax.hist(residual, bins=30, color=color, alpha=0.7, edgecolor='black', linewidth=1.2)
    ax.axvline(0, color='black', linestyle='--', linewidth=2)
    ax.set_title(f'{name}\nstd={np.std(residual):.5f}', fontsize=11, fontweight='bold')
    ax.set_xlabel('æ®‹å·®', fontsize=10)
    ax.set_ylabel('é¢‘æ•°', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

plt.suptitle('æ®‹å·®åˆ†å¸ƒåˆ†æ', fontsize=15, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(results_directory + 'residual_analysis_attention.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: residual_analysis_attention.png")
plt.close()

# 5. Attentionæƒé‡å¯è§†åŒ–
print("\nç”ŸæˆAttentionæƒé‡å¯è§†åŒ–...")
try:
    sample_idx = 0
    sample_input = X_test_seq[sample_idx:sample_idx + 1]

    # åˆ›å»ºå¯è§†åŒ–æ¨¡å‹ï¼ˆè¾“å‡ºattentionæƒé‡ï¼‰
    # ä¿®å¤ï¼šæ‰¾åˆ°æ­£ç¡®çš„Attentionå±‚
    attention_layer = None
    for layer in lstm_attn_final.layers:
        if isinstance(layer, AttentionLayer):
            attention_layer = layer
            break

    if attention_layer is not None:
        # åˆ›å»ºä¸­é—´æ¨¡å‹æ¥è·å–LSTMè¾“å‡º
        lstm_layer = lstm_attn_final.layers[1]  # LSTMå±‚
        intermediate_model = Model(
            inputs=lstm_attn_final.input,
            outputs=lstm_layer.output
        )

        # è·å–LSTMè¾“å‡º
        lstm_output = intermediate_model.predict(sample_input, verbose=0)

        # æ‰‹åŠ¨è®¡ç®—attentionæƒé‡
        context_vector, attention_weights = attention_layer(lstm_output)
        attention_weights = attention_weights.numpy()[0].flatten()

        fig, ax = plt.subplots(figsize=(12, 6))
        time_steps = range(1, len(attention_weights) + 1)

        bars = ax.bar(time_steps, attention_weights, color='#4ECDC4', alpha=0.7, edgecolor='black', linewidth=1.5)
        ax.set_xlabel('æ—¶é—´æ­¥ (Time Step)', fontsize=12, fontweight='bold')
        ax.set_ylabel('æ³¨æ„åŠ›æƒé‡ (Attention Weight)', fontsize=12, fontweight='bold')
        ax.set_title('LSTM+Attentionæ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒç¤ºä¾‹', fontsize=14, fontweight='bold', pad=15)
        ax.grid(True, alpha=0.3, axis='y')

        # æ ‡æ³¨æœ€é«˜æƒé‡
        max_idx = np.argmax(attention_weights)
        ax.annotate(f'æœ€å¤§æƒé‡\n{attention_weights[max_idx]:.3f}',
                    xy=(max_idx + 1, attention_weights[max_idx]),
                    xytext=(max_idx + 1, attention_weights[max_idx] + 0.05),
                    arrowprops=dict(arrowstyle='->', color='red', lw=2),
                    fontsize=11, fontweight='bold', color='red',
                    ha='center')

        plt.tight_layout()
        plt.savefig(results_directory + 'attention_weights_visualization.png', dpi=300, bbox_inches='tight')
        print("âœ“ ä¿å­˜: attention_weights_visualization.png")
        plt.close()
    else:
        print("âš  æœªæ‰¾åˆ°Attentionå±‚ï¼Œè·³è¿‡æƒé‡å¯è§†åŒ–")

except Exception as e:
    print(f"âš  Attentionæƒé‡å¯è§†åŒ–å¤±è´¥: {e}")
    print("  ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")

# 6. å¤šä¸ªæ ·æœ¬çš„Attentionæƒé‡çƒ­åŠ›å›¾
print("\nç”ŸæˆAttentionæƒé‡çƒ­åŠ›å›¾...")
try:
    n_samples = min(20, len(X_test_seq))
    attention_weights_matrix = []

    # æ‰¾åˆ°Attentionå±‚
    attention_layer = None
    for layer in lstm_attn_final.layers:
        if isinstance(layer, AttentionLayer):
            attention_layer = layer
            break

    if attention_layer is not None:
        # è·å–LSTMå±‚è¾“å‡º
        lstm_layer = lstm_attn_final.layers[1]
        intermediate_model = Model(
            inputs=lstm_attn_final.input,
            outputs=lstm_layer.output
        )

        for i in range(n_samples):
            sample = X_test_seq[i:i + 1]
            lstm_out = intermediate_model.predict(sample, verbose=0)
            _, attn_weights = attention_layer(lstm_out)
            attention_weights_matrix.append(attn_weights.numpy()[0].flatten())

        attention_weights_matrix = np.array(attention_weights_matrix)

        fig, ax = plt.subplots(figsize=(12, 8))
        im = ax.imshow(attention_weights_matrix, cmap='YlOrRd', aspect='auto')

        ax.set_xlabel('æ—¶é—´æ­¥', fontsize=12, fontweight='bold')
        ax.set_ylabel('æµ‹è¯•æ ·æœ¬', fontsize=12, fontweight='bold')
        ax.set_title('å¤šæ ·æœ¬Attentionæƒé‡çƒ­åŠ›å›¾', fontsize=14, fontweight='bold', pad=15)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('æ³¨æ„åŠ›æƒé‡', fontsize=11, fontweight='bold')

        plt.tight_layout()
        plt.savefig(results_directory + 'attention_heatmap.png', dpi=300, bbox_inches='tight')
        print("âœ“ ä¿å­˜: attention_heatmap.png")
        plt.close()
    else:
        print("âš  æœªæ‰¾åˆ°Attentionå±‚ï¼Œè·³è¿‡çƒ­åŠ›å›¾")

except Exception as e:
    print(f"âš  Attentionçƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
    print("  ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")

# 7. é›·è¾¾å›¾å¯¹æ¯”
print("\nç”Ÿæˆé›·è¾¾å›¾...")
fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

radar_strategies = [
    'åŸºç¡€LSTM',
    'LSTM+Attention',
    'Stackingèåˆ',
    best_attention_model[0] + '+XGBoost'
]

metrics_names = ['RÂ²', '1-MAE', '1-RMSE', 'ç¨³å®šæ€§', 'é€Ÿåº¦']
n_metrics = len(metrics_names)

angles = [n / float(n_metrics) * 2 * pi for n in range(n_metrics)]
angles += angles[:1]

for strategy_name in radar_strategies:
    pred = all_predictions[strategy_name]

    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    stability = 1 - np.std(y_test_seq - pred) / np.std(y_test_seq)

    # é€Ÿåº¦è¯„åˆ†
    if 'åŸºç¡€LSTM' == strategy_name:
        speed = 1.0
    elif 'Attention' in strategy_name and 'Stacked' not in strategy_name:
        speed = 0.8
    elif 'Stacked' in strategy_name:
        speed = 0.6
    elif 'XGBoost' in strategy_name:
        speed = 0.5
    else:
        speed = 0.7

    # å½’ä¸€åŒ–
    max_mae = 0.15
    max_rmse = 0.15
    values = [
        r2,
        max(0, 1 - (mae / max_mae)),
        max(0, 1 - (rmse / max_rmse)),
        max(0, stability),
        speed
    ]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=2, label=strategy_name, alpha=0.7)
    ax.fill(angles, values, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics_names, fontsize=11, fontweight='bold')
ax.set_ylim(0, 1)
ax.set_title('æ¨¡å‹å¤šç»´è¯„ä¼°é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=30)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
ax.grid(True)

plt.tight_layout()
plt.savefig(results_directory + 'models_radar_chart.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: models_radar_chart.png")
plt.close()

# ========== ä¿å­˜æ¨¡å‹ ==========
print("\n" + "=" * 100)
print("ä¿å­˜æ¨¡å‹å’Œç»“æœ".center(100))
print("=" * 100)

# ä¿å­˜Kerasæ¨¡å‹
lstm_final.save(results_directory + 'lstm_base_model.h5')
gru_final.save(results_directory + 'gru_base_model.h5')
lstm_attn_final.save(results_directory + 'lstm_attention_model.h5')
gru_attn_final.save(results_directory + 'gru_attention_model.h5')
stacked_attn_final.save(results_directory + 'stacked_lstm_attention_model.h5')

# ä¿å­˜XGBoostæ¨¡å‹
with open(results_directory + 'xgboost_residual_model.pkl', 'wb') as f:
    pickle.dump(xgb_residual, f)

# ä¿å­˜Ridge metaæ¨¡å‹
with open(results_directory + 'ridge_stacking_model.pkl', 'wb') as f:
    pickle.dump(ridge_meta, f)

# ä¿å­˜å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original}
for name, _, _, _, _, pred_orig in results_list:
    predictions_dict[name.replace('/', '_').replace('+', '_')] = pred_orig

results_df = pd.DataFrame(predictions_dict)
results_df.to_csv(results_directory + 'all_predictions_with_attention.csv', index=False)

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
metrics_data = []
for name, r2, mae, rmse, mape, _ in results_list:
    metrics_data.append({
        'model': name,
        'r2': r2,
        'mae': mae,
        'rmse': rmse,
        'mape': mape
    })

metrics_df = pd.DataFrame(metrics_data)
metrics_df.to_csv(results_directory + 'performance_metrics_attention.csv', index=False)

print("\nâœ“ æ¨¡å‹ä¿å­˜å®Œæˆï¼")
print(f"  - lstm_base_model.h5")
print(f"  - gru_base_model.h5")
print(f"  - lstm_attention_model.h5")
print(f"  - gru_attention_model.h5")
print(f"  - stacked_lstm_attention_model.h5")
print(f"  - xgboost_residual_model.pkl")
print(f"  - ridge_stacking_model.pkl")
print(f"  - scalers.pkl")
print(f"  - all_predictions_with_attention.csv")
print(f"  - performance_metrics_attention.csv")

# ========== æœ€ç»ˆæ€»ç»“ ==========
print("\n" + "=" * 100)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ€»ç»“æŠ¥å‘Š".center(100))
print("=" * 100)

print(f"\nğŸ“Š æ ¸å¿ƒæ”¹è¿›æ•ˆæœ:")
base_lstm_r2 = r2_score(y_test_seq, lstm_test_pred)
lstm_attn_r2 = r2_score(y_test_seq, lstm_attn_test_pred)
base_gru_r2 = r2_score(y_test_seq, gru_test_pred)
gru_attn_r2 = r2_score(y_test_seq, gru_attn_test_pred)

print(f"  âœ“ LSTM + Attention: RÂ² {base_lstm_r2:.4f} â†’ {lstm_attn_r2:.4f} (æå‡: {lstm_attn_r2 - base_lstm_r2:+.4f})")
print(f"  âœ“ GRU + Attention: RÂ² {base_gru_r2:.4f} â†’ {gru_attn_r2:.4f} (æå‡: {gru_attn_r2 - base_gru_r2:+.4f})")

print(f"\nğŸ’¡ Attentionæœºåˆ¶çš„ä¼˜åŠ¿:")
print(f"  1. è‡ªåŠ¨å­¦ä¹ æ—¶é—´åºåˆ—ä¸­ä¸åŒæ—¶é—´æ­¥çš„é‡è¦æ€§")
print(f"  2. æé«˜æ¨¡å‹å¯¹å…³é”®å†å²ä¿¡æ¯çš„å…³æ³¨åº¦")
print(f"  3. å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼ˆé€šè¿‡æƒé‡å¯è§†åŒ–ï¼‰")
print(f"  4. åœ¨é•¿åºåˆ—é¢„æµ‹ä¸­æ•ˆæœå°¤ä¸ºæ˜æ˜¾")
print(f"  5. å¯ä»¥é€šè¿‡çƒ­åŠ›å›¾è§‚å¯Ÿæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_name}")
print(f"   æœ€ç»ˆæ€§èƒ½: RÂ² = {best_r2:.4f}")
print(f"   MAE = {results_list[0][2]:.2f}")
print(f"   RMSE = {results_list[0][3]:.2f}")
print(f"   MAPE = {results_list[0][4]:.6f}")

print(f"\nğŸ“ˆ æ¨¡å‹æ’åï¼ˆTop 5ï¼‰:")
for rank in range(min(5, len(results_list))):
    name, r2, mae, rmse, mape, _ = results_list[rank]
    print(f"  {rank + 1}. {name:<30} RÂ²={r2:.4f}")

print(f"\nğŸ¯ ä½¿ç”¨å»ºè®®:")
print(f"  â€¢ å¦‚æœè¿½æ±‚æœ€é«˜ç²¾åº¦: ä½¿ç”¨ {best_name}")
print(f"  â€¢ å¦‚æœéœ€è¦å¯è§£é‡Šæ€§: ä½¿ç”¨ Attentionæ¨¡å‹ + æƒé‡å¯è§†åŒ–")
print(f"  â€¢ å¦‚æœéœ€è¦ç¨³å®šæ€§: ä½¿ç”¨ Stackingèåˆ")
print(f"  â€¢ å¦‚æœè®¡ç®—èµ„æºæœ‰é™: ä½¿ç”¨ å•ä¸ªAttentionæ¨¡å‹")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_directory}")
print(f"   - 7å¼ å¯è§†åŒ–å›¾è¡¨")
print(f"   - 5ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹")
print(f"   - 2ä¸ªé›†æˆå­¦ä¹ æ¨¡å‹")
print(f"   - å®Œæ•´çš„é¢„æµ‹ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡")

print("\n" + "=" * 100)
print("æ„Ÿè°¢ä½¿ç”¨ï¼å¦‚éœ€é¢„æµ‹æ–°æ•°æ®ï¼Œè¯·åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œå½’ä¸€åŒ–å™¨ã€‚".center(100))
# ==================================================================================
# å°†ä»¥ä¸‹ä»£ç è¿½åŠ åˆ°ä¸»ä»£ç çš„"æ„Ÿè°¢ä½¿ç”¨"é‚£æ®µä¹‹å
# ==================================================================================

print("\n" + "=" * 100)
print("ç”Ÿæˆé¢å¤–çš„é«˜çº§å¯è§†åŒ–".center(100))
print("=" * 100)

# 8. é¢„æµ‹è¯¯å·®åˆ†æå¯¹æ¯”å›¾
print("\nç”Ÿæˆé¢„æµ‹è¯¯å·®åˆ†æ...")
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# é€‰æ‹©4ä¸ªå…³é”®æ¨¡å‹
key_models = [
    ('åŸºç¡€LSTM', lstm_test_pred, '#FF6B6B'),
    ('LSTM+Attention', lstm_attn_test_pred, '#4ECDC4'),
    ('GRU+Attention', gru_attn_test_pred, '#95E1D3'),
    ('åŠ æƒèåˆ', weighted_pred, '#F38181')
]

for idx, (name, pred, color) in enumerate(key_models):
    ax = axes[idx // 2, idx % 2]

    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()

    # æ•£ç‚¹å›¾ï¼šé¢„æµ‹å€¼ vs çœŸå®å€¼
    ax.scatter(y_test_original, pred_orig, alpha=0.6, s=50, color=color, edgecolors='black', linewidth=0.5)

    # æ·»åŠ ç†æƒ³çº¿ï¼ˆy=xï¼‰
    min_val = min(y_test_original.min(), pred_orig.min())
    max_val = max(y_test_original.max(), pred_orig.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label='ç†æƒ³é¢„æµ‹çº¿')

    # è®¡ç®—RÂ²
    r2 = r2_score(y_test_original, pred_orig)
    mae = mean_absolute_error(y_test_original, pred_orig)

    ax.set_xlabel('çœŸå®å€¼', fontsize=11, fontweight='bold')
    ax.set_ylabel('é¢„æµ‹å€¼', fontsize=11, fontweight='bold')
    ax.set_title(f'{name}\nRÂ²={r2:.4f}, MAE={mae:.2f}', fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)

plt.suptitle('é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾', fontsize=15, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(results_directory + 'prediction_scatter_plots.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: prediction_scatter_plots.png")
plt.close()

# 9. æ—¶åºè¯¯å·®è¶‹åŠ¿å›¾
print("\nç”Ÿæˆæ—¶åºè¯¯å·®è¶‹åŠ¿...")
fig, axes = plt.subplots(2, 1, figsize=(16, 10))

# ä¸Šå›¾ï¼šç»å¯¹è¯¯å·®
ax1 = axes[0]
for name, pred, color in [
    ('åŸºç¡€LSTM', lstm_test_pred, '#FF6B6B'),
    ('LSTM+Attention', lstm_attn_test_pred, '#4ECDC4'),
    ('GRU+Attention', gru_attn_test_pred, '#95E1D3'),
    ('åŠ æƒèåˆ', weighted_pred, '#F38181')
]:
    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    abs_error = np.abs(pred_orig - y_test_original)
    ax1.plot(abs_error, label=name, linewidth=2, alpha=0.7, color=color)

ax1.set_xlabel('æ ·æœ¬åºå·', fontsize=11, fontweight='bold')
ax1.set_ylabel('ç»å¯¹è¯¯å·®', fontsize=11, fontweight='bold')
ax1.set_title('æ—¶åºç»å¯¹è¯¯å·®å˜åŒ–', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# ä¸‹å›¾ï¼šç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”
ax2 = axes[1]
for name, pred, color in [
    ('åŸºç¡€LSTM', lstm_test_pred, '#FF6B6B'),
    ('LSTM+Attention', lstm_attn_test_pred, '#4ECDC4'),
    ('GRU+Attention', gru_attn_test_pred, '#95E1D3'),
    ('åŠ æƒèåˆ', weighted_pred, '#F38181')
]:
    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    relative_error = np.abs((pred_orig - y_test_original) / y_test_original) * 100
    ax2.plot(relative_error, label=name, linewidth=2, alpha=0.7, color=color)

ax2.set_xlabel('æ ·æœ¬åºå·', fontsize=11, fontweight='bold')
ax2.set_ylabel('ç›¸å¯¹è¯¯å·® (%)', fontsize=11, fontweight='bold')
ax2.set_title('æ—¶åºç›¸å¯¹è¯¯å·®å˜åŒ–', fontsize=13, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'error_trend_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: error_trend_analysis.png")
plt.close()

# 10. æ¨¡å‹æ€§èƒ½ç»¼åˆå¯¹æ¯”çƒ­åŠ›å›¾
print("\nç”Ÿæˆæ€§èƒ½ç»¼åˆå¯¹æ¯”çƒ­åŠ›å›¾...")
fig, ax = plt.subplots(figsize=(12, 8))

# å‡†å¤‡æ•°æ® - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
models_for_heatmap = [
    'åŸºç¡€LSTM', 'åŸºç¡€GRU', 'LSTM+Attention', 'GRU+Attention',
    'Stacked LSTM+Attention', 'åŠ æƒèåˆ', 'Stackingèåˆ'  # ä¿®å¤ï¼šè¿™é‡Œæ”¹ä¸ºå®Œæ•´åç§°
]

metrics_for_heatmap = []
for model_name in models_for_heatmap:
    pred = all_predictions[model_name]
    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()

    r2 = r2_score(y_test_original, pred_orig)
    mae = mean_absolute_error(y_test_original, pred_orig)
    rmse = sqrt(mean_squared_error(y_test_original, pred_orig))
    mape = np.mean(np.abs((pred_orig - y_test_original) / (y_test_original + 1e-8)))

    # å½’ä¸€åŒ–åˆ°0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    metrics_for_heatmap.append([
        r2,  # RÂ² å·²ç»æ˜¯0-1
        1 - (mae / 100),  # å½’ä¸€åŒ–MAE
        1 - (rmse / 100),  # å½’ä¸€åŒ–RMSE
        1 - (mape * 10)  # å½’ä¸€åŒ–MAPE
    ])

metrics_for_heatmap = np.array(metrics_for_heatmap)

# ç»˜åˆ¶çƒ­åŠ›å›¾
im = ax.imshow(metrics_for_heatmap.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)

# è®¾ç½®åæ ‡è½´
ax.set_xticks(range(len(models_for_heatmap)))
ax.set_xticklabels(models_for_heatmap, rotation=45, ha='right', fontsize=10)
ax.set_yticks(range(4))
ax.set_yticklabels(['RÂ²', '1-norm(MAE)', '1-norm(RMSE)', '1-norm(MAPE)'], fontsize=11)

# æ·»åŠ æ•°å€¼æ ‡æ³¨
for i in range(len(models_for_heatmap)):
    for j in range(4):
        text = ax.text(i, j, f'{metrics_for_heatmap[i, j]:.3f}',
                       ha="center", va="center", color="black", fontsize=9, fontweight='bold')

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(im, ax=ax)
cbar.set_label('å½’ä¸€åŒ–æ€§èƒ½åˆ†æ•°\n(è¶Šé«˜è¶Šå¥½)', fontsize=11, fontweight='bold')

ax.set_title('æ¨¡å‹æ€§èƒ½ç»¼åˆå¯¹æ¯”çƒ­åŠ›å›¾', fontsize=14, fontweight='bold', pad=15)

plt.tight_layout()
plt.savefig(results_directory + 'performance_heatmap.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: performance_heatmap.png")
plt.close()

# 11. ç®±çº¿å›¾å¯¹æ¯”è¯¯å·®åˆ†å¸ƒ
print("\nç”Ÿæˆè¯¯å·®åˆ†å¸ƒç®±çº¿å›¾...")
fig, ax = plt.subplots(figsize=(14, 7))

error_data = []
error_labels = []

for name in ['åŸºç¡€LSTM', 'LSTM+Attention', 'GRU+Attention', 'åŠ æƒèåˆ', 'Stackingèåˆ']:
    pred = all_predictions[name]
    pred_orig = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()
    error = pred_orig - y_test_original
    error_data.append(error)
    error_labels.append(name)

bp = ax.boxplot(error_data, labels=error_labels, patch_artist=True,
                showmeans=True, meanline=True)

# è®¾ç½®é¢œè‰²
colors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#F38181', '#AA96DA']
for patch, color in zip(bp['boxes'], colors):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)

ax.axhline(y=0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®çº¿')
ax.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=12, fontweight='bold')
ax.set_title('æ¨¡å‹é¢„æµ‹è¯¯å·®åˆ†å¸ƒå¯¹æ¯”ï¼ˆç®±çº¿å›¾ï¼‰', fontsize=14, fontweight='bold', pad=15)
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3, axis='y')
plt.xticks(rotation=15, ha='right')

plt.tight_layout()
plt.savefig(results_directory + 'error_boxplot_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ä¿å­˜: error_boxplot_comparison.png")
plt.close()

# 12. å­¦ä¹ æ›²çº¿å¯¹æ¯”
print("\nç”Ÿæˆå­¦ä¹ æ›²çº¿å¯¹æ¯”...")
print("  é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥è·å–è®­ç»ƒå†å²...")

try:
    # åŸºç¡€LSTM
    lstm_simple = build_simple_lstm((X_train_seq.shape[1], X_train_seq.shape[2]))
    lstm_simple.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))
    history_lstm = lstm_simple.fit(
        X_train_seq, y_train_seq,
        validation_split=0.2,
        epochs=50,
        batch_size=32,
        verbose=0
    )

    # LSTM+Attention
    lstm_attn_temp = build_lstm_with_attention((X_train_seq.shape[1], X_train_seq.shape[2]))
    lstm_attn_temp.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))
    history_attn = lstm_attn_temp.fit(
        X_train_seq, y_train_seq,
        validation_split=0.2,
        epochs=50,
        batch_size=32,
        verbose=0
    )

    fig, axes = plt.subplots(1, 2, figsize=(16, 5))

    # è®­ç»ƒæŸå¤±å¯¹æ¯”
    ax1 = axes[0]
    ax1.plot(history_lstm.history['loss'], label='åŸºç¡€LSTM - è®­ç»ƒ', linewidth=2, color='#FF6B6B')
    ax1.plot(history_lstm.history['val_loss'], label='åŸºç¡€LSTM - éªŒè¯', linewidth=2,
             linestyle='--', color='#FF6B6B')
    ax1.plot(history_attn.history['loss'], label='LSTM+Attention - è®­ç»ƒ', linewidth=2, color='#4ECDC4')
    ax1.plot(history_attn.history['val_loss'], label='LSTM+Attention - éªŒè¯', linewidth=2,
             linestyle='--', color='#4ECDC4')

    ax1.set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=11, fontweight='bold')
    ax1.set_ylabel('æŸå¤± (MSE)', fontsize=11, fontweight='bold')
    ax1.set_title('å­¦ä¹ æ›²çº¿å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3)

    # éªŒè¯æŸå¤±æ”¾å¤§å¯¹æ¯”
    ax2 = axes[1]
    ax2.plot(history_lstm.history['val_loss'], label='åŸºç¡€LSTM', linewidth=2.5, color='#FF6B6B')
    ax2.plot(history_attn.history['val_loss'], label='LSTM+Attention', linewidth=2.5, color='#4ECDC4')

    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=11, fontweight='bold')
    ax2.set_ylabel('éªŒè¯æŸå¤± (MSE)', fontsize=11, fontweight='bold')
    ax2.set_title('éªŒè¯æŸå¤±å¯¹æ¯”ï¼ˆAttentionæ”¶æ•›æ›´å¿«ï¼‰', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(results_directory + 'learning_curves_comparison.png', dpi=300, bbox_inches='tight')
    print("âœ“ ä¿å­˜: learning_curves_comparison.png")
    plt.close()

    # æ¸…ç†ä¸´æ—¶æ¨¡å‹
    del lstm_simple, lstm_attn_temp
    tf.keras.backend.clear_session()

except Exception as e:
    print(f"âš  å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
    print("  è·³è¿‡è¯¥å›¾è¡¨ï¼Œç»§ç»­æ‰§è¡Œ...")

# 13. æœ€ç»ˆç»¼åˆæŠ¥å‘Šå›¾
print("\nç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Šå›¾...")
try:
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # 1. RÂ²æ’åï¼ˆå·¦ä¸Šï¼‰
    ax1 = fig.add_subplot(gs[0, :2])
    models_sorted = sorted(
        [(name, r2_score(y_test_seq, all_predictions[name]))
         for name in ['åŸºç¡€LSTM', 'åŸºç¡€GRU', 'LSTM+Attention', 'GRU+Attention',
                      'åŠ æƒèåˆ', 'Stackingèåˆ']],
        key=lambda x: x[1], reverse=True
    )
    names = [m[0] for m in models_sorted]
    scores = [m[1] for m in models_sorted]
    colors_bar = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(names)))

    bars = ax1.barh(names, scores, color=colors_bar, alpha=0.8, edgecolor='black')
    for bar, score in zip(bars, scores):
        ax1.text(bar.get_width() - 0.02, bar.get_y() + bar.get_height() / 2,
                 f'{score:.4f}', ha='right', va='center', fontweight='bold', fontsize=9, color='white')
    ax1.set_xlabel('RÂ² Score', fontsize=10, fontweight='bold')
    ax1.set_title('æ¨¡å‹æ€§èƒ½æ’å', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='x')

    # 2. æœ€ä½³æ¨¡å‹é¢„æµ‹å›¾ï¼ˆå³ä¸Šï¼‰
    ax2 = fig.add_subplot(gs[0, 2])
    best_pred_orig = y_scaler.inverse_transform(weighted_pred.reshape(-1, 1)).flatten()
    ax2.plot(y_test_original[:50], label='çœŸå®å€¼', linewidth=2.5, color='black', marker='o', markersize=4)
    ax2.plot(best_pred_orig[:50], label='åŠ æƒèåˆ', linewidth=2, color='#F38181', marker='s', markersize=3)
    ax2.set_xlabel('æ ·æœ¬åºå·', fontsize=9)
    ax2.set_ylabel('ç‰ç±³ä»·æ ¼', fontsize=9)
    ax2.set_title('æœ€ä½³æ¨¡å‹é¢„æµ‹ï¼ˆå‰50æ ·æœ¬ï¼‰', fontsize=11, fontweight='bold')
    ax2.legend(fontsize=8)
    ax2.grid(True, alpha=0.3)

    # 3. Attentionæå‡å¯¹æ¯”ï¼ˆä¸­å·¦ï¼‰
    ax3 = fig.add_subplot(gs[1, 0])
    base_models = ['LSTM', 'GRU']
    base_scores = [
        r2_score(y_test_seq, lstm_test_pred),
        r2_score(y_test_seq, gru_test_pred)
    ]
    attn_scores = [
        r2_score(y_test_seq, lstm_attn_test_pred),
        r2_score(y_test_seq, gru_attn_test_pred)
    ]

    x = np.arange(len(base_models))
    width = 0.35
    ax3.bar(x - width / 2, base_scores, width, label='åŸºç¡€æ¨¡å‹', color='#FF6B6B', alpha=0.8)
    ax3.bar(x + width / 2, attn_scores, width, label='+ Attention', color='#4ECDC4', alpha=0.8)

    ax3.set_ylabel('RÂ² Score', fontsize=9, fontweight='bold')
    ax3.set_title('Attentionæœºåˆ¶æå‡æ•ˆæœ', fontsize=11, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(base_models)
    ax3.legend(fontsize=8)
    ax3.grid(True, alpha=0.3, axis='y')

    # 4. è¯¯å·®åˆ†å¸ƒå°æç´å›¾ï¼ˆä¸­ä¸­ï¼‰
    ax4 = fig.add_subplot(gs[1, 1])
    error_lstm = y_scaler.inverse_transform(lstm_test_pred.reshape(-1, 1)).flatten() - y_test_original
    error_attn = y_scaler.inverse_transform(lstm_attn_test_pred.reshape(-1, 1)).flatten() - y_test_original

    parts = ax4.violinplot([error_lstm, error_attn], positions=[1, 2], showmeans=True, showmedians=True)
    for pc in parts['bodies']:
        pc.set_facecolor('#4ECDC4')
        pc.set_alpha(0.7)

    ax4.axhline(y=0, color='red', linestyle='--', linewidth=1.5)
    ax4.set_xticks([1, 2])
    ax4.set_xticklabels(['åŸºç¡€LSTM', 'LSTM+Attn'], fontsize=9)
    ax4.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=9, fontweight='bold')
    ax4.set_title('è¯¯å·®åˆ†å¸ƒå¯¹æ¯”', fontsize=11, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')

    # 5. å…³é”®æŒ‡æ ‡å¯¹æ¯”ï¼ˆä¸­å³ï¼‰
    ax5 = fig.add_subplot(gs[1, 2])
    metrics_comparison = pd.DataFrame({
        'æ¨¡å‹': ['åŸºç¡€LSTM', 'LSTM+Attn', 'åŠ æƒèåˆ'],
        'RÂ²': [
            r2_score(y_test_seq, lstm_test_pred),
            r2_score(y_test_seq, lstm_attn_test_pred),
            r2_score(y_test_seq, weighted_pred)
        ],
        'MAE': [
            mean_absolute_error(y_test_original, y_scaler.inverse_transform(lstm_test_pred.reshape(-1, 1)).flatten()),
            mean_absolute_error(y_test_original,
                                y_scaler.inverse_transform(lstm_attn_test_pred.reshape(-1, 1)).flatten()),
            mean_absolute_error(y_test_original, best_pred_orig)
        ]
    })

    # æ ¼å¼åŒ–æ•°å€¼
    metrics_values = []
    for idx, row in metrics_comparison.iterrows():
        metrics_values.append([
            row['æ¨¡å‹'],
            f"{row['RÂ²']:.4f}",
            f"{row['MAE']:.2f}"
        ])

    ax5.axis('tight')
    ax5.axis('off')
    table = ax5.table(cellText=metrics_values,
                      colLabels=['æ¨¡å‹', 'RÂ²', 'MAE'],
                      cellLoc='center',
                      loc='center',
                      bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)

    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(3):
        table[(0, i)].set_facecolor('#4ECDC4')
        table[(0, i)].set_text_props(weight='bold', color='white')

    ax5.set_title('å…³é”®æŒ‡æ ‡å¯¹æ¯”è¡¨', fontsize=11, fontweight='bold', pad=20)

    # 6. æ®‹å·®æ—¶åºå›¾ï¼ˆä¸‹æ–¹ï¼Œè·¨3åˆ—ï¼‰
    ax6 = fig.add_subplot(gs[2, :])
    residual_lstm = y_test_seq - lstm_test_pred
    residual_attn = y_test_seq - lstm_attn_test_pred
    residual_fusion = y_test_seq - weighted_pred

    ax6.plot(residual_lstm, label='åŸºç¡€LSTM', linewidth=1.5, alpha=0.7, color='#FF6B6B')
    ax6.plot(residual_attn, label='LSTM+Attention', linewidth=1.5, alpha=0.7, color='#4ECDC4')
    ax6.plot(residual_fusion, label='åŠ æƒèåˆ', linewidth=2, alpha=0.8, color='#F38181')
    ax6.axhline(y=0, color='black', linestyle='--', linewidth=1.5)
    ax6.fill_between(range(len(residual_fusion)), residual_fusion, alpha=0.3, color='#F38181')

    ax6.set_xlabel('æ ·æœ¬åºå·', fontsize=10, fontweight='bold')
    ax6.set_ylabel('æ®‹å·®', fontsize=10, fontweight='bold')
    ax6.set_title('æ®‹å·®æ—¶åºå¯¹æ¯”ï¼ˆè¶Šæ¥è¿‘0è¶Šå¥½ï¼‰', fontsize=12, fontweight='bold')
    ax6.legend(fontsize=9, loc='best')
    ax6.grid(True, alpha=0.3)

    plt.suptitle('ğŸ¯ LSTM+Attentionæ—¶é—´åºåˆ—é¢„æµ‹ - ç»¼åˆæŠ¥å‘Š',
                 fontsize=16, fontweight='bold', y=0.995)

    plt.savefig(results_directory + 'comprehensive_report.png', dpi=300, bbox_inches='tight')
    print("âœ“ ä¿å­˜: comprehensive_report.png")
    plt.close()

except Exception as e:
    print(f"âš  ç»¼åˆæŠ¥å‘Šå›¾ç”Ÿæˆå¤±è´¥: {e}")
    print("  è·³è¿‡è¯¥å›¾è¡¨ï¼Œç»§ç»­æ‰§è¡Œ...")

print("\n" + "=" * 100)
print("âœ… æ‰€æœ‰é¢å¤–å¯è§†åŒ–ç”Ÿæˆå®Œæˆï¼".center(100))
print("=" * 100)

print("\nğŸ“Š æ–°å¢å¯è§†åŒ–å›¾è¡¨:")
print("  8. prediction_scatter_plots.png - é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾")
print("  9. error_trend_analysis.png - æ—¶åºè¯¯å·®è¶‹åŠ¿åˆ†æ")
print(" 10. performance_heatmap.png - æ€§èƒ½ç»¼åˆå¯¹æ¯”çƒ­åŠ›å›¾")
print(" 11. error_boxplot_comparison.png - è¯¯å·®åˆ†å¸ƒç®±çº¿å›¾")
print(" 12. learning_curves_comparison.png - å­¦ä¹ æ›²çº¿å¯¹æ¯”")
print(" 13. comprehensive_report.png - æœ€ç»ˆç»¼åˆæŠ¥å‘Šå›¾")

print(f"\nğŸ’¾ æ€»è®¡13å¼ ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {results_directory}")
print("=" * 100)
print("=" * 100)