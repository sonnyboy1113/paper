import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from tensorflow.keras import Sequential, layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from xgboost import XGBRegressor
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("LSTM + GRU + XGBoost èåˆæ—¶é—´åºåˆ—é¢„æµ‹ - Hyperoptä¼˜åŒ–ä¿®å¤ç‰ˆ".center(100))
print("æ ¸å¿ƒæ”¹è¿›ï¼šåˆç†çš„è¶…å‚æ•°æœç´¢ç©ºé—´ + é˜²æ­¢è¿‡æ‹Ÿåˆ + æ›´å°‘è¿­ä»£æ¬¡æ•°".center(100))
print("=" * 100)

# ========== é…ç½®å‚æ•° ==========
ENABLE_HYPEROPT = True  # æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
HYPEROPT_EVALS = 15  # Hyperoptè¿­ä»£æ¬¡æ•°ï¼ˆé™ä½ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
CV_SPLITS = 5  # äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆå¢åŠ ä»¥è·å¾—æ›´ç¨³å®šçš„è¯„ä¼°ï¼‰

print(f"\né…ç½®:")
print(f"  å¯ç”¨Hyperopt: {ENABLE_HYPEROPT}")
print(f"  ä¼˜åŒ–è¿­ä»£æ¬¡æ•°: {HYPEROPT_EVALS}")
print(f"  äº¤å‰éªŒè¯æŠ˜æ•°: {CV_SPLITS}")

# ========== åŠ è½½æ•°æ® ==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print("\næ•°æ®é›†ä¿¡æ¯:")
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ ==========
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

print(f"\næ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train_raw)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test_raw)} æ ·æœ¬")

# å½’ä¸€åŒ–
feature_scalers = {}
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

y_scaler = MinMaxScaler()
y_train = y_scaler.fit_transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# ========== æ·»åŠ æ»åç‰¹å¾ ==========
def add_features(X, y):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]

print(f"\næ·»åŠ ç‰¹å¾å:")
print(f"è®­ç»ƒé›†: X_train={X_train_feat.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†: X_test={X_test_feat.shape}, y_test={y_test.shape}")


# æ„é€ åºåˆ—æ•°æ®
def create_sequences(X, y, seq_len=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X.iloc[i:i + seq_len].values)
        y_seq.append(y.iloc[i + seq_len])
    return np.array(X_seq), np.array(y_seq)


def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_seq, y_train_seq = create_sequences(X_train_feat, y_train, seq_len)
X_test_seq, y_test_seq = create_sequences(X_test_feat, y_test, seq_len)

X_train_flat, _ = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, _ = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"LSTM/GRUè¾“å…¥: train={X_train_seq.shape}, test={X_test_seq.shape}")
print(f"XGBoostè¾“å…¥: train={X_train_flat.shape}, test={X_test_flat.shape}")

# ========== é»˜è®¤ä¿å®ˆå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰==========
default_lstm_params = {
    'units': 100,
    'dropout': 0.3,
    'l2_reg': 0.01,
    'learning_rate': 0.001,
    'batch_size': 32
}

default_gru_params = {
    'units': 100,
    'dropout': 0.3,
    'l2_reg': 0.01,
    'learning_rate': 0.001,
    'batch_size': 32
}

default_xgb_params = {
    'n_estimators': 100,
    'learning_rate': 0.01,
    'max_depth': 3,
    'min_child_weight': 5,
    'subsample': 0.7,
    'colsample_bytree': 0.7,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0
}

default_ridge_params = {
    'alpha': 10.0
}

# ========== Hyperoptè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ›´ä¿å®ˆçš„æœç´¢ç©ºé—´ï¼‰==========
if ENABLE_HYPEROPT:
    print("\n" + "=" * 100)
    print("ã€Hyperoptè¶…å‚æ•°ä¼˜åŒ–ã€‘ä¿å®ˆç­–ç•¥ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ".center(100))
    print("=" * 100)

    # æ›´ä¿å®ˆçš„RNNå‚æ•°ç©ºé—´
    rnn_space = {
        'units': hp.choice('units', [80, 100, 120]),  # å‡å°‘é€‰é¡¹
        'dropout': hp.uniform('dropout', 0.2, 0.4),  # æ›´çª„èŒƒå›´
        'l2_reg': hp.loguniform('l2_reg', np.log(0.005), np.log(0.05)),  # æ›´ä¿å®ˆ
        'learning_rate': hp.loguniform('learning_rate', np.log(0.0005), np.log(0.005)),  # æ›´ä¿å®ˆ
        'batch_size': hp.choice('batch_size', [32])  # å›ºå®šbatch size
    }


    def objective_lstm(params):
        """LSTMè¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆå¢åŠ CVæŠ˜æ•°ï¼Œå‡å°‘epochsï¼‰"""
        units = params['units']
        dropout = params['dropout']
        l2_reg = params['l2_reg']
        lr = params['learning_rate']
        batch_size = params['batch_size']

        tscv = TimeSeriesSplit(n_splits=CV_SPLITS)  # å¢åŠ æŠ˜æ•°
        val_scores = []

        for train_idx, val_idx in tscv.split(X_train_seq):
            X_tr, X_val = X_train_seq[train_idx], X_train_seq[val_idx]
            y_tr, y_val = y_train_seq[train_idx], y_train_seq[val_idx]

            model = Sequential([
                layers.LSTM(
                    units=units,
                    input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
                    kernel_regularizer=l2(l2_reg),
                    recurrent_regularizer=l2(l2_reg)
                ),
                layers.Dropout(dropout),
                layers.Dense(1)
            ])

            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=lr))
            early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)

            model.fit(X_tr, y_tr, validation_data=(X_val, y_val),
                      epochs=100, batch_size=batch_size, callbacks=[early_stop], verbose=0)

            val_pred = model.predict(X_val, verbose=0).flatten()
            val_score = r2_score(y_val, val_pred)
            val_scores.append(val_score)

            del model
            tf.keras.backend.clear_session()

        # ä½¿ç”¨ä¸­ä½æ•°è€Œä¸æ˜¯å¹³å‡å€¼ï¼Œæ›´ç¨³å¥
        median_score = np.median(val_scores)
        return {'loss': -median_score, 'status': STATUS_OK}


    def objective_gru(params):
        """GRUè¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        units = params['units']
        dropout = params['dropout']
        l2_reg = params['l2_reg']
        lr = params['learning_rate']
        batch_size = params['batch_size']

        tscv = TimeSeriesSplit(n_splits=CV_SPLITS)
        val_scores = []

        for train_idx, val_idx in tscv.split(X_train_seq):
            X_tr, X_val = X_train_seq[train_idx], X_train_seq[val_idx]
            y_tr, y_val = y_train_seq[train_idx], y_train_seq[val_idx]

            model = Sequential([
                layers.GRU(
                    units=units,
                    input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
                    kernel_regularizer=l2(l2_reg),
                    recurrent_regularizer=l2(l2_reg)
                ),
                layers.Dropout(dropout),
                layers.Dense(1)
            ])

            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=lr))
            early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)

            model.fit(X_tr, y_tr, validation_data=(X_val, y_val),
                      epochs=100, batch_size=batch_size, callbacks=[early_stop], verbose=0)

            val_pred = model.predict(X_val, verbose=0).flatten()
            val_score = r2_score(y_val, val_pred)
            val_scores.append(val_score)

            del model
            tf.keras.backend.clear_session()

        median_score = np.median(val_scores)
        return {'loss': -median_score, 'status': STATUS_OK}


    print("\nã€1/2ã€‘ä¼˜åŒ–LSTMè¶…å‚æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰...")
    lstm_trials = Trials()
    best_lstm = fmin(fn=objective_lstm, space=rnn_space, algo=tpe.suggest,
                     max_evals=HYPEROPT_EVALS, trials=lstm_trials, verbose=0)
    best_lstm_params = space_eval(rnn_space, best_lstm)
    print(f"âœ“ æœ€ä½³LSTMå‚æ•°: {best_lstm_params}")
    print(f"  æœ€ä½³éªŒè¯RÂ²: {-lstm_trials.best_trial['result']['loss']:.4f}")

    print("\nã€2/2ã€‘ä¼˜åŒ–GRUè¶…å‚æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰...")
    gru_trials = Trials()
    best_gru = fmin(fn=objective_gru, space=rnn_space, algo=tpe.suggest,
                    max_evals=HYPEROPT_EVALS, trials=gru_trials, verbose=0)
    best_gru_params = space_eval(rnn_space, best_gru)
    print(f"âœ“ æœ€ä½³GRUå‚æ•°: {best_gru_params}")
    print(f"  æœ€ä½³éªŒè¯RÂ²: {-gru_trials.best_trial['result']['loss']:.4f}")

    # å¯¹æ¯”é»˜è®¤å‚æ•°
    print(f"\nã€å‚æ•°å¯¹æ¯”ã€‘")
    print(f"LSTMé»˜è®¤ vs ä¼˜åŒ–:")
    for key in default_lstm_params:
        print(f"  {key}: {default_lstm_params[key]} â†’ {best_lstm_params[key]}")

    print(f"\nGRUé»˜è®¤ vs ä¼˜åŒ–:")
    for key in default_gru_params:
        print(f"  {key}: {default_gru_params[key]} â†’ {best_gru_params[key]}")

else:
    print("\nâš ï¸  è¶…å‚æ•°ä¼˜åŒ–å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤ä¿å®ˆå‚æ•°")
    best_lstm_params = default_lstm_params
    best_gru_params = default_gru_params
    best_xgb_params = default_xgb_params
    best_ridge_params = default_ridge_params

# ========== ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒRNNæ¨¡å‹ ==========
print("\n" + "=" * 100)
print("ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒLSTMå’ŒGRUæ¨¡å‹".center(100))
print("=" * 100)

print("\nè®­ç»ƒLSTMæ¨¡å‹...")
lstm_final = Sequential([
    layers.LSTM(
        units=best_lstm_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(best_lstm_params['l2_reg']),
        recurrent_regularizer=l2(best_lstm_params['l2_reg'])
    ),
    layers.Dropout(best_lstm_params['dropout']),
    layers.Dense(1)
])
lstm_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_lstm_params['learning_rate']))
lstm_history = lstm_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=best_lstm_params['batch_size'],
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
    ],
    verbose=0
)
print("âœ“ LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")

print("\nè®­ç»ƒGRUæ¨¡å‹...")
gru_final = Sequential([
    layers.GRU(
        units=best_gru_params['units'],
        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),
        kernel_regularizer=l2(best_gru_params['l2_reg']),
        recurrent_regularizer=l2(best_gru_params['l2_reg'])
    ),
    layers.Dropout(best_gru_params['dropout']),
    layers.Dense(1)
])
gru_final.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=best_gru_params['learning_rate']))
gru_history = gru_final.fit(
    X_train_seq, y_train_seq,
    validation_split=0.2,
    epochs=200,
    batch_size=best_gru_params['batch_size'],
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
    ],
    verbose=0
)
print("âœ“ GRUæ¨¡å‹è®­ç»ƒå®Œæˆ")

# è·å–é¢„æµ‹
lstm_test_pred = lstm_final.predict(X_test_seq, verbose=0).flatten()
gru_test_pred = gru_final.predict(X_test_seq, verbose=0).flatten()

lstm_train_pred = lstm_final.predict(X_train_seq, verbose=0).flatten()
gru_train_pred = gru_final.predict(X_train_seq, verbose=0).flatten()

# è¿‡æ‹Ÿåˆè¯Šæ–­
lstm_train_r2 = r2_score(y_train_seq, lstm_train_pred)
lstm_test_r2 = r2_score(y_test_seq, lstm_test_pred)
gru_train_r2 = r2_score(y_train_seq, gru_train_pred)
gru_test_r2 = r2_score(y_test_seq, gru_test_pred)

print(f"\nã€è¿‡æ‹Ÿåˆè¯Šæ–­ã€‘")
print(f"LSTM - è®­ç»ƒRÂ²: {lstm_train_r2:.4f}, æµ‹è¯•RÂ²: {lstm_test_r2:.4f}, å·®è·: {lstm_train_r2 - lstm_test_r2:.4f}")
print(f"GRU  - è®­ç»ƒRÂ²: {gru_train_r2:.4f}, æµ‹è¯•RÂ²: {gru_test_r2:.4f}, å·®è·: {gru_train_r2 - gru_test_r2:.4f}")

overfitting_detected = max(lstm_train_r2 - lstm_test_r2, gru_train_r2 - gru_test_r2) > 0.15
if overfitting_detected:
    print(f"âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œå°†é‡‡ç”¨ä¿å®ˆæ®‹å·®å­¦ä¹ ç­–ç•¥ï¼")
else:
    print(f"âœ“ è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½ï¼Œå¯å°è¯•å¤šç§æ®‹å·®ç­–ç•¥")

# ========== ç”ŸæˆOOFé¢„æµ‹ï¼ˆä½¿ç”¨é»˜è®¤ä¿å®ˆå‚æ•°ï¼‰==========
print("\n" + "=" * 100)
print("ç¬¬äºŒæ­¥ï¼šç”ŸæˆLSTMå’ŒGRUçš„OOFé¢„æµ‹ï¼ˆä½¿ç”¨ä¿å®ˆå‚æ•°é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰".center(100))
print("=" * 100)


def build_lstm_conservative(input_shape):
    """ä¿å®ˆçš„LSTMæ¨¡å‹ï¼ˆç”¨äºOOFï¼‰"""
    return Sequential([
        layers.LSTM(
            units=100,
            input_shape=input_shape,
            kernel_regularizer=l2(0.01),
            recurrent_regularizer=l2(0.01)
        ),
        layers.Dropout(0.3),
        layers.Dense(1)
    ])


def build_gru_conservative(input_shape):
    """ä¿å®ˆçš„GRUæ¨¡å‹ï¼ˆç”¨äºOOFï¼‰"""
    return Sequential([
        layers.GRU(
            units=100,
            input_shape=input_shape,
            kernel_regularizer=l2(0.01),
            recurrent_regularizer=l2(0.01)
        ),
        layers.Dropout(0.3),
        layers.Dense(1)
    ])


def get_oof_predictions(X_seq, y_seq, model_type='lstm', n_splits=5):
    """ä½¿ç”¨ä¿å®ˆå‚æ•°ç”ŸæˆOOFé¢„æµ‹"""
    print(f"\nç”Ÿæˆ{model_type.upper()} OOFé¢„æµ‹ï¼ˆä¿å®ˆå‚æ•°ï¼ŒTimeSeriesSplit with {n_splits} splitsï¼‰...")

    tscv = TimeSeriesSplit(n_splits=n_splits)
    oof_preds = np.zeros(len(y_seq))

    fold = 1
    for train_idx, val_idx in tscv.split(X_seq):
        print(f"  Fold {fold}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}")

        X_fold_train, X_fold_val = X_seq[train_idx], X_seq[val_idx]
        y_fold_train, y_fold_val = y_seq[train_idx], y_seq[val_idx]

        if model_type == 'lstm':
            model = build_lstm_conservative((X_seq.shape[1], X_seq.shape[2]))
        else:
            model = build_gru_conservative((X_seq.shape[1], X_seq.shape[2]))

        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0)
        model.fit(
            X_fold_train, y_fold_train,
            validation_data=(X_fold_val, y_fold_val),
            epochs=200,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )

        val_pred = model.predict(X_fold_val, verbose=0)
        oof_preds[val_idx] = val_pred.flatten()

        del model
        tf.keras.backend.clear_session()
        fold += 1

    return oof_preds


lstm_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'lstm', n_splits=5)
gru_oof_preds = get_oof_predictions(X_train_seq, y_train_seq, 'gru', n_splits=5)

print(f"\nOOFé¢„æµ‹ç”Ÿæˆå®Œæˆï¼")
print(f"LSTM OOF RÂ²: {r2_score(y_train_seq, lstm_oof_preds):.4f}")
print(f"GRU OOF RÂ²: {r2_score(y_train_seq, gru_oof_preds):.4f}")


# ========== ç‰¹å¾å·¥ç¨‹å‡½æ•° ==========
def create_simplified_features(X_flat, lstm_preds, gru_preds):
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


# ========== å‡†å¤‡ç‰¹å¾å’Œæ®‹å·® ==========
basic_features_train = X_train_flat[:len(gru_oof_preds)]
basic_features_test = X_test_flat

simplified_train = create_simplified_features(basic_features_train, lstm_oof_preds, gru_oof_preds)
simplified_test = create_simplified_features(basic_features_test, lstm_test_pred, gru_test_pred)

lstm_oof_residual = y_train_seq - lstm_oof_preds
gru_oof_residual = y_train_seq - gru_oof_preds
avg_oof_preds = (lstm_oof_preds + gru_oof_preds) / 2
avg_oof_residual = y_train_seq - avg_oof_preds


# ========== æ®‹å·®å­¦ä¹ è®­ç»ƒå‡½æ•°ï¼ˆä½¿ç”¨é»˜è®¤ä¿å®ˆå‚æ•°ï¼‰==========
def train_conservative_xgboost(X_train, y_train):
    """ä¿å®ˆXGBoostå‚æ•°"""
    model = XGBRegressor(
        n_estimators=100,
        learning_rate=0.01,
        max_depth=3,
        min_child_weight=5,
        subsample=0.7,
        colsample_bytree=0.7,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)
    return model


def train_ridge_model(X_train, y_train, alpha=10.0):
    """Ridgeçº¿æ€§å›å½’"""
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    return model


def clip_residual(residual_pred, threshold=2.0):
    std = np.std(residual_pred)
    mean = np.mean(residual_pred)
    return np.clip(residual_pred, mean - threshold * std, mean + threshold * std)


def weighted_residual_correction(base_pred, residual_pred, weight=0.5):
    return base_pred + weight * residual_pred


# ========== åŸºå‡†ï¼šç®€å•å¹³å‡ ==========
print("\n" + "=" * 100)
print("ã€åŸºå‡†ã€‘ç®€å•å¹³å‡èåˆ".center(100))
print("=" * 100)

avg_test_pred = (lstm_test_pred + gru_test_pred) / 2
avg_r2 = r2_score(y_test_seq, avg_test_pred)
print(f"ç®€å•å¹³å‡æµ‹è¯•é›†RÂ²: {avg_r2:.4f}")

print(f"\næ®‹å·®ç»Ÿè®¡:")
print(f"LSTMæ®‹å·® - å‡å€¼: {np.mean(lstm_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(lstm_oof_residual):.6f}")
print(f"GRUæ®‹å·®  - å‡å€¼: {np.mean(gru_oof_residual):.6f}, æ ‡å‡†å·®: {np.std(gru_oof_residual):.6f}")

# ========== ç­–ç•¥é›†åˆï¼ˆä½¿ç”¨ä¿å®ˆå‚æ•°ï¼‰==========
strategies_results = {}

print(f"\nç‰¹å¾ç»´åº¦: {simplified_train.shape[1]} ç»´")

# ========== ç­–ç•¥1ï¼šLSTMæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥1ã€‘LSTMæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_lstm_conservative = train_conservative_xgboost(simplified_train, lstm_oof_residual)
lstm_residual = xgb_lstm_conservative.predict(simplified_test)
pred_lstm_residual = lstm_test_pred + lstm_residual

r2_lstm_residual = r2_score(y_test_seq, pred_lstm_residual)
print(f"âœ“ RÂ²: {r2_lstm_residual:.4f} (vsç®€å•å¹³å‡: {r2_lstm_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥1-LSTMæ®‹å·®å­¦ä¹ '] = pred_lstm_residual

# ========== ç­–ç•¥2ï¼šGRUæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥2ã€‘GRUæ®‹å·®å­¦ä¹ ï¼šç®€åŒ–ç‰¹å¾ + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_gru_conservative = train_conservative_xgboost(simplified_train, gru_oof_residual)
gru_residual = xgb_gru_conservative.predict(simplified_test)
pred_gru_residual = gru_test_pred + gru_residual

r2_gru_residual = r2_score(y_test_seq, pred_gru_residual)
print(f"âœ“ RÂ²: {r2_gru_residual:.4f} (vsç®€å•å¹³å‡: {r2_gru_residual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥2-GRUæ®‹å·®å­¦ä¹ '] = pred_gru_residual

# ========== ç­–ç•¥3ï¼šåŒæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥3ã€‘åŒæ®‹å·®å­¦ä¹ ï¼š(LSTM+GRU)/2 + ä¿å®ˆXGBoost".center(100))
print("=" * 100)

xgb_dual = train_conservative_xgboost(simplified_train, avg_oof_residual)
dual_residual = xgb_dual.predict(simplified_test)
pred_dual = avg_test_pred + dual_residual

r2_dual = r2_score(y_test_seq, pred_dual)
print(f"âœ“ RÂ²: {r2_dual:.4f} (vsç®€å•å¹³å‡: {r2_dual - avg_r2:+.4f})")
strategies_results['ç­–ç•¥3-åŒæ®‹å·®å­¦ä¹ '] = pred_dual

# ========== ç­–ç•¥4ï¼šæ®‹å·®å‰ªè£ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥4ã€‘GRUæ®‹å·®å­¦ä¹  + æ®‹å·®å‰ªè£".center(100))
print("=" * 100)

gru_residual_clipped = clip_residual(gru_residual, threshold=2.0)
pred_gru_clipped = gru_test_pred + gru_residual_clipped

r2_gru_clipped = r2_score(y_test_seq, pred_gru_clipped)
print(f"âœ“ RÂ²: {r2_gru_clipped:.4f} (vsç®€å•å¹³å‡: {r2_gru_clipped - avg_r2:+.4f})")
strategies_results['ç­–ç•¥4-æ®‹å·®å‰ªè£'] = pred_gru_clipped

# ========== ç­–ç•¥5ï¼šåŠ æƒèåˆï¼ˆ30%ï¼‰==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥5ã€‘GRUæ®‹å·®å­¦ä¹  + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_gru_weighted = weighted_residual_correction(gru_test_pred, gru_residual, weight=0.3)

r2_gru_weighted = r2_score(y_test_seq, pred_gru_weighted)
print(f"âœ“ RÂ²: {r2_gru_weighted:.4f} (vsç®€å•å¹³å‡: {r2_gru_weighted - avg_r2:+.4f})")
strategies_results['ç­–ç•¥5-åŠ æƒèåˆ30%'] = pred_gru_weighted

# ========== ç­–ç•¥6ï¼šç»ˆæç»„åˆ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥6ã€‘ç»ˆæç»„åˆï¼šæ®‹å·®å‰ªè£ + åŠ æƒèåˆ(30%)".center(100))
print("=" * 100)

pred_ultimate = weighted_residual_correction(gru_test_pred, gru_residual_clipped, weight=0.3)

r2_ultimate = r2_score(y_test_seq, pred_ultimate)
print(f"âœ“ RÂ²: {r2_ultimate:.4f} (vsç®€å•å¹³å‡: {r2_ultimate - avg_r2:+.4f})")
strategies_results['ç­–ç•¥6-ç»ˆæç»„åˆ'] = pred_ultimate

# ========== ç­–ç•¥7ï¼šåŠ¨æ€æƒé‡èåˆ ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥7ã€‘åŠ¨æ€æƒé‡èåˆï¼šRidgeå­¦ä¹ LSTMå’ŒGRUæƒé‡".center(100))
print("=" * 100)

lstm_res_train = xgb_lstm_conservative.predict(simplified_train)
gru_res_train = xgb_gru_conservative.predict(simplified_train)

lstm_res_test = xgb_lstm_conservative.predict(simplified_test)
gru_res_test = xgb_gru_conservative.predict(simplified_test)

meta_features_train = np.column_stack([
    lstm_oof_preds + lstm_res_train,
    gru_oof_preds + gru_res_train
])
meta_features_test = np.column_stack([
    lstm_test_pred + lstm_res_test,
    gru_test_pred + gru_res_test
])

meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_features_train, y_train_seq)
pred_dynamic = meta_model.predict(meta_features_test)

lstm_weight = meta_model.coef_[0]
gru_weight = meta_model.coef_[1]

r2_dynamic = r2_score(y_test_seq, pred_dynamic)
print(f"å­¦ä¹ åˆ°çš„æƒé‡: LSTM={lstm_weight:.4f}, GRU={gru_weight:.4f}")
print(f"âœ“ RÂ²: {r2_dynamic:.4f} (vsç®€å•å¹³å‡: {r2_dynamic - avg_r2:+.4f})")
strategies_results['ç­–ç•¥7-åŠ¨æ€æƒé‡èåˆ'] = pred_dynamic

# ========== ç­–ç•¥8ï¼šRidgeæ®‹å·®å­¦ä¹  ==========
print("\n" + "=" * 100)
print("ã€ç­–ç•¥8ã€‘Ridgeæ®‹å·®å­¦ä¹ ï¼šæœ€ä¿å®ˆçš„çº¿æ€§æ¨¡å‹".center(100))
print("=" * 100)

ridge_model = train_ridge_model(simplified_train, gru_oof_residual, alpha=10.0)
ridge_residual = ridge_model.predict(simplified_test)
pred_ridge = gru_test_pred + ridge_residual

r2_ridge = r2_score(y_test_seq, pred_ridge)
print(f"âœ“ RÂ²: {r2_ridge:.4f} (vsç®€å•å¹³å‡: {r2_ridge - avg_r2:+.4f})")
strategies_results['ç­–ç•¥8-Ridgeæ®‹å·®'] = pred_ridge

# ========== ç»¼åˆå¯¹æ¯” ==========
print("\n" + "=" * 100)
print("æ‰€æœ‰ç­–ç•¥æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰".center(100))
print("=" * 100)

all_strategies = {
    'LSTMå•æ¨¡å‹': lstm_test_pred,
    'GRUå•æ¨¡å‹': gru_test_pred,
    'ç®€å•å¹³å‡(åŸºçº¿)': avg_test_pred,
    **strategies_results
}

print(f"\n{'ç­–ç•¥':<30} {'RÂ²':>10} {'vsåŸºçº¿':>10} {'MAE':>12} {'RMSE':>12}")
print("-" * 75)

results_list = []
for name, pred in all_strategies.items():
    r2 = r2_score(y_test_seq, pred)
    mae = mean_absolute_error(y_test_seq, pred)
    rmse = sqrt(mean_squared_error(y_test_seq, pred))
    improvement = r2 - avg_r2
    results_list.append((name, r2, improvement, mae, rmse, pred))
    print(f"{name:<30} {r2:>10.4f} {improvement:>10.4f} {mae:>12.6f} {rmse:>12.6f}")

results_list.sort(key=lambda x: x[1], reverse=True)

print("\n" + "=" * 100)
print("æ€§èƒ½æ’åï¼ˆæŒ‰RÂ²é™åºï¼‰".center(100))
print("=" * 100)

best_r2 = results_list[0][1]
best_name = results_list[0][0]

for rank, (name, r2, improvement, mae, rmse, pred) in enumerate(results_list, 1):
    marker = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
    print(f"{marker} {rank:>2}. {name:<30} RÂ²={r2:.4f} (vsåŸºçº¿: {improvement:+.4f})")

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_name} (RÂ² = {best_r2:.4f})")

# ========== åŸå§‹å°ºåº¦è¯„ä¼° ==========
print("\n" + "=" * 100)
print("åŸå§‹å°ºåº¦æ€§èƒ½å¯¹æ¯”".center(100))
print("=" * 100)

y_test_original = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1))
strategies_original = {}

print(f"\n{'ç­–ç•¥':<30} {'RÂ²':>10} {'MAE':>12} {'RMSE':>12} {'MAPE':>12}")
print("-" * 75)

for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    strategies_original[name] = pred_original

    r2 = r2_score(y_test_original, pred_original)
    mae = mean_absolute_error(y_test_original, pred_original)
    rmse = sqrt(mean_squared_error(y_test_original, pred_original))
    mape = np.mean(np.abs((pred_original - y_test_original) / (y_test_original + 1e-8)))

    print(f"{name:<30} {r2:>10.4f} {mae:>12.2f} {rmse:>12.2f} {mape:>12.6f}")

# ========== ä¿å­˜ç»“æœ ==========
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

import pickle
import json

# ä¿å­˜æ¨¡å‹
lstm_final.save(results_directory + 'lstm_final_fixed.h5')
gru_final.save(results_directory + 'gru_final_fixed.h5')

with open(results_directory + 'xgb_gru_conservative_fixed.pkl', 'wb') as f:
    pickle.dump(xgb_gru_conservative, f)

with open(results_directory + 'ridge_meta_model_fixed.pkl', 'wb') as f:
    pickle.dump(meta_model, f)

with open(results_directory + 'scalers.pkl', 'wb') as f:
    pickle.dump({'feature_scalers': feature_scalers, 'y_scaler': y_scaler}, f)

# ä¿å­˜å‚æ•°
if ENABLE_HYPEROPT:
    def convert_to_serializable(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj


    hyperparams = {
        'lstm': {k: convert_to_serializable(v) for k, v in best_lstm_params.items()},
        'gru': {k: convert_to_serializable(v) for k, v in best_gru_params.items()},
        'hyperopt_enabled': True,
        'hyperopt_evals': HYPEROPT_EVALS,
        'cv_splits': CV_SPLITS
    }

    with open(results_directory + 'hyperparameters_fixed.json', 'w', encoding='utf-8') as f:
        json.dump(hyperparams, f, indent=4, ensure_ascii=False)

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_dict = {'true_value': y_test_original.flatten()}
for name, pred in all_strategies.items():
    pred_original = y_scaler.inverse_transform(pred.reshape(-1, 1))
    predictions_dict[name.replace('/', '_').replace('(', '').replace(')', '')] = pred_original.flatten()

results_df = pd.DataFrame(predictions_dict)
results_df.to_csv(results_directory + 'all_predictions_fixed.csv', index=False)

# ========== æœ€ç»ˆæ€»ç»“ ==========
print("\n" + "=" * 100)
print("ğŸ‰ ä¿®å¤ç‰ˆHyperoptä¼˜åŒ–æ¨¡å‹è®­ç»ƒå®Œæˆï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š æ ¸å¿ƒä¿®å¤:")
print(f"  âœ“ ä½¿ç”¨æ›´ä¿å®ˆçš„è¶…å‚æ•°æœç´¢ç©ºé—´")
print(f"  âœ“ å¢åŠ äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆ3â†’5ï¼‰")
print(f"  âœ“ å‡å°‘ä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼ˆ20â†’15ï¼‰")
print(f"  âœ“ OOFé¢„æµ‹ä½¿ç”¨å›ºå®šä¿å®ˆå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰")
print(f"  âœ“ ä½¿ç”¨ä¸­ä½æ•°ä»£æ›¿å¹³å‡å€¼ï¼ˆæ›´ç¨³å¥ï¼‰")
print(f"  âœ“ æ®‹å·®å­¦ä¹ ä½¿ç”¨é»˜è®¤ä¿å®ˆå‚æ•°")

if ENABLE_HYPEROPT:
    print(f"\nğŸ”§ ä¼˜åŒ–åçš„å‚æ•°:")
    print(f"  LSTM: units={best_lstm_params['units']}, dropout={best_lstm_params['dropout']:.3f}, "
          f"l2={best_lstm_params['l2_reg']:.4f}")
    print(f"  GRU: units={best_gru_params['units']}, dropout={best_gru_params['dropout']:.3f}, "
          f"l2={best_gru_params['l2_reg']:.4f}")
else:
    print(f"\nâš ï¸  ä½¿ç”¨é»˜è®¤ä¿å®ˆå‚æ•°ï¼ˆæœªå¯ç”¨Hyperoptï¼‰")

print(f"\nğŸ“ˆ å®éªŒç»“æœ:")
print(f"  LSTMå•æ¨¡å‹: RÂ² = {lstm_test_r2:.4f}")
print(f"  GRUå•æ¨¡å‹:  RÂ² = {gru_test_r2:.4f}")
print(f"  åŸºçº¿ï¼ˆç®€å•å¹³å‡ï¼‰: RÂ² = {avg_r2:.4f}")
print(f"  æœ€ä½³ç­–ç•¥: {best_name}")
print(f"  æœ€ä½³æ€§èƒ½: RÂ² = {best_r2:.4f}")
print(f"  æ€§èƒ½æå‡: {best_r2 - avg_r2:+.4f} ({(best_r2 - avg_r2) / avg_r2 * 100:+.2f}%)")

print(f"\nğŸ† Top5ç­–ç•¥æ’å:")
for rank, (name, r2, improvement, _, _, _) in enumerate(results_list[:5], 1):
    print(f"  {rank}. {name:<30} RÂ²={r2:.4f} (æ”¹è¿›: {improvement:+.4f})")

print(f"\nğŸ’¡ å…³é”®æ”¹è¿›è¯´æ˜:")
print(f"  1. æ›´ä¿å®ˆçš„è¶…å‚æ•°ç©ºé—´é˜²æ­¢æ¿€è¿›ä¼˜åŒ–")
print(f"  2. OOFä½¿ç”¨å›ºå®šå‚æ•°é¿å…è¿‡æ‹Ÿåˆä¼ æ’­")
print(f"  3. æ®‹å·®å­¦ä¹ ä½¿ç”¨é»˜è®¤å‚æ•°ä¿è¯ç¨³å®šæ€§")
print(f"  4. å¯é€šè¿‡ENABLE_HYPEROPT=Falseå®Œå…¨ç¦ç”¨ä¼˜åŒ–")

print(f"\nğŸ” è¿‡æ‹Ÿåˆåˆ†æ:")
if overfitting_detected:
    print(f"  âš ï¸  åŸºç¡€æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆï¼ˆLSTMå·®è·={lstm_train_r2 - lstm_test_r2:.4f}, GRUå·®è·={gru_train_r2 - gru_test_r2:.4f}ï¼‰")
    print(f"  ğŸ’¡ å·²é‡‡ç”¨å¤šé‡ç­–ç•¥ç¼“è§£")
else:
    print(f"  âœ… è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½")
    print(f"  âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¾ƒå¼º")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_directory}")
print("=" * 100)