import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import LinearRegression
from tensorflow.keras import Sequential, layers, Model
from tensorflow.keras.callbacks import EarlyStopping
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# åŠ è½½æ•°æ®é›†
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])
print(dataset.info())

# ========== æ•°æ®å‡†å¤‡ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›† ==========

# ç‰¹å¾æ•°æ®é›†å’Œæ ‡ç­¾æ•°æ®é›†
X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

# ç¬¬ä¸€æ­¥ï¼šæ•°æ®é›†åˆ†ç¦»ï¼ˆè®­ç»ƒé›†60%ï¼ŒéªŒè¯é›†20%ï¼Œæµ‹è¯•é›†20%ï¼‰
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=666)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, shuffle=False, random_state=666)

print(f"\nåŸå§‹æ•°æ®åˆ†å‰²:")
print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

# ç¬¬äºŒæ­¥ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šfitå½’ä¸€åŒ–å™¨
feature_scalers = {}
for col in X_train.columns:
    scaler = MinMaxScaler()
    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))
    X_val[col] = scaler.transform(X_val[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))
    feature_scalers[col] = scaler

# å¯¹æ ‡ç­¾è¿›è¡Œå½’ä¸€åŒ–
y_scaler = MinMaxScaler()
y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1)).flatten()
y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

# è½¬æ¢å›Serieså¹¶ä¿æŒç´¢å¼•
y_train = pd.Series(y_train_scaled, index=y_train.index)
y_val = pd.Series(y_val_scaled, index=y_val.index)
y_test = pd.Series(y_test_scaled, index=y_test.index)

# ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ æ»åç‰¹å¾
for i in range(1, 6):
    X_train[f'Corn_lag_{i}'] = y_train.shift(i)
    X_val[f'Corn_lag_{i}'] = y_val.shift(i)
    X_test[f'Corn_lag_{i}'] = y_test.shift(i)

# åˆ é™¤å› æ»åç‰¹å¾äº§ç”Ÿçš„ç¼ºå¤±å€¼
X_train = X_train.dropna()
y_train = y_train.loc[X_train.index]

X_val = X_val.dropna()
y_val = y_val.loc[X_val.index]

X_test = X_test.dropna()
y_test = y_test.loc[X_test.index]

print(f"\næ·»åŠ æ»åç‰¹å¾å:")
print(f"è®­ç»ƒé›†å½¢çŠ¶: X_train={X_train.shape}, y_train={y_train.shape}")
print(f"éªŒè¯é›†å½¢çŠ¶: X_val={X_val.shape}, y_val={y_val.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: X_test={X_test.shape}, y_test={y_test.shape}")


# æ„é€ ç‰¹å¾æ•°æ®é›†
def create_dataset(X, y, seq_len=5):
    features = []
    targets = []
    for i in range(0, len(X) - seq_len, 1):
        data = X.iloc[i:i + seq_len]
        label = y.iloc[i + seq_len]
        features.append(data)
        targets.append(label)
    return np.array(features), np.array(targets)


train_dataset, train_labels = create_dataset(X_train, y_train, seq_len=5)
val_dataset, val_labels = create_dataset(X_val, y_val, seq_len=5)
test_dataset, test_labels = create_dataset(X_test, y_test, seq_len=5)

print(f"\nåºåˆ—æ•°æ®å½¢çŠ¶:")
print(f"train_dataset={train_dataset.shape}")
print(f"val_dataset={val_dataset.shape}")
print(f"test_dataset={test_dataset.shape}")


# æ„é€ æ‰¹æ•°æ®
def create_batch_dataset(X, y, train=True, buffer_size=200, batch_size=32):
    batch_data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))
    if train:
        return batch_data.cache().shuffle(buffer_size).batch(batch_size)
    else:
        return batch_data.batch(batch_size)


train_batch_dataset = create_batch_dataset(train_dataset, train_labels)
val_batch_dataset = create_batch_dataset(val_dataset, val_labels, train=False)
test_batch_dataset = create_batch_dataset(test_dataset, test_labels, train=False)

# æ—©åœå›è°ƒ
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)

# ======================= æ„å»ºè¿”å›éšè—çŠ¶æ€çš„LSTMæ¨¡å‹ =======================
print("\nå¼€å§‹è®­ç»ƒLSTMæ¨¡å‹ï¼ˆè¿”å›éšè—çŠ¶æ€ï¼‰...")
lstm_input = layers.Input(shape=(5, 14))
lstm_layer, lstm_hidden_state, lstm_cell_state = layers.LSTM(
    units=100,
    return_sequences=False,
    return_state=True
)(lstm_input)
lstm_output = layers.Dense(1)(lstm_layer)

lstm_model = Model(inputs=lstm_input, outputs=[lstm_output, lstm_hidden_state])
lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')


# è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼ˆåªç”¨é¢„æµ‹è¾“å‡ºè®¡ç®—æŸå¤±ï¼‰
class CustomLSTMCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}, Loss: {logs['loss']:.6f}, Val Loss: {logs['val_loss']:.6f}")


# é‡æ–°å®šä¹‰æ¨¡å‹åªè¾“å‡ºé¢„æµ‹å€¼ç”¨äºè®­ç»ƒ
lstm_pred_model = Model(inputs=lstm_input, outputs=lstm_output)
lstm_pred_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')

lstm_history = lstm_pred_model.fit(
    train_batch_dataset,
    epochs=200,
    validation_data=val_batch_dataset,
    callbacks=[early_stop, CustomLSTMCallback()],
    verbose=0
)

# æ›´æ–°å®Œæ•´æ¨¡å‹çš„æƒé‡
lstm_model.set_weights(lstm_pred_model.get_weights())

# ======================= æ„å»ºè¿”å›éšè—çŠ¶æ€çš„GRUæ¨¡å‹ =======================
print("\nå¼€å§‹è®­ç»ƒGRUæ¨¡å‹ï¼ˆè¿”å›éšè—çŠ¶æ€ï¼‰...")
gru_input = layers.Input(shape=(5, 14))
gru_layer, gru_hidden_state = layers.GRU(
    units=100,
    return_sequences=False,
    return_state=True
)(gru_input)
gru_output = layers.Dense(1)(gru_layer)

gru_model = Model(inputs=gru_input, outputs=[gru_output, gru_hidden_state])
gru_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))

# é‡æ–°å®šä¹‰æ¨¡å‹åªè¾“å‡ºé¢„æµ‹å€¼ç”¨äºè®­ç»ƒ
gru_pred_model = Model(inputs=gru_input, outputs=gru_output)
gru_pred_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')

gru_history = gru_pred_model.fit(
    train_batch_dataset,
    epochs=200,
    validation_data=val_batch_dataset,
    callbacks=[early_stop, CustomLSTMCallback()],
    verbose=0
)

# æ›´æ–°å®Œæ•´æ¨¡å‹çš„æƒé‡
gru_model.set_weights(gru_pred_model.get_weights())

# ======================= æå–éšè—çŠ¶æ€ç‰¹å¾ =======================
print("\næå–LSTMå’ŒGRUçš„éšè—çŠ¶æ€ç‰¹å¾...")

# è®­ç»ƒé›†ç‰¹å¾æå–
lstm_train_pred, lstm_train_hidden = lstm_model.predict(train_dataset, verbose=0)
gru_train_pred, gru_train_hidden = gru_model.predict(train_dataset, verbose=0)

# éªŒè¯é›†ç‰¹å¾æå–
lstm_val_pred, lstm_val_hidden = lstm_model.predict(val_dataset, verbose=0)
gru_val_pred, gru_val_hidden = gru_model.predict(val_dataset, verbose=0)

# æµ‹è¯•é›†ç‰¹å¾æå–
lstm_test_pred, lstm_test_hidden = lstm_model.predict(test_dataset, verbose=0)
gru_test_pred, gru_test_hidden = gru_model.predict(test_dataset, verbose=0)

print(f"LSTMéšè—çŠ¶æ€ç»´åº¦: {lstm_train_hidden.shape}")
print(f"GRUéšè—çŠ¶æ€ç»´åº¦: {gru_train_hidden.shape}")

# ======================= ä½¿ç”¨éšè—ç‰¹å¾è®­ç»ƒXGBoost =======================
print("\nä½¿ç”¨LSTM+GRUéšè—ç‰¹å¾è®­ç»ƒXGBoost...")

# æ–¹æ¡ˆ1: åªä½¿ç”¨éšè—ç‰¹å¾
hidden_train_features = np.concatenate([lstm_train_hidden, gru_train_hidden], axis=1)
hidden_val_features = np.concatenate([lstm_val_hidden, gru_val_hidden], axis=1)
hidden_test_features = np.concatenate([lstm_test_hidden, gru_test_hidden], axis=1)

# æ”¹è¿›1: é™ç»´ - ä½¿ç”¨PCAå‡å°‘ç»´åº¦
from sklearn.decomposition import PCA

print(f"åŸå§‹éšè—ç‰¹å¾ç»´åº¦: {hidden_train_features.shape[1]}")

pca = PCA(n_components=30, random_state=666)  # é™è‡³30ç»´
hidden_train_pca = pca.fit_transform(hidden_train_features)
hidden_val_pca = pca.transform(hidden_val_features)
hidden_test_pca = pca.transform(hidden_test_features)
print(f"PCAåç»´åº¦: {hidden_train_pca.shape[1]}, ä¿ç•™æ–¹å·®: {pca.explained_variance_ratio_.sum():.4f}")

# æ”¹è¿›2: ä¼˜åŒ–XGBoostè¶…å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰- ä¿®å¤ç‰ˆæœ¬
xgb_hidden_model = XGBRegressor(
    n_estimators=1000,  # è®¾ç½®è¾ƒå¤§å€¼ï¼Œè®©early_stoppingå†³å®šå®é™…è¿­ä»£æ¬¡æ•°
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    early_stopping_rounds=10,  # åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
    random_state=666
)

xgb_hidden_model.fit(
    hidden_train_pca,
    train_labels,
    eval_set=[(hidden_val_pca, val_labels)],
    verbose=False
)

# æ–¹æ¡ˆ2: ä¼ ç»ŸXGBoostï¼ˆå±•å¹³è¾“å…¥ï¼‰ - åŒæ ·ä¼˜åŒ–å‚æ•°
train_dataset_flat = train_dataset.reshape(train_dataset.shape[0], -1)
val_dataset_flat = val_dataset.reshape(val_dataset.shape[0], -1)
test_dataset_flat = test_dataset.reshape(test_dataset.shape[0], -1)

xgb_flat_model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    early_stopping_rounds=10,  # åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
    random_state=666
)

xgb_flat_model.fit(
    train_dataset_flat,
    train_labels,
    eval_set=[(val_dataset_flat, val_labels)],
    verbose=False
)

# ======================= ç”Ÿæˆæ‰€æœ‰é¢„æµ‹ =======================
print("\nç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")

# LSTMå’ŒGRUçš„ç›´æ¥é¢„æµ‹
lstm_test_pred = lstm_test_pred.flatten()
gru_test_pred = gru_test_pred.flatten()

# åŸºäºéšè—ç‰¹å¾çš„XGBoosté¢„æµ‹ï¼ˆä½¿ç”¨PCAè½¬æ¢ï¼‰
xgb_hidden_pred = xgb_hidden_model.predict(hidden_test_pca)

# ä¼ ç»ŸXGBoosté¢„æµ‹
xgb_flat_pred = xgb_flat_model.predict(test_dataset_flat)

# ======================= å¤šç§èåˆç­–ç•¥ =======================
print("\nåº”ç”¨ä¸åŒçš„èåˆç­–ç•¥...")

# ç­–ç•¥1: ç®€å•å¹³å‡ï¼ˆå‚è€ƒç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼‰
ensemble_avg = (lstm_test_pred + gru_test_pred + xgb_hidden_pred) / 3

# ç­–ç•¥2: åŠ æƒå¹³å‡ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½ï¼‰
lstm_val_r2 = r2_score(val_labels, lstm_val_pred.flatten())
gru_val_r2 = r2_score(val_labels, gru_val_pred.flatten())
xgb_hidden_val_pred = xgb_hidden_model.predict(hidden_val_pca)
xgb_val_r2 = r2_score(val_labels, xgb_hidden_val_pred)

# åªä½¿ç”¨æ­£RÂ²çš„æ¨¡å‹ï¼Œé¿å…è´Ÿè´¡çŒ®
weights = []
models_for_ensemble = []
if lstm_val_r2 > 0:
    weights.append(lstm_val_r2)
    models_for_ensemble.append(('lstm', lstm_test_pred))
if gru_val_r2 > 0:
    weights.append(gru_val_r2)
    models_for_ensemble.append(('gru', gru_test_pred))
if xgb_val_r2 > 0:
    weights.append(xgb_val_r2)
    models_for_ensemble.append(('xgb', xgb_hidden_pred))

total_weight = sum(weights)
if total_weight > 0:
    ensemble_weighted = sum(w / total_weight * pred for w, (_, pred) in zip(weights, models_for_ensemble))
    print(f"\nåŠ æƒç³»æ•°:")
    for (name, _), w in zip(models_for_ensemble, weights):
        print(f"  {name.upper()}: {w / total_weight:.3f}")
else:
    # å¦‚æœæ‰€æœ‰æ¨¡å‹RÂ²éƒ½<=0ï¼Œä½¿ç”¨ç®€å•å¹³å‡
    ensemble_weighted = ensemble_avg
    print("\nè­¦å‘Š: æ‰€æœ‰æ¨¡å‹RÂ²éƒ½ä¸ä¸ºæ­£ï¼Œä½¿ç”¨ç®€å•å¹³å‡")

# ç­–ç•¥3: çº¿æ€§å›å½’å…ƒæ¨¡å‹
val_stacked_features = np.column_stack((lstm_val_pred.flatten(), gru_val_pred.flatten(), xgb_hidden_val_pred))
meta_model = LinearRegression()
meta_model.fit(val_stacked_features, val_labels)

test_stacked_features = np.column_stack((lstm_test_pred, gru_test_pred, xgb_hidden_pred))
ensemble_meta = meta_model.predict(test_stacked_features)

print(f"\nçº¿æ€§å›å½’ç³»æ•° (LSTM, GRU, XGBoost): {meta_model.coef_}")
print(f"çº¿æ€§å›å½’æˆªè·: {meta_model.intercept_}")

# ç­–ç•¥4: åªä½¿ç”¨LSTMå’ŒGRUçš„ç®€å•å¹³å‡ï¼ˆæ’é™¤XGBoostï¼‰
ensemble_lstm_gru_only = (lstm_test_pred + gru_test_pred) / 2

# ç­–ç•¥5: åªä½¿ç”¨LSTMå’ŒGRUçš„åŠ æƒå¹³å‡
lstm_gru_total = lstm_val_r2 + gru_val_r2
if lstm_gru_total > 0:
    w_lstm_only = lstm_val_r2 / lstm_gru_total
    w_gru_only = gru_val_r2 / lstm_gru_total
    ensemble_lstm_gru_weighted = w_lstm_only * lstm_test_pred + w_gru_only * gru_test_pred
    print(f"\nLSTM+GRUåŠ æƒç³»æ•° (LSTM: {w_lstm_only:.3f}, GRU: {w_gru_only:.3f})")
else:
    ensemble_lstm_gru_weighted = ensemble_lstm_gru_only

# ======================= æ€§èƒ½è¯„ä¼° =======================
print("\n" + "=" * 80)
print("éªŒè¯é›†æ€§èƒ½ï¼ˆç”¨äºæƒé‡è®¡ç®—ï¼‰:")
print("-" * 80)
print(f"LSTM  - Val RÂ²: {lstm_val_r2:.6f}")
print(f"GRU   - Val RÂ²: {gru_val_r2:.6f}")
print(f"XGBoost - Val RÂ²: {xgb_val_r2:.6f}")
if xgb_val_r2 < 0:
    print("âš ï¸  è­¦å‘Š: XGBooståœ¨éªŒè¯é›†ä¸ŠRÂ²ä¸ºè´Ÿï¼Œè¡¨æ˜æ¨¡å‹ä¸¥é‡è¿‡æ‹Ÿåˆæˆ–ä¸é€‚é…")
print("=" * 80)

print("\nå„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”ï¼ˆå½’ä¸€åŒ–æ•°æ®ï¼‰:")
print("-" * 80)

models_results = {
    'LSTM': lstm_test_pred,
    'GRU': gru_test_pred,
    'XGBoost(éšè—ç‰¹å¾+PCA)': xgb_hidden_pred,
    'XGBoost(å±•å¹³æ•°æ®)': xgb_flat_pred,
    'é›†æˆ-ç®€å•å¹³å‡(3æ¨¡å‹)': ensemble_avg,
    'é›†æˆ-åŠ æƒå¹³å‡(æ­£RÂ²æ¨¡å‹)': ensemble_weighted,
    'é›†æˆ-çº¿æ€§å›å½’': ensemble_meta,
    'é›†æˆ-LSTM+GRUç®€å•å¹³å‡': ensemble_lstm_gru_only,
    'é›†æˆ-LSTM+GRUåŠ æƒå¹³å‡': ensemble_lstm_gru_weighted
}

for name, preds in models_results.items():
    r2 = r2_score(test_labels, preds)
    rmse = sqrt(mean_squared_error(test_labels, preds))
    mae = mean_absolute_error(test_labels, preds)

    # æ·»åŠ æ€§èƒ½æ ‡è®°
    if r2 > 0.85:
        marker = "âœ… ä¼˜ç§€"
    elif r2 > 0.70:
        marker = "âœ“ è‰¯å¥½"
    elif r2 > 0.50:
        marker = "â—‹ ä¸€èˆ¬"
    elif r2 > 0:
        marker = "â–³ è¾ƒå·®"
    else:
        marker = "âœ— å¤±è´¥"

    print(f"{name:30s} | RÂ²: {r2:.6f} | RMSE: {rmse:.6f} | MAE: {mae:.6f} {marker}")

print("=" * 80)

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = max(models_results.keys(),
                      key=lambda x: r2_score(test_labels, models_results[x]))
final_preds = models_results[best_model_name]

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   æµ‹è¯•é›†RÂ²: {r2_score(test_labels, final_preds):.6f}")

# åˆ†æå’Œå»ºè®®
print("\n" + "=" * 80)
print("ğŸ“Š æ¨¡å‹è¡¨ç°åˆ†æ:")
print("-" * 80)

if xgb_val_r2 < 0:
    print("âš ï¸  XGBoostå¤±è´¥åŸå› åˆ†æ:")
    print(f"   1. æ ·æœ¬æ•°({len(train_labels)}) vs åŸå§‹ç‰¹å¾ç»´(200) - æ¯”ä¾‹è¿‡ä½")
    print(f"   2. PCAé™ç»´å({hidden_train_pca.shape[1]}ç»´)æ”¹å–„æ•ˆæœæœ‰é™")
    print(f"   3. æ·±åº¦å­¦ä¹ éšè—ç‰¹å¾å¯èƒ½ä¸é€‚åˆæ ‘æ¨¡å‹çš„åˆ†è£‚ç­–ç•¥")
    print("\nğŸ’¡ å»ºè®®:")
    print("   - å¯¹äºæ­¤æ•°æ®é›†,æ·±åº¦å­¦ä¹ æ¨¡å‹(LSTM/GRU)å·²ç»è¶³å¤Ÿå¥½")
    print("   - é›†æˆæ–¹æ³•åº”æ’é™¤è¡¨ç°å·®çš„XGBoost")
    print("   - ä½¿ç”¨LSTM+GRUçš„åŠ æƒå¹³å‡å¯èƒ½æ˜¯æœ€ä½³é€‰æ‹©")
else:
    print("âœ… æ‰€æœ‰æ¨¡å‹å‡è¡¨ç°æ­£å¸¸")

# è¾“å‡ºæœ€ä¼˜é›†æˆç­–ç•¥
lstm_gru_weighted_r2 = r2_score(test_labels, ensemble_lstm_gru_weighted)
best_ensemble_r2 = r2_score(test_labels, final_preds)

if abs(lstm_gru_weighted_r2 - best_ensemble_r2) < 0.01:
    print(f"\nğŸ¯ æ¨èä½¿ç”¨: LSTM+GRUåŠ æƒå¹³å‡ (æ›´ç¨³å®šã€æ— XGBoostä¾èµ–)")
    print(f"   RÂ² = {lstm_gru_weighted_r2:.6f}")

print("=" * 80)

# ========== åå½’ä¸€åŒ– ==========
test_labels_original = y_scaler.inverse_transform(test_labels.reshape(-1, 1))
final_preds_original = y_scaler.inverse_transform(final_preds.reshape(-1, 1))
lstm_preds_original = y_scaler.inverse_transform(lstm_test_pred.reshape(-1, 1))
gru_preds_original = y_scaler.inverse_transform(gru_test_pred.reshape(-1, 1))
xgb_hidden_preds_original = y_scaler.inverse_transform(xgb_hidden_pred.reshape(-1, 1))

# åŸå§‹å°ºåº¦æŒ‡æ ‡
print("\næœ€ä½³æ¨¡å‹åœ¨åŸå§‹å°ºåº¦ä¸Šçš„æŒ‡æ ‡:")
print(f"RÂ² å€¼: {r2_score(test_labels_original, final_preds_original):.6f}")
print(f"MAE: {mean_absolute_error(test_labels_original, final_preds_original):.6f}")
print(f"MSE: {mean_squared_error(test_labels_original, final_preds_original):.6f}")
print(f"RMSE: {sqrt(mean_squared_error(test_labels_original, final_preds_original)):.6f}")
print(f"MAPE: {np.mean(np.abs((final_preds_original - test_labels_original) / (test_labels_original + 1e-8))):.6f}")
print("=" * 80)

# ======================= å¯è§†åŒ–ç»“æœ =======================
results_directory = "./Predict/"
if not os.path.exists(results_directory):
    os.makedirs(results_directory)

# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['loss'], label='Train Loss', linewidth=2)
plt.plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('LSTM æ¨¡å‹è®­ç»ƒä¸éªŒè¯æŸå¤±', fontsize=14)
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='Train Loss', linewidth=2)
plt.plot(gru_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('GRU æ¨¡å‹è®­ç»ƒä¸éªŒè¯æŸå¤±', fontsize=14)
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_directory + 'model_training_loss.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”
plt.figure(figsize=(20, 12))

plt.subplot(3, 4, 1)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(lstm_preds_original, label="LSTMé¢„æµ‹", linewidth=2, alpha=0.7)
plt.title(f"LSTM (RÂ²={r2_score(test_labels, lstm_test_pred):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 2)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(gru_preds_original, label="GRUé¢„æµ‹", linewidth=2, alpha=0.7)
plt.title(f"GRU (RÂ²={r2_score(test_labels, gru_test_pred):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 3)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(xgb_hidden_preds_original, label="XGBoosté¢„æµ‹", linewidth=2, alpha=0.7)
r2_xgb = r2_score(test_labels, xgb_hidden_pred)
title_color = 'red' if r2_xgb < 0 else 'black'
plt.title(f"XGBoost-PCA (RÂ²={r2_xgb:.4f})", color=title_color)
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 4)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(y_scaler.inverse_transform(ensemble_avg.reshape(-1, 1)),
         label="ç®€å•å¹³å‡", linewidth=2, alpha=0.7, color='purple')
plt.title(f"é›†æˆ-ç®€å•å¹³å‡ (RÂ²={r2_score(test_labels, ensemble_avg):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 5)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(y_scaler.inverse_transform(ensemble_weighted.reshape(-1, 1)),
         label="æ™ºèƒ½åŠ æƒ", linewidth=2, alpha=0.7, color='orange')
plt.title(f"é›†æˆ-æ™ºèƒ½åŠ æƒ (RÂ²={r2_score(test_labels, ensemble_weighted):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 6)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(y_scaler.inverse_transform(ensemble_meta.reshape(-1, 1)),
         label="çº¿æ€§å›å½’", linewidth=2, alpha=0.7, color='green')
plt.title(f"é›†æˆ-çº¿æ€§å›å½’ (RÂ²={r2_score(test_labels, ensemble_meta):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 7)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(y_scaler.inverse_transform(ensemble_lstm_gru_only.reshape(-1, 1)),
         label="LSTM+GRUå¹³å‡", linewidth=2, alpha=0.7, color='cyan')
plt.title(f"LSTM+GRUç®€å•å¹³å‡ (RÂ²={r2_score(test_labels, ensemble_lstm_gru_only):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 8)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2, color='black')
plt.plot(y_scaler.inverse_transform(ensemble_lstm_gru_weighted.reshape(-1, 1)),
         label="LSTM+GRUåŠ æƒ", linewidth=2, alpha=0.7, color='magenta')
plt.title(f"LSTM+GRUåŠ æƒå¹³å‡ (RÂ²={r2_score(test_labels, ensemble_lstm_gru_weighted):.4f})")
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 9)
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2.5, color='black')
plt.plot(final_preds_original, label=f"æœ€ä½³({best_model_name})",
         linewidth=2, alpha=0.8, color='red')
plt.title("ğŸ† æœ€ä½³æ¨¡å‹é¢„æµ‹", fontsize=12, fontweight='bold')
plt.legend(fontsize=8)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 10)
# æ®‹å·®å›¾
residuals = test_labels_original.flatten() - final_preds_original.flatten()
plt.scatter(final_preds_original, residuals, alpha=0.6, color='crimson', s=20)
plt.axhline(0, color='black', linestyle='--', linewidth=2)
plt.title("æ®‹å·®åˆ†æå›¾")
plt.xlabel("é¢„æµ‹å€¼")
plt.ylabel("æ®‹å·®")
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 11)
# æ‰€æœ‰æ¨¡å‹å¯¹æ¯”
plt.plot(test_labels_original, label="çœŸå®å€¼", linewidth=2.5, color='black', alpha=0.8)
plt.plot(lstm_preds_original, label="LSTM", linewidth=1, alpha=0.4)
plt.plot(gru_preds_original, label="GRU", linewidth=1, alpha=0.4)
if r2_xgb > 0:
    plt.plot(xgb_hidden_preds_original, label="XGBoost", linewidth=1, alpha=0.4)
plt.plot(final_preds_original, label="æœ€ä½³é›†æˆ", linewidth=2, alpha=0.8, color='red')
plt.title("æ‰€æœ‰æ¨¡å‹å¯¹æ¯”")
plt.legend(fontsize=7)
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 12)
# RÂ²å¯¹æ¯”æŸ±çŠ¶å›¾
model_names = ['LSTM', 'GRU', 'XGB-PCA', 'LSTM+GRU\nåŠ æƒ']
r2_scores_plot = [
    r2_score(test_labels, lstm_test_pred),
    r2_score(test_labels, gru_test_pred),
    r2_score(test_labels, xgb_hidden_pred),
    r2_score(test_labels, ensemble_lstm_gru_weighted)
]
colors = ['skyblue', 'lightgreen', 'salmon' if r2_scores_plot[2] < 0 else 'lightyellow', 'gold']
bars = plt.bar(model_names, r2_scores_plot, color=colors, alpha=0.7, edgecolor='black')
plt.axhline(0, color='red', linestyle='--', linewidth=1)
plt.title("RÂ² æ€§èƒ½å¯¹æ¯”")
plt.ylabel("RÂ² Score")
plt.xticks(rotation=15, ha='right', fontsize=9)
plt.grid(True, alpha=0.3, axis='y')
# åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
for bar, score in zip(bars, r2_scores_plot):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2., height,
             f'{score:.3f}',
             ha='center', va='bottom' if height > 0 else 'top', fontsize=9)

plt.tight_layout()
plt.savefig(results_directory + 'improved_stacked_model_comparison.png', dpi=300, bbox_inches='tight')
plt.show(block=True)

# ======================= ä¿å­˜æ¨¡å‹ =======================
import pickle

with open(results_directory + 'stacked_scalers.pkl', 'wb') as f:
    pickle.dump({
        'feature_scalers': feature_scalers,
        'y_scaler': y_scaler,
        'pca': pca
    }, f)

with open(results_directory + 'meta_model.pkl', 'wb') as f:
    pickle.dump(meta_model, f)

lstm_model.save(results_directory + 'lstm_hidden_model.h5')
gru_model.save(results_directory + 'gru_hidden_model.h5')
xgb_hidden_model.save_model(results_directory + 'xgboost_hidden_model.json')
xgb_flat_model.save_model(results_directory + 'xgboost_flat_model.json')

print('\næ‰€æœ‰æ¨¡å‹å’Œå½’ä¸€åŒ–å™¨å·²ä¿å­˜åˆ°:', results_directory)
print('=' * 80)