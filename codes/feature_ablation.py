"""
ç‰¹å¾çº§åˆ«æ¶ˆèå®éªŒæ¨¡å—
====================

ä¸“é—¨ç”¨äºé‡åŒ–æ¯ä¸ªè¾“å…¥ç‰¹å¾ï¼ˆå¦‚EPUã€GPRï¼‰å¯¹é¢„æµ‹æ€§èƒ½çš„è´¡çŒ®åº¦

åŠŸèƒ½ï¼š
1. é€ä¸ªç§»é™¤ç‰¹å¾çš„æ¶ˆèå®éªŒ
2. é‡åŒ–æ¯ä¸ªç‰¹å¾çš„ç»å¯¹/ç›¸å¯¹è´¡çŒ®
3. ç‰¹å¾é‡è¦æ€§æ’åº
4. ç‰¹å¾ç»„åˆæ•ˆåº”åˆ†æ
5. å­¦æœ¯æŠ¥å‘Šç”Ÿæˆ

ä½œè€…ï¼šFeature Ablation Module
ç‰ˆæœ¬ï¼š1.0.0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from math import sqrt
import os
import time
from collections import defaultdict
import warnings

warnings.filterwarnings('ignore')


class FeatureAblationAnalyzer:
    """
    ç‰¹å¾æ¶ˆèåˆ†æå™¨

    ä¸“é—¨ç”¨äºåˆ†ææ¯ä¸ªè¾“å…¥ç‰¹å¾ï¼ˆå¦‚EPUã€GPRã€VIXç­‰ï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®
    """

    def __init__(self, y_test_true, feature_names, output_dir='./feature_ablation/'):
        """
        åˆå§‹åŒ–ç‰¹å¾æ¶ˆèåˆ†æå™¨

        Parameters:
        -----------
        y_test_true : array-like
            æµ‹è¯•é›†çœŸå®å€¼
        feature_names : list
            æ‰€æœ‰ç‰¹å¾åç§°åˆ—è¡¨
        output_dir : str
            ç»“æœä¿å­˜ç›®å½•
        """
        self.y_test_true = np.array(y_test_true).flatten()
        self.feature_names = feature_names
        self.output_dir = output_dir

        self.experiments = {}
        self.baseline_performance = None

        os.makedirs(output_dir, exist_ok=True)

        print("\n" + "=" * 100)
        print("ã€ç‰¹å¾çº§åˆ«æ¶ˆèå®éªŒï¼ˆFeature-Level Ablation Studyï¼‰ã€‘".center(100))
        print("é‡åŒ–æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹æ€§èƒ½çš„è´¡çŒ®åº¦".center(100))
        print("=" * 100)
        print(f"\nåˆå§‹åŒ–å®Œæˆ:")
        print(f"  - ç‰¹å¾æ€»æ•°: {len(feature_names)}")
        print(f"  - ç‰¹å¾åˆ—è¡¨: {feature_names}")
        print(f"  - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(self.y_test_true)}")
        print(f"  - è¾“å‡ºç›®å½•: {output_dir}")

    def add_baseline(self, predictions, description="ä½¿ç”¨å…¨éƒ¨ç‰¹å¾"):
        """
        æ·»åŠ åŸºçº¿å®éªŒï¼ˆä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰

        Parameters:
        -----------
        predictions : array-like
            åŸºçº¿æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        description : str
            å®éªŒæè¿°
        """
        predictions = np.array(predictions).flatten()

        metrics = self._calculate_metrics(predictions)

        self.experiments['å…¨éƒ¨ç‰¹å¾ï¼ˆåŸºçº¿ï¼‰'] = {
            'type': 'baseline',
            'predictions': predictions,
            'metrics': metrics,
            'description': description,
            'removed_features': [],
            'remaining_features': self.feature_names.copy()
        }

        self.baseline_performance = metrics

        print(f"\nâœ… åŸºçº¿å®éªŒå·²æ·»åŠ ï¼ˆä½¿ç”¨å…¨éƒ¨{len(self.feature_names)}ä¸ªç‰¹å¾ï¼‰")
        print(f"   åŸºçº¿R^2: {metrics['R^2']:.6f}")
        print(f"   åŸºçº¿MAE: {metrics['MAE']:.6f}")
        print(f"   åŸºçº¿RMSE: {metrics['RMSE']:.6f}")

    def add_single_feature_removal(self, feature_name, predictions,
                                   train_time=None, description=""):
        """
        æ·»åŠ å•ä¸ªç‰¹å¾ç§»é™¤å®éªŒ

        Parameters:
        -----------
        feature_name : str
            è¢«ç§»é™¤çš„ç‰¹å¾åç§°
        predictions : array-like
            ç§»é™¤è¯¥ç‰¹å¾åçš„é¢„æµ‹ç»“æœ
        train_time : float, optional
            è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
        description : str
            å®éªŒæè¿°
        """
        if feature_name not in self.feature_names:
            raise ValueError(f"ç‰¹å¾ '{feature_name}' ä¸åœ¨ç‰¹å¾åˆ—è¡¨ä¸­")

        predictions = np.array(predictions).flatten()
        metrics = self._calculate_metrics(predictions)

        if train_time:
            metrics['train_time'] = train_time

        exp_name = f'w/o {feature_name}'

        self.experiments[exp_name] = {
            'type': 'single_removal',
            'predictions': predictions,
            'metrics': metrics,
            'description': description or f'ç§»é™¤ç‰¹å¾: {feature_name}',
            'removed_features': [feature_name],
            'remaining_features': [f for f in self.feature_names if f != feature_name]
        }

        # è®¡ç®—è´¡çŒ®åº¦
        if self.baseline_performance:
            contribution = self._calculate_contribution(metrics)
            self.experiments[exp_name]['contribution'] = contribution

            print(f"\nâœ“ å®éªŒå·²æ·»åŠ : {exp_name}")
            print(f"   R^2: {metrics['R^2']:.6f} (vsåŸºçº¿: {contribution['r2_drop']:+.6f})")
            print(f"   ç›¸å¯¹è´¡çŒ®: {contribution['relative_contribution']:.2f}%")
        else:
            print(f"\nâœ“ å®éªŒå·²æ·»åŠ : {exp_name}")
            print(f"   âš ï¸  è¯·å…ˆæ·»åŠ åŸºçº¿å®éªŒä»¥è®¡ç®—è´¡çŒ®åº¦")

    def _calculate_metrics(self, predictions):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        return {
            'R^2': r2_score(self.y_test_true, predictions),
            'MAE': mean_absolute_error(self.y_test_true, predictions),
            'RMSE': sqrt(mean_squared_error(self.y_test_true, predictions)),
            'MAPE': np.mean(np.abs((self.y_test_true - predictions) /
                                  (self.y_test_true + 1e-8))) * 100
        }

    def _calculate_contribution(self, ablated_metrics):
        """è®¡ç®—ç‰¹å¾è´¡çŒ®åº¦"""
        baseline_r2 = self.baseline_performance['R^2']
        ablated_r2 = ablated_metrics['R^2']

        # R^2ä¸‹é™ï¼ˆç»å¯¹è´¡çŒ®ï¼‰
        r2_drop = baseline_r2 - ablated_r2

        # ç›¸å¯¹è´¡çŒ®ï¼ˆ%ï¼‰
        relative_contribution = (r2_drop / baseline_r2) * 100 if baseline_r2 != 0 else 0

        # é‡è¦æ€§ç­‰çº§
        if relative_contribution >= 5:
            importance = "â­â­â­ å…³é”®"
        elif relative_contribution >= 2:
            importance = "â­â­ é‡è¦"
        elif relative_contribution >= 1:
            importance = "â­ ä¸€èˆ¬"
        else:
            importance = "â—‹ è½»å¾®"

        return {
            'r2_drop': r2_drop,
            'relative_contribution': relative_contribution,
            'importance_level': importance
        }

    def generate_feature_importance_report(self):
        """
        ç”Ÿæˆç‰¹å¾é‡è¦æ€§æŠ¥å‘Š

        Returns:
        --------
        report_df : DataFrame
            ç‰¹å¾é‡è¦æ€§æŠ¥å‘Šè¡¨
        """
        print("\n" + "=" * 100)
        print("ã€ç‰¹å¾é‡è¦æ€§æŠ¥å‘Šï¼ˆFeature Importance Reportï¼‰ã€‘".center(100))
        print("=" * 100)

        if self.baseline_performance is None:
            print("âŒ é”™è¯¯ï¼šè¯·å…ˆæ·»åŠ åŸºçº¿å®éªŒ")
            return None

        # æ”¶é›†æ•°æ®
        report_data = []

        for exp_name, exp_data in self.experiments.items():
            if exp_data['type'] == 'baseline':
                continue

            if exp_data['type'] == 'single_removal':
                feature = exp_data['removed_features'][0]
                metrics = exp_data['metrics']
                contribution = exp_data.get('contribution', {})

                report_data.append({
                    'ç‰¹å¾åç§° (Feature)': feature,
                    'ç§»é™¤åR^2 (R^2 w/o)': f"{metrics['R^2']:.6f}",
                    'R^2ä¸‹é™ (R^2 Drop)': f"{contribution.get('r2_drop', 0):.6f}",
                    'ç›¸å¯¹è´¡çŒ®(%) (Contribution)': f"{contribution.get('relative_contribution', 0):.2f}%",
                    'é‡è¦æ€§ (Importance)': contribution.get('importance_level', 'N/A'),
                    'MAE w/o': f"{metrics['MAE']:.6f}",
                    'RMSE w/o': f"{metrics['RMSE']:.6f}"
                })

        # åˆ›å»ºDataFrame
        report_df = pd.DataFrame(report_data)

        # æ’åºï¼ˆæŒ‰R^2ä¸‹é™é™åºï¼‰
        if len(report_df) > 0:
            report_df['_sort_key'] = report_df['R^2ä¸‹é™ (R^2 Drop)'].str.replace('R^2ä¸‹é™ \\(R^2 Drop\\)', '').astype(float)
            report_df = report_df.sort_values('_sort_key', ascending=False).drop('_sort_key', axis=1)

        # æ‰“å°æŠ¥å‘Š
        print(f"\nåŸºçº¿æ€§èƒ½ï¼ˆä½¿ç”¨å…¨éƒ¨{len(self.feature_names)}ä¸ªç‰¹å¾ï¼‰ï¼š")
        print(f"  R^2 = {self.baseline_performance['R^2']:.6f}")
        print(f"  MAE = {self.baseline_performance['MAE']:.6f}")
        print(f"  RMSE = {self.baseline_performance['RMSE']:.6f}")

        print(f"\n" + "-" * 100)
        print("ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆæŒ‰è´¡çŒ®åº¦é™åºï¼‰ï¼š")
        print("-" * 100)
        print(report_df.to_string(index=False))

        # ä¿å­˜CSV
        csv_path = os.path.join(self.output_dir, 'feature_importance_ranking.csv')
        report_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {csv_path}")

        # å…³é”®å‘ç°
        self._print_key_findings(report_df)

        return report_df

    def _print_key_findings(self, report_df):
        """æ‰“å°å…³é”®å‘ç°"""
        print("\n" + "=" * 100)
        print("ã€å…³é”®å‘ç°ï¼ˆKey Findingsï¼‰ã€‘".center(100))
        print("=" * 100)

        if len(report_df) == 0:
            return

        # Top 3 æœ€é‡è¦ç‰¹å¾
        print(f"\nğŸ† Top 3 æœ€é‡è¦ç‰¹å¾:")
        for i in range(min(3, len(report_df))):
            row = report_df.iloc[i]
            print(f"  {i+1}. {row['ç‰¹å¾åç§° (Feature)']}")
            print(f"     - R^2ä¸‹é™: {row['R^2ä¸‹é™ (R^2 Drop)']}")
            print(f"     - ç›¸å¯¹è´¡çŒ®: {row['ç›¸å¯¹è´¡çŒ®(%) (Contribution)']}")
            print(f"     - é‡è¦æ€§: {row['é‡è¦æ€§ (Importance)']}")

        # ç»Ÿè®¡æ‘˜è¦
        contributions = [float(row['ç›¸å¯¹è´¡çŒ®(%) (Contribution)'].rstrip('%'))
                        for _, row in report_df.iterrows()]

        critical_count = sum(1 for c in contributions if c >= 5)
        important_count = sum(1 for c in contributions if 2 <= c < 5)
        moderate_count = sum(1 for c in contributions if 1 <= c < 2)
        minor_count = sum(1 for c in contributions if c < 1)

        print(f"\nğŸ“Š é‡è¦æ€§ç»Ÿè®¡:")
        print(f"  - å…³é”®ç‰¹å¾ (â‰¥5%): {critical_count}")
        print(f"  - é‡è¦ç‰¹å¾ (2-5%): {important_count}")
        print(f"  - ä¸€èˆ¬ç‰¹å¾ (1-2%): {moderate_count}")
        print(f"  - è½»å¾®ç‰¹å¾ (<1%): {minor_count}")

        # ç´¯è®¡è´¡çŒ®åº¦
        total_contribution = sum(contributions)
        print(f"\nğŸ’¡ ç´¯è®¡ç›¸å¯¹è´¡çŒ®åº¦: {total_contribution:.2f}%")
        print(f"   è¯´æ˜: æ‰€æœ‰ç‰¹å¾å…±åŒè´¡çŒ®äº†åŸºçº¿æ¨¡å‹{total_contribution:.1f}%çš„æ€§èƒ½")

    def visualize_feature_importance(self, show=True):
        """
        å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§

        Parameters:
        -----------
        show : bool
            æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        """
        print("\nç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯è§†åŒ–...")

        self._plot_feature_ranking(show)
        self._plot_contribution_breakdown(show)
        self._plot_performance_comparison(show)

        print(f"\nâœ… æ‰€æœ‰å¯è§†åŒ–å·²ä¿å­˜è‡³: {self.output_dir}")

    def _plot_feature_ranking(self, show=True):
        """å›¾1: ç‰¹å¾é‡è¦æ€§æ’åº"""
        # æå–æ•°æ®
        features = []
        r2_drops = []
        relative_contribs = []

        for exp_name, exp_data in sorted(
            self.experiments.items(),
            key=lambda x: x[1].get('contribution', {}).get('r2_drop', 0),
            reverse=True
        ):
            if exp_data['type'] == 'single_removal':
                feature = exp_data['removed_features'][0]
                contribution = exp_data.get('contribution', {})

                features.append(feature)
                r2_drops.append(contribution.get('r2_drop', 0))
                relative_contribs.append(contribution.get('relative_contribution', 0))

        if len(features) == 0:
            print("  âš ï¸ æ— ç‰¹å¾æ•°æ®ï¼Œè·³è¿‡è¯¥å›¾è¡¨")
            return

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

        # é…è‰²æ–¹æ¡ˆ
        colors = []
        for contrib in relative_contribs:
            if contrib >= 5:
                colors.append('#e74c3c')  # çº¢è‰² - å…³é”®
            elif contrib >= 2:
                colors.append('#f39c12')  # æ©™è‰² - é‡è¦
            elif contrib >= 1:
                colors.append('#f1c40f')  # é»„è‰² - ä¸€èˆ¬
            else:
                colors.append('#95a5a6')  # ç°è‰² - è½»å¾®

        # å­å›¾1: ç»å¯¹è´¡çŒ®ï¼ˆR^2ä¸‹é™ï¼‰
        bars1 = ax1.barh(range(len(features)), r2_drops, color=colors, alpha=0.8)

        for i, (bar, drop, contrib) in enumerate(zip(bars1, r2_drops, relative_contribs)):
            label = f'{drop:.6f}\n({contrib:.2f}%)'
            ax1.text(bar.get_width() + 0.0005, bar.get_y() + bar.get_height()/2,
                    label, ha='left', va='center', fontsize=9, fontweight='bold')

        ax1.set_yticks(range(len(features)))
        ax1.set_yticklabels(features, fontsize=11)
        ax1.set_xlabel('R^2 ä¸‹é™ï¼ˆç»å¯¹è´¡çŒ®ï¼‰', fontsize=12, fontweight='bold')
        ax1.set_title('ç§»é™¤ç‰¹å¾åçš„æ€§èƒ½ä¸‹é™\n(æ•°å€¼è¶Šå¤§=ç‰¹å¾è¶Šé‡è¦)',
                     fontsize=13, fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='x')
        ax1.axvline(0, color='black', linewidth=1)

        # å­å›¾2: ç›¸å¯¹è´¡çŒ®ï¼ˆ%ï¼‰
        bars2 = ax2.barh(range(len(features)), relative_contribs, color=colors, alpha=0.8)

        # é‡è¦æ€§é˜ˆå€¼çº¿
        ax2.axvline(5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='å…³é”® (â‰¥5%)')
        ax2.axvline(2, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='é‡è¦ (â‰¥2%)')
        ax2.axvline(1, color='yellow', linestyle='--', alpha=0.5, linewidth=2, label='ä¸€èˆ¬ (â‰¥1%)')

        for i, (bar, contrib) in enumerate(zip(bars2, relative_contribs)):
            ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,
                    f'{contrib:.2f}%', ha='left', va='center',
                    fontsize=10, fontweight='bold')

        ax2.set_yticks(range(len(features)))
        ax2.set_yticklabels(features, fontsize=11)
        ax2.set_xlabel('ç›¸å¯¹è´¡çŒ®åº¦ (%)', fontsize=12, fontweight='bold')
        ax2.set_title('ç‰¹å¾é‡è¦æ€§ï¼ˆç›¸å¯¹å€¼ï¼‰\n(å åŸºçº¿æ€§èƒ½çš„ç™¾åˆ†æ¯”)',
                     fontsize=13, fontweight='bold')
        ax2.legend(loc='lower right', fontsize=10)
        ax2.grid(True, alpha=0.3, axis='x')

        plt.suptitle(f'ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆFeature Importance Rankingï¼‰\nåŸºçº¿R^2={self.baseline_performance["R^2"]:.6f}',
                    fontsize=14, fontweight='bold')
        plt.tight_layout()

        save_path = os.path.join(self.output_dir, '01_feature_importance_ranking.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

        if show:
            plt.show()
        plt.close()

        print("âœ“ å›¾1: ç‰¹å¾é‡è¦æ€§æ’åº")

    def _plot_contribution_breakdown(self, show=True):
        """å›¾2: è´¡çŒ®åº¦åˆ†è§£ï¼ˆé¥¼å›¾+æŸ±çŠ¶å›¾ï¼‰"""
        # æå–æ•°æ®
        features = []
        relative_contribs = []

        for exp_name, exp_data in self.experiments.items():
            if exp_data['type'] == 'single_removal':
                feature = exp_data['removed_features'][0]
                contribution = exp_data.get('contribution', {})
                relative_contribs.append(contribution.get('relative_contribution', 0))
                features.append(feature)

        if len(features) == 0:
            return

        # æ’åº
        sorted_indices = np.argsort(relative_contribs)[::-1]
        features = [features[i] for i in sorted_indices]
        relative_contribs = [relative_contribs[i] for i in sorted_indices]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

        # å­å›¾1: é¥¼å›¾ï¼ˆTop 5ï¼‰
        top_n = min(5, len(features))
        top_features = features[:top_n]
        top_contribs = relative_contribs[:top_n]

        if len(features) > top_n:
            other_contrib = sum(relative_contribs[top_n:])
            top_features.append('å…¶ä»–ç‰¹å¾')
            top_contribs.append(other_contrib)

        colors_pie = plt.cm.Set3(np.linspace(0, 1, len(top_features)))

        wedges, texts, autotexts = ax1.pie(
            top_contribs,
            labels=top_features,
            autopct='%1.1f%%',
            startangle=90,
            colors=colors_pie,
            textprops={'fontsize': 10, 'fontweight': 'bold'}
        )

        ax1.set_title(f'Top {top_n} ç‰¹å¾è´¡çŒ®åº¦å æ¯”',
                     fontsize=13, fontweight='bold')

        # å­å›¾2: ç´¯è®¡è´¡çŒ®æ›²çº¿
        cumulative_contribs = np.cumsum(relative_contribs)

        ax2.bar(range(len(features)), relative_contribs,
               color=plt.cm.viridis(np.linspace(0.2, 0.8, len(features))),
               alpha=0.7, label='å•ä¸ªç‰¹å¾è´¡çŒ®')

        ax2_twin = ax2.twinx()
        ax2_twin.plot(range(len(features)), cumulative_contribs,
                     'ro-', linewidth=3, markersize=8, label='ç´¯è®¡è´¡çŒ®')
        ax2_twin.axhline(80, color='red', linestyle='--', alpha=0.5, label='80%é˜ˆå€¼')

        ax2.set_xlabel('ç‰¹å¾ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰', fontsize=12, fontweight='bold')
        ax2.set_ylabel('ç›¸å¯¹è´¡çŒ®åº¦ (%)', fontsize=11, fontweight='bold')
        ax2_twin.set_ylabel('ç´¯è®¡è´¡çŒ®åº¦ (%)', fontsize=11, fontweight='bold', color='red')

        ax2.set_xticks(range(len(features)))
        ax2.set_xticklabels(features, rotation=45, ha='right', fontsize=9)

        ax2.set_title('ç‰¹å¾è´¡çŒ®åº¦ç´¯è®¡åˆ†æ',
                     fontsize=13, fontweight='bold')
        ax2.legend(loc='upper left', fontsize=10)
        ax2_twin.legend(loc='upper right', fontsize=10)
        ax2.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()

        save_path = os.path.join(self.output_dir, '02_contribution_breakdown.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

        if show:
            plt.show()
        plt.close()

        print("âœ“ å›¾2: è´¡çŒ®åº¦åˆ†è§£")

    def _plot_performance_comparison(self, show=True):
        """å›¾3: æ€§èƒ½å¯¹æ¯”çŸ©é˜µ"""
        # æå–æ•°æ®
        exp_names = []
        r2_scores = []
        mae_scores = []
        rmse_scores = []

        # å…ˆæ·»åŠ åŸºçº¿
        if self.baseline_performance:
            exp_names.append('å…¨éƒ¨ç‰¹å¾\n(åŸºçº¿)')
            r2_scores.append(self.baseline_performance['R^2'])
            mae_scores.append(self.baseline_performance['MAE'])
            rmse_scores.append(self.baseline_performance['RMSE'])

        # æ·»åŠ å„ä¸ªæ¶ˆèå®éªŒ
        for exp_name, exp_data in sorted(
            self.experiments.items(),
            key=lambda x: x[1]['metrics']['R^2'],
            reverse=True
        ):
            if exp_data['type'] == 'single_removal':
                feature = exp_data['removed_features'][0]
                exp_names.append(f'w/o\n{feature}')
                r2_scores.append(exp_data['metrics']['R^2'])
                mae_scores.append(exp_data['metrics']['MAE'])
                rmse_scores.append(exp_data['metrics']['RMSE'])

        if len(exp_names) <= 1:
            return

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(1, 3, figsize=(20, 7))

        x_pos = np.arange(len(exp_names))
        colors = ['gold'] + ['lightblue'] * (len(exp_names) - 1)

        # å­å›¾1: R^2
        bars1 = axes[0].bar(x_pos, r2_scores, color=colors, alpha=0.8, edgecolor='black')
        axes[0].axhline(self.baseline_performance['R^2'], color='red',
                       linestyle='--', linewidth=2, alpha=0.7, label='åŸºçº¿')

        for i, (bar, score) in enumerate(zip(bars1, r2_scores)):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                        f'{score:.4f}', ha='center', va='bottom', fontsize=9)

        axes[0].set_xticks(x_pos)
        axes[0].set_xticklabels(exp_names, fontsize=9)
        axes[0].set_ylabel('R^2 Score', fontsize=11, fontweight='bold')
        axes[0].set_title('R^2 å¯¹æ¯”\n(â†‘ è¶Šé«˜è¶Šå¥½)', fontsize=12, fontweight='bold')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3, axis='y')

        # å­å›¾2: MAE
        bars2 = axes[1].bar(x_pos, mae_scores, color=colors, alpha=0.8, edgecolor='black')
        axes[1].axhline(self.baseline_performance['MAE'], color='red',
                       linestyle='--', linewidth=2, alpha=0.7, label='åŸºçº¿')

        for i, (bar, score) in enumerate(zip(bars2, mae_scores)):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,
                        f'{score:.4f}', ha='center', va='bottom', fontsize=9)

        axes[1].set_xticks(x_pos)
        axes[1].set_xticklabels(exp_names, fontsize=9)
        axes[1].set_ylabel('MAE', fontsize=11, fontweight='bold')
        axes[1].set_title('MAE å¯¹æ¯”\n(â†“ è¶Šä½è¶Šå¥½)', fontsize=12, fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3, axis='y')

        # å­å›¾3: RMSE
        bars3 = axes[2].bar(x_pos, rmse_scores, color=colors, alpha=0.8, edgecolor='black')
        axes[2].axhline(self.baseline_performance['RMSE'], color='red',
                       linestyle='--', linewidth=2, alpha=0.7, label='åŸºçº¿')

        for i, (bar, score) in enumerate(zip(bars3, rmse_scores)):
            axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,
                        f'{score:.4f}', ha='center', va='bottom', fontsize=9)

        axes[2].set_xticks(x_pos)
        axes[2].set_xticklabels(exp_names, fontsize=9)
        axes[2].set_ylabel('RMSE', fontsize=11, fontweight='bold')
        axes[2].set_title('RMSE å¯¹æ¯”\n(â†“ è¶Šä½è¶Šå¥½)', fontsize=12, fontweight='bold')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3, axis='y')

        plt.suptitle('ç§»é™¤ä¸åŒç‰¹å¾åçš„æ€§èƒ½å¯¹æ¯”',
                    fontsize=14, fontweight='bold')
        plt.tight_layout()

        save_path = os.path.join(self.output_dir, '03_performance_comparison.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

        if show:
            plt.show()
        plt.close()

        print("âœ“ å›¾3: æ€§èƒ½å¯¹æ¯”çŸ©é˜µ")

    def export_to_latex(self, filename='feature_importance.tex'):
        """å¯¼å‡ºLaTeXè¡¨æ ¼"""
        report_df = self.generate_feature_importance_report()

        if report_df is None:
            return

        latex_content = []
        latex_content.append("% Feature Importance Table for LaTeX\n")
        latex_content.append("% Auto-generated by FeatureAblationAnalyzer\n\n")

        latex_content.append(report_df.to_latex(index=False, escape=False))

        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.writelines(latex_content)

        print(f"\nâœ… LaTeXè¡¨æ ¼å·²ä¿å­˜: {filepath}")


# =====================================================================================
# ä¾¿æ·å‡½æ•°
# =====================================================================================

def quick_feature_ablation(y_test, feature_names, baseline_pred,
                           ablation_predictions_dict, output_dir='./feature_ablation/'):
    """
    å¿«é€Ÿæ‰§è¡Œç‰¹å¾æ¶ˆèå®éªŒ

    Parameters:
    -----------
    y_test : array-like
        æµ‹è¯•é›†çœŸå®å€¼
    feature_names : list
        ç‰¹å¾åç§°åˆ—è¡¨
    baseline_pred : array-like
        åŸºçº¿é¢„æµ‹ï¼ˆä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
    ablation_predictions_dict : dict
        æ¶ˆèé¢„æµ‹å­—å…¸ï¼Œæ ¼å¼ï¼š{feature_name: predictions}
    output_dir : str
        è¾“å‡ºç›®å½•

    Returns:
    --------
    analyzer : FeatureAblationAnalyzer
        ç‰¹å¾æ¶ˆèåˆ†æå™¨å¯¹è±¡

    Examples:
    ---------
    >>> feature_names = ['EPU', 'GPR', 'VIX', 'OIL']
    >>> ablation_preds = {
    ...     'EPU': pred_without_epu,
    ...     'GPR': pred_without_gpr,
    ...     'VIX': pred_without_vix,
    ...     'OIL': pred_without_oil
    ... }
    >>> analyzer = quick_feature_ablation(
    ...     y_test, feature_names, baseline_pred, ablation_preds
    ... )
    """
    analyzer = FeatureAblationAnalyzer(y_test, feature_names, output_dir)

    # æ·»åŠ åŸºçº¿
    analyzer.add_baseline(baseline_pred)

    # æ·»åŠ å„ä¸ªæ¶ˆèå®éªŒ
    for feature_name, predictions in ablation_predictions_dict.items():
        analyzer.add_single_feature_removal(feature_name, predictions)

    # ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
    analyzer.generate_feature_importance_report()
    analyzer.visualize_feature_importance(show=False)
    analyzer.export_to_latex()

    return analyzer


if __name__ == "__main__":
    print("Feature-Level Ablation Study Module v1.0.0")
    print("\nä¸»è¦åŠŸèƒ½ï¼š")
    print("  âœ“ é‡åŒ–æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®åº¦")
    print("  âœ“ ç‰¹å¾é‡è¦æ€§æ’åº")
    print("  âœ“ ç”Ÿæˆå­¦æœ¯æŠ¥å‘Šå’Œå¯è§†åŒ–")
    print("\nä½¿ç”¨ç¤ºä¾‹ï¼š")
    print("  from feature_ablation import FeatureAblationAnalyzer")
    print("  analyzer = FeatureAblationAnalyzer(y_test, feature_names)")
    print("  analyzer.add_baseline(baseline_predictions)")
    print("  analyzer.add_single_feature_removal('EPU', pred_without_epu)")
    print("  analyzer.generate_feature_importance_report()")