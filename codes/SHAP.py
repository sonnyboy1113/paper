import os
import numpy as np
import pandas as pd
import matplotlib

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import shap
import pickle
from sklearn.inspection import permutation_importance
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 100)
print("SHAPå¯è§£é‡Šæ€§åˆ†æ - LSTM+GRU+XGBoostèåˆæ¨¡å‹".center(100))
print("=" * 100)

# ========== åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œæ•°æ® ==========
results_directory = "./Predict/"
shap_directory = "./SHAP_Analysis/"
if not os.path.exists(shap_directory):
    os.makedirs(shap_directory)

print("\nåŠ è½½æ¨¡å‹å’Œæ•°æ®...")

# åŠ è½½XGBoostæ¨¡å‹
with open(results_directory + 'xgb_gru_conservative.pkl', 'rb') as f:
    xgb_gru_model = pickle.load(f)

with open(results_directory + 'xgb_dual.pkl', 'rb') as f:
    xgb_dual_model = pickle.load(f)

with open(results_directory + 'ridge_meta_model.pkl', 'rb') as f:
    ridge_meta_model = pickle.load(f)

# åŠ è½½å½’ä¸€åŒ–å™¨
with open(results_directory + 'scalers.pkl', 'rb') as f:
    scalers_data = pickle.load(f)
    feature_scalers = scalers_data['feature_scalers']
    y_scaler = scalers_data['y_scaler']

print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

# ========== é‡æ–°å‡†å¤‡æ•°æ®ï¼ˆä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´ï¼‰==========
dataset = pd.read_csv('Corn-new.csv', parse_dates=['Date'], index_col=['Date'])

X = dataset.drop(columns=['Corn'], axis=1)
y = dataset['Corn']

split_idx = int(len(X) * 0.8)
X_train_raw, X_test_raw = X.iloc[:split_idx], X.iloc[split_idx:]
y_train_raw, y_test_raw = y.iloc[:split_idx], y.iloc[split_idx:]

# å½’ä¸€åŒ–
X_train = X_train_raw.copy()
X_test = X_test_raw.copy()

for col in X_train.columns:
    scaler = feature_scalers[col]
    X_train[col] = scaler.transform(X_train[col].values.reshape(-1, 1))
    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))

y_train = y_scaler.transform(y_train_raw.values.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.values.reshape(-1, 1)).flatten()

y_train = pd.Series(y_train, index=y_train_raw.index)
y_test = pd.Series(y_test, index=y_test_raw.index)


# æ·»åŠ æ»åç‰¹å¾
def add_features(X, y):
    X_new = X.copy()
    for i in range(1, 6):
        X_new[f'Corn_lag_{i}'] = y.shift(i)
    return X_new.dropna()


X_train_feat = add_features(X_train, y_train)
y_train = y_train.loc[X_train_feat.index]

X_test_feat = add_features(X_test, y_test)
y_test = y_test.loc[X_test_feat.index]


# æ„é€ åºåˆ—æ•°æ®
def create_flat_sequences(X, y, seq_len=5):
    X_flat, y_flat = [], []
    for i in range(len(X) - seq_len):
        X_flat.append(X.iloc[i:i + seq_len].values.flatten())
        y_flat.append(y.iloc[i + seq_len])
    return np.array(X_flat), np.array(y_flat)


seq_len = 5
X_train_flat, y_train_seq = create_flat_sequences(X_train_feat, y_train, seq_len)
X_test_flat, y_test_seq = create_flat_sequences(X_test_feat, y_test, seq_len)

print(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ: train={X_train_flat.shape}, test={X_test_flat.shape}")

# ========== ç”Ÿæˆç‰¹å¾åç§° ==========
base_feature_names = list(X_train_feat.columns)
feature_names_flat = []
for t in range(seq_len):
    for feat in base_feature_names:
        feature_names_flat.append(f'{feat}_t-{seq_len - t - 1}')

print(f"âœ“ ç‰¹å¾æ€»æ•°: {len(feature_names_flat)}")

# ========== ä¸ºSHAPåˆ†æå‡†å¤‡å¸¦é¢„æµ‹å€¼çš„å®Œæ•´ç‰¹å¾ ==========
# åŠ è½½ä¿å­˜çš„é¢„æµ‹ç»“æœ
predictions_df = pd.read_csv(results_directory + 'all_predictions.csv')

# ä»å½’ä¸€åŒ–æ•°æ®åæ¨LSTMå’ŒGRUçš„é¢„æµ‹ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
gru_test_pred_norm = y_scaler.transform(predictions_df['GRUå•æ¨¡å‹'].values.reshape(-1, 1)).flatten()
lstm_test_pred_norm = y_scaler.transform(predictions_df['LSTMå•æ¨¡å‹'].values.reshape(-1, 1)).flatten()


# æ„å»ºç®€åŒ–ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
def create_simplified_features_for_shap(X_flat, lstm_preds, gru_preds):
    """æ„å»ºç”¨äºXGBoostçš„å®Œæ•´ç‰¹å¾"""
    features_list = [X_flat]
    features_list.append(lstm_preds.reshape(-1, 1))
    features_list.append(gru_preds.reshape(-1, 1))
    features_list.append(((lstm_preds + gru_preds) / 2).reshape(-1, 1))
    features_list.append(np.abs(lstm_preds - gru_preds).reshape(-1, 1))
    return np.hstack(features_list)


X_test_simplified = create_simplified_features_for_shap(
    X_test_flat, lstm_test_pred_norm, gru_test_pred_norm
)

# ç”Ÿæˆæ‰©å±•ç‰¹å¾å
extended_feature_names = feature_names_flat + [
    'LSTM_Prediction',
    'GRU_Prediction',
    'Average_Prediction',
    'Prediction_Difference'
]

print(f"âœ“ æ‰©å±•ç‰¹å¾ç»´åº¦: {X_test_simplified.shape[1]}")

# ========== 1. XGBoostæ¨¡å‹çš„SHAPåˆ†æ ==========
print("\n" + "=" * 100)
print("ã€1ã€‘XGBoostæ¨¡å‹ - SHAP TreeExplaineråˆ†æ".center(100))
print("=" * 100)

# ä½¿ç”¨æµ‹è¯•é›†çš„å­é›†ï¼ˆSHAPè®¡ç®—è¾ƒæ…¢ï¼‰
sample_size = min(100, len(X_test_simplified))
X_sample = X_test_simplified[:sample_size]

print(f"\nè®¡ç®—SHAPå€¼ï¼ˆæ ·æœ¬æ•°: {sample_size}ï¼‰...")
explainer_xgb = shap.TreeExplainer(xgb_gru_model)
shap_values_xgb = explainer_xgb.shap_values(X_sample)

print("âœ“ SHAPå€¼è®¡ç®—å®Œæˆ")

# 1.1 SHAP Summary Plotï¼ˆç‰¹å¾é‡è¦æ€§æ¦‚è§ˆï¼‰
plt.figure(figsize=(14, 10))
shap.summary_plot(
    shap_values_xgb,
    X_sample,
    feature_names=extended_feature_names,
    show=False,
    max_display=20
)
plt.title('XGBoostæ¨¡å‹ - SHAPç‰¹å¾é‡è¦æ€§æ¦‚è§ˆï¼ˆTop 20ï¼‰',
          fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig(shap_directory + '01_xgb_shap_summary.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾1: SHAP Summary Plot")

# 1.2 SHAP Bar Plotï¼ˆå¹³å‡ç»å¯¹SHAPå€¼ï¼‰
plt.figure(figsize=(12, 10))
shap.summary_plot(
    shap_values_xgb,
    X_sample,
    feature_names=extended_feature_names,
    plot_type='bar',
    show=False,
    max_display=20
)
plt.title('XGBoostæ¨¡å‹ - å¹³å‡ç‰¹å¾å½±å“åŠ›ï¼ˆTop 20ï¼‰',
          fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '02_xgb_shap_bar.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾2: SHAP Bar Plot")

# 1.3 å•ä¸ªæ ·æœ¬çš„SHAP Force Plot
sample_idx = 0
plt.figure(figsize=(16, 3))
shap.force_plot(
    explainer_xgb.expected_value,
    shap_values_xgb[sample_idx],
    np.round(X_sample[sample_idx], 3),
    feature_names=extended_feature_names,
    matplotlib=True,
    show=False
)
plt.title(f'å•æ ·æœ¬SHAPè§£é‡Šï¼ˆæ ·æœ¬#{sample_idx}ï¼‰', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '03_xgb_force_plot_sample.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾3: Force Plotï¼ˆå•æ ·æœ¬ï¼‰")

# 1.4 SHAP Dependence Plotï¼ˆå…³é”®ç‰¹å¾ï¼‰
# æ‰¾å‡ºæœ€é‡è¦çš„å‡ ä¸ªç‰¹å¾
mean_abs_shap = np.abs(shap_values_xgb).mean(axis=0)
top_feature_indices = np.argsort(mean_abs_shap)[-6:][::-1]

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, feat_idx in enumerate(top_feature_indices):
    ax = axes[i]
    shap.dependence_plot(
        feat_idx,
        shap_values_xgb,
        X_sample,
        feature_names=extended_feature_names,
        ax=ax,
        show=False
    )
    ax.set_title(f'{extended_feature_names[feat_idx]}', fontsize=10, fontweight='bold')

plt.suptitle('XGBoostæ¨¡å‹ - Top 6ç‰¹å¾çš„SHAPä¾èµ–å›¾', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '04_xgb_dependence_plots.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾4: SHAP Dependence Plots")

# 1.5 SHAP Waterfall Plotï¼ˆè¯¦ç»†åˆ†è§£å•ä¸ªé¢„æµ‹ï¼‰
plt.figure(figsize=(10, 12))
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values_xgb[sample_idx],
        base_values=explainer_xgb.expected_value,
        data=X_sample[sample_idx],
        feature_names=extended_feature_names
    ),
    max_display=20,
    show=False
)
plt.title(f'SHAP Waterfall Plot - æ ·æœ¬#{sample_idx}é¢„æµ‹åˆ†è§£', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '05_xgb_waterfall.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾5: SHAP Waterfall Plot")

# ========== 2. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”åˆ†æ ==========
print("\n" + "=" * 100)
print("ã€2ã€‘å¤šè§’åº¦ç‰¹å¾é‡è¦æ€§å¯¹æ¯”".center(100))
print("=" * 100)

# 2.1 XGBoostå†…ç½®ç‰¹å¾é‡è¦æ€§
xgb_importance = xgb_gru_model.get_booster().get_score(importance_type='gain')
xgb_importance_df = pd.DataFrame([
    {'feature': extended_feature_names[int(k.replace('f', ''))], 'importance': v}
    for k, v in xgb_importance.items()
]).sort_values('importance', ascending=False).head(20)

# 2.2 SHAPç‰¹å¾é‡è¦æ€§
shap_importance_df = pd.DataFrame({
    'feature': extended_feature_names,
    'importance': np.abs(shap_values_xgb).mean(axis=0)
}).sort_values('importance', ascending=False).head(20)

# 2.3 æ’åˆ—é‡è¦æ€§ï¼ˆPermutation Importanceï¼‰
print("\nè®¡ç®—æ’åˆ—é‡è¦æ€§...")
perm_importance = permutation_importance(
    xgb_gru_model,
    X_test_simplified,
    y_test_seq,
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)
perm_importance_df = pd.DataFrame({
    'feature': extended_feature_names,
    'importance': perm_importance.importances_mean
}).sort_values('importance', ascending=False).head(20)
print("âœ“ æ’åˆ—é‡è¦æ€§è®¡ç®—å®Œæˆ")

# ç»˜åˆ¶å¯¹æ¯”å›¾
fig, axes = plt.subplots(1, 3, figsize=(20, 8))

# XGBoost Gain
axes[0].barh(range(len(xgb_importance_df)), xgb_importance_df['importance'].values,
             color='steelblue', alpha=0.7)
axes[0].set_yticks(range(len(xgb_importance_df)))
axes[0].set_yticklabels(xgb_importance_df['feature'].values, fontsize=8)
axes[0].set_xlabel('Importance (Gain)', fontsize=10)
axes[0].set_title('XGBoostå†…ç½®ç‰¹å¾é‡è¦æ€§\n(Gain)', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(True, alpha=0.3, axis='x')

# SHAP Importance
axes[1].barh(range(len(shap_importance_df)), shap_importance_df['importance'].values,
             color='coral', alpha=0.7)
axes[1].set_yticks(range(len(shap_importance_df)))
axes[1].set_yticklabels(shap_importance_df['feature'].values, fontsize=8)
axes[1].set_xlabel('Mean |SHAP value|', fontsize=10)
axes[1].set_title('SHAPç‰¹å¾é‡è¦æ€§\n(å¹³å‡ç»å¯¹å€¼)', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()
axes[1].grid(True, alpha=0.3, axis='x')

# Permutation Importance
axes[2].barh(range(len(perm_importance_df)), perm_importance_df['importance'].values,
             color='seagreen', alpha=0.7)
axes[2].set_yticks(range(len(perm_importance_df)))
axes[2].set_yticklabels(perm_importance_df['feature'].values, fontsize=8)
axes[2].set_xlabel('Permutation Importance', fontsize=10)
axes[2].set_title('æ’åˆ—ç‰¹å¾é‡è¦æ€§\n(æ¨¡å‹æ— å…³)', fontsize=12, fontweight='bold')
axes[2].invert_yaxis()
axes[2].grid(True, alpha=0.3, axis='x')

plt.suptitle('ä¸‰ç§æ–¹æ³•çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆTop 20ï¼‰', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '06_importance_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾6: ç‰¹å¾é‡è¦æ€§å¯¹æ¯”")

# ========== 3. ç‰¹å¾ç±»åˆ«åˆ†æ ==========
print("\n" + "=" * 100)
print("ã€3ã€‘ç‰¹å¾ç±»åˆ«å½±å“åŠ›åˆ†æ".center(100))
print("=" * 100)


# å°†ç‰¹å¾åˆ†ç±»
def categorize_features(feature_names):
    categories = {
        'LSTM/GRUé¢„æµ‹': [],
        'æ»åç‰¹å¾(Corn_lag)': [],
        'åŸå§‹ç‰¹å¾': []
    }

    for i, name in enumerate(feature_names):
        if any(pred in name for pred in ['LSTM', 'GRU', 'Average', 'Difference']):
            categories['LSTM/GRUé¢„æµ‹'].append(i)
        elif 'Corn_lag' in name:
            categories['æ»åç‰¹å¾(Corn_lag)'].append(i)
        else:
            categories['åŸå§‹ç‰¹å¾'].append(i)

    return categories


feature_categories = categorize_features(extended_feature_names)

# è®¡ç®—æ¯ç±»ç‰¹å¾çš„æ€»SHAPè´¡çŒ®
category_importance = {}
for cat_name, indices in feature_categories.items():
    if len(indices) > 0:
        cat_shap = np.abs(shap_values_xgb[:, indices]).sum(axis=1).mean()
        category_importance[cat_name] = cat_shap

# ç»˜åˆ¶ç±»åˆ«é‡è¦æ€§
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# é¥¼å›¾
colors = ['#ff9999', '#66b3ff', '#99ff99']
ax1.pie(category_importance.values(), labels=category_importance.keys(),
        autopct='%1.1f%%', startangle=90, colors=colors, textprops={'fontsize': 11})
ax1.set_title('ç‰¹å¾ç±»åˆ«å¯¹æ¨¡å‹çš„å¹³å‡è´¡çŒ®å æ¯”', fontsize=13, fontweight='bold')

# æŸ±çŠ¶å›¾
ax2.bar(category_importance.keys(), category_importance.values(),
        color=colors, alpha=0.7, edgecolor='black')
ax2.set_ylabel('Mean |SHAP value|', fontsize=11)
ax2.set_title('ç‰¹å¾ç±»åˆ«çš„SHAPé‡è¦æ€§', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')
for i, (k, v) in enumerate(category_importance.items()):
    ax2.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig(shap_directory + '07_category_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾7: ç‰¹å¾ç±»åˆ«åˆ†æ")

# ========== 4. æ—¶é—´ç»´åº¦åˆ†æ ==========
print("\n" + "=" * 100)
print("ã€4ã€‘æ—¶é—´æ»åç»´åº¦çš„å½±å“åˆ†æ".center(100))
print("=" * 100)

# æŒ‰æ—¶é—´æ­¥èšåˆSHAPå€¼
timestep_importance = {}
for t in range(seq_len):
    timestep_features = [i for i, name in enumerate(extended_feature_names)
                         if f't-{t}' in name]
    if len(timestep_features) > 0:
        timestep_importance[f't-{t}'] = np.abs(shap_values_xgb[:, timestep_features]).mean()

# ç»˜åˆ¶æ—¶é—´ç»´åº¦é‡è¦æ€§
fig, ax = plt.subplots(figsize=(12, 6))

timesteps = list(timestep_importance.keys())
importances = list(timestep_importance.values())

bars = ax.bar(timesteps, importances, color=plt.cm.viridis(np.linspace(0.3, 0.9, len(timesteps))),
              alpha=0.8, edgecolor='black')

ax.set_xlabel('æ—¶é—´æ­¥ï¼ˆt-0ä¸ºæœ€è¿‘ï¼‰', fontsize=12)
ax.set_ylabel('å¹³å‡ |SHAP value|', fontsize=12)
ax.set_title('ä¸åŒæ—¶é—´æ»åæœŸçš„ç‰¹å¾é‡è¦æ€§', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')

for bar, imp in zip(bars, importances):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2., height,
            f'{imp:.5f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

plt.tight_layout()
plt.savefig(shap_directory + '08_timestep_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾8: æ—¶é—´ç»´åº¦åˆ†æ")

# ========== 5. é¢„æµ‹æ¡ˆä¾‹æ·±åº¦è§£æ ==========
print("\n" + "=" * 100)
print("ã€5ã€‘å…¸å‹é¢„æµ‹æ¡ˆä¾‹çš„æ·±åº¦è§£æ".center(100))
print("=" * 100)

# é€‰æ‹©å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„æ ·æœ¬
y_pred_sample = xgb_gru_model.predict(X_sample)
errors = np.abs(y_pred_sample - y_test_seq[:sample_size])

# æœ€å¥½ã€æœ€å·®ã€ä¸­ç­‰é¢„æµ‹
best_idx = np.argmin(errors)
worst_idx = np.argmax(errors)
median_idx = np.argsort(errors)[len(errors) // 2]

case_indices = {
    'æœ€ä½³é¢„æµ‹': best_idx,
    'æœ€å·®é¢„æµ‹': worst_idx,
    'ä¸­ç­‰é¢„æµ‹': median_idx
}

fig, axes = plt.subplots(len(case_indices), 1, figsize=(14, 12))

for i, (case_name, idx) in enumerate(case_indices.items()):
    ax = axes[i]

    # è·å–è¯¥æ ·æœ¬çš„SHAPå€¼å’Œç‰¹å¾å€¼
    sample_shap = shap_values_xgb[idx]
    sample_features = X_sample[idx]

    # æ‰¾å‡ºæœ€é‡è¦çš„10ä¸ªç‰¹å¾
    top_k = 10
    top_indices = np.argsort(np.abs(sample_shap))[-top_k:][::-1]

    # ç»˜åˆ¶
    y_pos = np.arange(top_k)
    colors_bar = ['green' if sample_shap[j] > 0 else 'red' for j in top_indices]

    ax.barh(y_pos, [sample_shap[j] for j in top_indices], color=colors_bar, alpha=0.7)
    ax.set_yticks(y_pos)
    ax.set_yticklabels([extended_feature_names[j] for j in top_indices], fontsize=9)
    ax.set_xlabel('SHAP value', fontsize=10)

    true_val = y_test_seq[idx]
    pred_val = y_pred_sample[idx]
    error_val = errors[idx]

    ax.set_title(f'{case_name}ï¼ˆæ ·æœ¬#{idx}ï¼‰\nçœŸå®å€¼={true_val:.4f}, é¢„æµ‹å€¼={pred_val:.4f}, è¯¯å·®={error_val:.4f}',
                 fontsize=11, fontweight='bold')
    ax.axvline(0, color='black', linestyle='--', linewidth=1)
    ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(shap_directory + '09_case_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾9: å…¸å‹æ¡ˆä¾‹åˆ†æ")

# ========== 6. SHAPäº¤äº’æ•ˆåº”åˆ†æ ==========
print("\n" + "=" * 100)
print("ã€6ã€‘ç‰¹å¾äº¤äº’æ•ˆåº”åˆ†æï¼ˆSHAP Interactionï¼‰".center(100))
print("=" * 100)

# ç”±äºè®¡ç®—é‡å¤§ï¼Œä½¿ç”¨æ›´å°çš„æ ·æœ¬
interaction_sample_size = min(50, sample_size)
X_interaction = X_sample[:interaction_sample_size]

print(f"è®¡ç®—SHAPäº¤äº’å€¼ï¼ˆæ ·æœ¬æ•°: {interaction_sample_size}ï¼‰...")
shap_interaction_values = explainer_xgb.shap_interaction_values(X_interaction)
print("âœ“ äº¤äº’å€¼è®¡ç®—å®Œæˆ")

# é€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªç‰¹å¾è¿›è¡Œäº¤äº’åˆ†æ
top_n_features = 6
top_features_idx = np.argsort(mean_abs_shap)[-top_n_features:][::-1]

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, main_feat_idx in enumerate(top_features_idx):
    ax = axes[i]

    # å¯¹äºæ¯ä¸ªä¸»ç‰¹å¾ï¼Œæ‰¾å‡ºä¸å®ƒäº¤äº’æœ€å¼ºçš„ç‰¹å¾
    interaction_strength = np.abs(shap_interaction_values[:, main_feat_idx, :]).mean(axis=0)
    interaction_strength[main_feat_idx] = 0  # æ’é™¤è‡ªèº«
    interact_feat_idx = np.argmax(interaction_strength)

    # ç»˜åˆ¶äº¤äº’ä¾èµ–å›¾
    shap.dependence_plot(
        (main_feat_idx, interact_feat_idx),
        shap_interaction_values,
        X_interaction,
        feature_names=extended_feature_names,
        ax=ax,
        show=False
    )
    ax.set_title(f'{extended_feature_names[main_feat_idx][:30]}', fontsize=9, fontweight='bold')

plt.suptitle('Top 6ç‰¹å¾çš„SHAPäº¤äº’æ•ˆåº”åˆ†æ', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(shap_directory + '10_interaction_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾10: ç‰¹å¾äº¤äº’åˆ†æ")

# ========== åœ¨ç¬¬7éƒ¨åˆ†ä¹‹å‰æ·»åŠ è¿™æ®µä»£ç  ==========
# ä½ç½®ï¼šåœ¨ "ã€7ã€‘ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š" ä¹‹å‰

# ç”Ÿæˆç±»åˆ«è´¡çŒ®æŠ¥å‘Š
category_report = pd.DataFrame([
    {'Category': cat, 'Feature_Count': len(indices),
     'Total_SHAP_Contribution': category_importance[cat],
     'Avg_SHAP_Per_Feature': category_importance[cat] / len(indices) if len(indices) > 0 else 0}
    for cat, indices in feature_categories.items() if len(indices) > 0
]).sort_values('Total_SHAP_Contribution', ascending=False)
category_report.to_csv(shap_directory + 'category_contribution_report.csv', index=False)
print("âœ“ ç±»åˆ«è´¡çŒ®æŠ¥å‘Šå·²ä¿å­˜")

# æ¡ˆä¾‹åˆ†ææŠ¥å‘Š
case_report = []
for case_name, idx in case_indices.items():
    top_features_idx = np.argsort(np.abs(shap_values_xgb[idx]))[-5:][::-1]
    case_report.append({
        'Case': case_name,
        'Sample_Index': idx,
        'True_Value': y_test_seq[idx],
        'Predicted_Value': y_pred_sample[idx],
        'Error': errors[idx],
        'Top_Feature_1': extended_feature_names[top_features_idx[0]],
        'Top_Feature_1_SHAP': shap_values_xgb[idx, top_features_idx[0]],
        'Top_Feature_2': extended_feature_names[top_features_idx[1]],
        'Top_Feature_2_SHAP': shap_values_xgb[idx, top_features_idx[1]],
        'Top_Feature_3': extended_feature_names[top_features_idx[2]],
        'Top_Feature_3_SHAP': shap_values_xgb[idx, top_features_idx[2]]
    })

case_df = pd.DataFrame(case_report)
case_df.to_csv(shap_directory + 'case_analysis_report.csv', index=False)
print("âœ“ æ¡ˆä¾‹åˆ†ææŠ¥å‘Šå·²ä¿å­˜")

# ========== ç„¶åæ‰æ˜¯ç¬¬7éƒ¨åˆ† ==========
print("\n" + "=" * 100)
print("ã€7ã€‘ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š".center(100))
print("=" * 100)

# ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ°CSV
importance_report = pd.DataFrame({
    'Feature': extended_feature_names,
    'SHAP_Importance': np.abs(shap_values_xgb).mean(axis=0),
    'XGB_Gain': [xgb_gru_model.get_booster().get_score(importance_type='gain').get(f'f{i}', 0)
                 for i in range(len(extended_feature_names))],
    'Permutation_Importance': perm_importance.importances_mean
})
importance_report = importance_report.sort_values('SHAP_Importance', ascending=False)
importance_report.to_csv(shap_directory + 'feature_importance_report.csv', index=False)
print("âœ“ ç‰¹å¾é‡è¦æ€§æŠ¥å‘Šå·²ä¿å­˜")

# ç±»åˆ«è´¡çŒ®ï¼ˆç°åœ¨å¯ä»¥ä½¿ç”¨ category_report äº†ï¼‰
print(f"\n  ã€ç‰¹å¾ç±»åˆ«è´¡çŒ®æ’åã€‘:")
for i, row in category_report.iterrows():
    print(f"    {i + 1}. {row['Category']}")
    print(f"       ç‰¹å¾æ•°é‡: {row['Feature_Count']}, æ€»è´¡çŒ®: {row['Total_SHAP_Contribution']:.6f}")
    print(f"       å¹³å‡è´¡çŒ®: {row['Avg_SHAP_Per_Feature']:.6f}")

# æ—¶é—´æ­¥é‡è¦æ€§
print(f"\n  ã€æ—¶é—´æ»åæœŸå½±å“åŠ›ã€‘:")
sorted_timesteps = sorted(timestep_importance.items(), key=lambda x: x[1], reverse=True)
for ts, imp in sorted_timesteps:
    print(f"    {ts}: {imp:.6f}")

print(f"\nğŸ’¡ è§£é‡Šæ€§æ´å¯Ÿ:")
print(f"  1. LSTM/GRUé¢„æµ‹å€¼ç‰¹å¾æ˜¾è‘—å½±å“æ®‹å·®å­¦ä¹ æ•ˆæœ")
print(f"  2. è¿‘æœŸæ—¶é—´æ­¥ï¼ˆt-0, t-1ï¼‰æ¯”è¿œæœŸæ—¶é—´æ­¥æ›´é‡è¦")
print(f"  3. ç‰ç±³æ»åç‰¹å¾ï¼ˆCorn_lagï¼‰å¯¹é¢„æµ‹æœ‰å…³é”®ä½œç”¨")
print(f"  4. ç‰¹å¾é—´å­˜åœ¨äº¤äº’æ•ˆåº”ï¼Œéœ€è¦éçº¿æ€§æ¨¡å‹æ•æ‰")


print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"  å›¾è¡¨:")
print(f"    01_xgb_shap_summary.png - SHAPç‰¹å¾é‡è¦æ€§æ¦‚è§ˆ")
print(f"    02_xgb_shap_bar.png - å¹³å‡ç‰¹å¾å½±å“åŠ›")
print(f"    03_xgb_force_plot_sample.png - å•æ ·æœ¬Force Plot")
print(f"    04_xgb_dependence_plots.png - Top 6ç‰¹å¾ä¾èµ–å›¾")
print(f"    05_xgb_waterfall.png - Waterfallé¢„æµ‹åˆ†è§£")
print(f"    06_importance_comparison.png - ä¸‰ç§æ–¹æ³•å¯¹æ¯”")
print(f"    07_category_importance.png - ç‰¹å¾ç±»åˆ«åˆ†æ")
print(f"    08_timestep_importance.png - æ—¶é—´ç»´åº¦åˆ†æ")
print(f"    09_case_analysis.png - å…¸å‹æ¡ˆä¾‹è§£æ")
print(f"    10_interaction_analysis.png - ç‰¹å¾äº¤äº’åˆ†æ")

print(f"\n  æŠ¥å‘Š:")
print(f"    feature_importance_report.csv - å®Œæ•´ç‰¹å¾é‡è¦æ€§")
print(f"    category_contribution_report.csv - ç±»åˆ«è´¡çŒ®åˆ†æ")
print(f"    case_analysis_report.csv - æ¡ˆä¾‹æ·±åº¦åˆ†æ")

print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {shap_directory}")

# ========== é¢å¤–ï¼šç”ŸæˆHTMLäº¤äº’å¼æŠ¥å‘Š ==========
print("\n" + "=" * 100)
print("ã€8ã€‘ç”Ÿæˆäº¤äº’å¼HTMLæŠ¥å‘Š".center(100))
print("=" * 100)

try:
    # SHAP Force Plot HTML
    shap.force_plot(
        explainer_xgb.expected_value,
        shap_values_xgb[:50],  # å‰50ä¸ªæ ·æœ¬
        X_sample[:50],
        feature_names=extended_feature_names,
        show=False
    )
    shap.save_html(shap_directory + 'shap_force_plot_interactive.html',
                   shap.force_plot(
                       explainer_xgb.expected_value,
                       shap_values_xgb[:50],
                       X_sample[:50],
                       feature_names=extended_feature_names
                   ))
    print("âœ“ äº¤äº’å¼Force Plotå·²ç”Ÿæˆ: shap_force_plot_interactive.html")
except Exception as e:
    print(f"âš ï¸  HTMLç”Ÿæˆå¤±è´¥: {e}")

# ========== æ¨¡å‹å†³ç­–è¾¹ç•Œå¯è§†åŒ– ==========
print("\n" + "=" * 100)
print("ã€9ã€‘æ¨¡å‹å†³ç­–è¡Œä¸ºåˆ†æ".center(100))
print("=" * 100)

# åˆ†æé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å…³ç³»
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 9.1 é¢„æµ‹å€¼ vs çœŸå®å€¼
ax = axes[0, 0]
ax.scatter(y_test_seq[:sample_size], y_pred_sample, alpha=0.6, s=50)
ax.plot([y_test_seq[:sample_size].min(), y_test_seq[:sample_size].max()],
        [y_test_seq[:sample_size].min(), y_test_seq[:sample_size].max()],
        'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
ax.set_xlabel('çœŸå®å€¼', fontsize=11)
ax.set_ylabel('é¢„æµ‹å€¼', fontsize=11)
ax.set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# 9.2 SHAPå€¼ä¸é¢„æµ‹è¯¯å·®çš„å…³ç³»
ax = axes[0, 1]
total_shap_impact = np.abs(shap_values_xgb).sum(axis=1)
ax.scatter(total_shap_impact, errors, alpha=0.6, s=50, c=errors, cmap='RdYlGn_r')
ax.set_xlabel('æ€»SHAPå½±å“åŠ›ï¼ˆç»å¯¹å€¼ä¹‹å’Œï¼‰', fontsize=11)
ax.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=11)
ax.set_title('SHAPæ€»å½±å“ vs é¢„æµ‹è¯¯å·®', fontsize=12, fontweight='bold')
ax.grid(True, alpha=0.3)

# 9.3 é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
ax = axes[1, 0]
# ä½¿ç”¨SHAPå€¼çš„æ ‡å‡†å·®ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
shap_std = np.std(shap_values_xgb, axis=1)
ax.scatter(shap_std, errors, alpha=0.6, s=50, c=errors, cmap='RdYlGn_r')
ax.set_xlabel('SHAPå€¼æ ‡å‡†å·®ï¼ˆä¸ç¡®å®šæ€§ï¼‰', fontsize=11)
ax.set_ylabel('é¢„æµ‹è¯¯å·®', fontsize=11)
ax.set_title('æ¨¡å‹ä¸ç¡®å®šæ€§ vs é¢„æµ‹è¯¯å·®', fontsize=12, fontweight='bold')
ax.grid(True, alpha=0.3)

# 9.4 è¯¯å·®åˆ†å¸ƒ
ax = axes[1, 1]
ax.hist(errors, bins=30, alpha=0.7, edgecolor='black', color='steelblue')
ax.axvline(np.mean(errors), color='red', linestyle='--', linewidth=2,
           label=f'å‡å€¼={np.mean(errors):.6f}')
ax.axvline(np.median(errors), color='green', linestyle='--', linewidth=2,
           label=f'ä¸­ä½æ•°={np.median(errors):.6f}')
ax.set_xlabel('é¢„æµ‹è¯¯å·®', fontsize=11)
ax.set_ylabel('é¢‘æ•°', fontsize=11)
ax.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(shap_directory + '11_model_behavior_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾11: æ¨¡å‹å†³ç­–è¡Œä¸ºåˆ†æ")

# ========== ç‰¹å¾è´¡çŒ®çƒ­åŠ›å›¾ ==========
print("\n" + "=" * 100)
print("ã€10ã€‘ç‰¹å¾è´¡çŒ®çƒ­åŠ›å›¾".center(100))
print("=" * 100)

# é€‰æ‹©å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
top_20_indices = np.argsort(mean_abs_shap)[-20:][::-1]
top_20_names = [extended_feature_names[i] for i in top_20_indices]

# é€‰æ‹©30ä¸ªæ ·æœ¬ç”¨äºçƒ­åŠ›å›¾
heatmap_samples = min(30, sample_size)
shap_heatmap_data = shap_values_xgb[:heatmap_samples, top_20_indices]

fig, ax = plt.subplots(figsize=(14, 10))
im = ax.imshow(shap_heatmap_data.T, aspect='auto', cmap='RdBu_r',
               vmin=-np.abs(shap_heatmap_data).max(),
               vmax=np.abs(shap_heatmap_data).max())

ax.set_xticks(np.arange(heatmap_samples))
ax.set_yticks(np.arange(len(top_20_names)))
ax.set_xticklabels(np.arange(heatmap_samples), fontsize=8)
ax.set_yticklabels([name[:40] for name in top_20_names], fontsize=8)

ax.set_xlabel('æ ·æœ¬ç´¢å¼•', fontsize=11)
ax.set_ylabel('ç‰¹å¾', fontsize=11)
ax.set_title('Top 20ç‰¹å¾çš„SHAPå€¼çƒ­åŠ›å›¾\nï¼ˆçº¢è‰²=æ­£å‘å½±å“ï¼Œè“è‰²=è´Ÿå‘å½±å“ï¼‰',
             fontsize=13, fontweight='bold')

cbar = plt.colorbar(im, ax=ax)
cbar.set_label('SHAP value', fontsize=10)

plt.tight_layout()
plt.savefig(shap_directory + '12_shap_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ å›¾12: SHAPå€¼çƒ­åŠ›å›¾")

# ========== ç”ŸæˆMarkdownæŠ¥å‘Š ==========
print("\nç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š...")

markdown_report = f"""# LSTM+GRU+XGBoostèåˆæ¨¡å‹ - SHAPå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1. æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šä½¿ç”¨SHAPï¼ˆSHapley Additive exPlanationsï¼‰æ–¹æ³•å¯¹æ—¶é—´åºåˆ—é¢„æµ‹èåˆæ¨¡å‹è¿›è¡Œå…¨é¢çš„å¯è§£é‡Šæ€§åˆ†æã€‚

### å…³é”®æŒ‡æ ‡
- **åˆ†ææ ·æœ¬æ•°**: {sample_size}
- **ç‰¹å¾æ€»æ•°**: {len(extended_feature_names)}
- **ç”Ÿæˆå¯è§†åŒ–**: 12å¼ 
- **ç”Ÿæˆæ•°æ®æŠ¥å‘Š**: 3ä»½

---

## 2. Top 10 æœ€é‡è¦ç‰¹å¾

| æ’å | ç‰¹å¾åç§° | SHAPé‡è¦æ€§ | XGBoost Gain | æ’åˆ—é‡è¦æ€§ |
|------|---------|-----------|--------------|-----------|
"""

for idx, row in importance_report.head(10).iterrows():
    markdown_report += f"| {idx + 1} | {row['Feature']} | {row['SHAP_Importance']:.6f} | {row['XGB_Gain']:.6f} | {row['Permutation_Importance']:.6f} |\n"

markdown_report += f"""

---

## 3. ç‰¹å¾ç±»åˆ«è´¡çŒ®åˆ†æ

"""

for idx, row in category_report.iterrows():
    pct = (row['Total_SHAP_Contribution'] / sum(category_report['Total_SHAP_Contribution'])) * 100
    markdown_report += f"""### {row['Category']}
- **ç‰¹å¾æ•°é‡**: {row['Feature_Count']}
- **æ€»è´¡çŒ®åº¦**: {row['Total_SHAP_Contribution']:.6f} ({pct:.1f}%)
- **å¹³å‡è´¡çŒ®**: {row['Avg_SHAP_Per_Feature']:.6f}

"""

markdown_report += f"""---

## 4. æ—¶é—´æ»ååˆ†æ

ä¸åŒæ—¶é—´æ­¥çš„ç‰¹å¾é‡è¦æ€§ï¼š

"""

for ts, imp in sorted_timesteps:
    markdown_report += f"- **{ts}**: {imp:.6f}\n"

markdown_report += f"""

**å…³é”®å‘ç°**: è¿‘æœŸæ—¶é—´æ­¥ï¼ˆt-0, t-1ï¼‰çš„ç‰¹å¾å¯¹é¢„æµ‹å½±å“æœ€å¤§ï¼Œè¡¨æ˜æ¨¡å‹æ›´å…³æ³¨æœ€è¿‘çš„å†å²ä¿¡æ¯ã€‚

---

## 5. å…¸å‹æ¡ˆä¾‹åˆ†æ

"""

for idx, row in case_df.iterrows():
    markdown_report += f"""### {row['Case']}

- **æ ·æœ¬ç´¢å¼•**: {row['Sample_Index']}
- **çœŸå®å€¼**: {row['True_Value']:.6f}
- **é¢„æµ‹å€¼**: {row['Predicted_Value']:.6f}
- **é¢„æµ‹è¯¯å·®**: {row['Error']:.6f}

**æœ€é‡è¦çš„3ä¸ªç‰¹å¾**:
1. {row['Top_Feature_1']} (SHAP: {row['Top_Feature_1_SHAP']:.6f})
2. {row['Top_Feature_2']} (SHAP: {row['Top_Feature_2_SHAP']:.6f})
3. {row['Top_Feature_3']} (SHAP: {row['Top_Feature_3_SHAP']:.6f})

"""

markdown_report += f"""---

## 6. å…³é”®æ´å¯Ÿä¸å»ºè®®

### æ¨¡å‹è¡Œä¸ºç†è§£
1. **LSTM/GRUé¢„æµ‹ç‰¹å¾è‡³å…³é‡è¦**: æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹å€¼æœ¬èº«æ˜¯XGBoostæ®‹å·®å­¦ä¹ çš„é‡è¦è¾“å…¥
2. **æ—¶é—´ä¾èµ–æ€§**: æœ€è¿‘çš„æ—¶é—´æ­¥ä¿¡æ¯æƒé‡æœ€é«˜ï¼Œç¬¦åˆæ—¶é—´åºåˆ—çš„ç›´è§‰
3. **æ»åç‰¹å¾çš„ä½œç”¨**: ç‰ç±³ä»·æ ¼çš„å†å²æ»åå€¼å¯¹é¢„æµ‹æœ‰æ˜¾è‘—è´¡çŒ®

### ç‰¹å¾å·¥ç¨‹å»ºè®®
1. ä¿ç•™å¹¶ä¼˜åŒ–LSTM/GRUé¢„æµ‹ç‰¹å¾
2. å¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤šè¿‘æœŸæ—¶é—´æ­¥çš„ç‰¹å¾
3. æ¢ç´¢éçº¿æ€§ç‰¹å¾äº¤äº’ï¼ˆå·²é€šè¿‡XGBoostæ•æ‰ï¼‰

### æ¨¡å‹ä¼˜åŒ–æ–¹å‘
1. é’ˆå¯¹é¢„æµ‹è¯¯å·®è¾ƒå¤§çš„æ ·æœ¬ï¼Œåˆ†æå…¶SHAPæ¨¡å¼æ‰¾å‡ºè–„å¼±ç¯èŠ‚
2. è€ƒè™‘å¯¹é«˜ä¸ç¡®å®šæ€§æ ·æœ¬è¿›è¡Œé›†æˆæˆ–åŠ æƒå¤„ç†
3. æŒç»­ç›‘æ§ç‰¹å¾é‡è¦æ€§å˜åŒ–ï¼ŒåŠæ—¶è°ƒæ•´ç‰¹å¾é›†

---

## 7. é™„å½•ï¼šç”Ÿæˆæ–‡ä»¶æ¸…å•

### å¯è§†åŒ–å›¾è¡¨
- `01_xgb_shap_summary.png` - SHAPç‰¹å¾é‡è¦æ€§æ¦‚è§ˆ
- `02_xgb_shap_bar.png` - å¹³å‡ç‰¹å¾å½±å“åŠ›
- `03_xgb_force_plot_sample.png` - å•æ ·æœ¬Force Plot
- `04_xgb_dependence_plots.png` - Top 6ç‰¹å¾ä¾èµ–å›¾
- `05_xgb_waterfall.png` - Waterfallé¢„æµ‹åˆ†è§£
- `06_importance_comparison.png` - ä¸‰ç§æ–¹æ³•å¯¹æ¯”
- `07_category_importance.png` - ç‰¹å¾ç±»åˆ«åˆ†æ
- `08_timestep_importance.png` - æ—¶é—´ç»´åº¦åˆ†æ
- `09_case_analysis.png` - å…¸å‹æ¡ˆä¾‹è§£æ
- `10_interaction_analysis.png` - ç‰¹å¾äº¤äº’åˆ†æ
- `11_model_behavior_analysis.png` - æ¨¡å‹å†³ç­–è¡Œä¸º
- `12_shap_heatmap.png` - SHAPå€¼çƒ­åŠ›å›¾

### æ•°æ®æŠ¥å‘Š
- `feature_importance_report.csv` - å®Œæ•´ç‰¹å¾é‡è¦æ€§
- `category_contribution_report.csv` - ç±»åˆ«è´¡çŒ®åˆ†æ
- `case_analysis_report.csv` - æ¡ˆä¾‹æ·±åº¦åˆ†æ

### äº¤äº’å¼æ–‡ä»¶
- `shap_force_plot_interactive.html` - äº¤äº’å¼SHAPè§£é‡Š

---

**æŠ¥å‘Šç»“æŸ**
"""

# ä¿å­˜MarkdownæŠ¥å‘Š
with open(shap_directory + 'SHAP_Analysis_Report.md', 'w', encoding='utf-8') as f:
    f.write(markdown_report)

print("âœ“ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: SHAP_Analysis_Report.md")

print("\n" + "=" * 100)
print("âœ… å®Œæ•´çš„SHAPå¯è§£é‡Šæ€§åˆ†æå·²å®Œæˆï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š åˆ†ææˆæœæ±‡æ€»:")
print(f"  â€¢ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨: 12å¼ ")
print(f"  â€¢ ç”Ÿæˆæ•°æ®æŠ¥å‘Š: 3ä»½CSV")
print(f"  â€¢ ç”ŸæˆMarkdownæŠ¥å‘Š: 1ä»½")
print(f"  â€¢ ç”Ÿæˆäº¤äº’å¼HTML: 1ä»½")

print(f"\nğŸ¯ æ ¸å¿ƒä»·å€¼:")
print(f"  1. æ­ç¤ºäº†æ¨¡å‹é¢„æµ‹çš„å†…åœ¨æœºåˆ¶")
print(f"  2. è¯†åˆ«äº†æœ€é‡è¦çš„é¢„æµ‹ç‰¹å¾")
print(f"  3. åˆ†æäº†ç‰¹å¾é—´çš„äº¤äº’æ•ˆåº”")
print(f"  4. æä¾›äº†æ¨¡å‹ä¼˜åŒ–çš„å…·ä½“æ–¹å‘")

print(f"\nğŸ“‚ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {shap_directory}")
print(f"\nğŸ’¡ å»ºè®®: æŸ¥çœ‹ SHAP_Analysis_Report.md è·å–å®Œæ•´åˆ†ææŠ¥å‘Š")

print("\n" + "=" * 100)



# ========== æœ€ç»ˆæ€»ç»“ ==========
print("\n" + "=" * 100)
print("ğŸ‰ SHAPå¯è§£é‡Šæ€§åˆ†æå®Œæˆï¼".center(100))
print("=" * 100)

print(f"\nğŸ“Š åˆ†ææ€»ç»“:")
print(f"  â€¢ æ ·æœ¬æ•°é‡: {sample_size}")
print(f"  â€¢ ç‰¹å¾æ€»æ•°: {len(extended_feature_names)}")
print(f"  â€¢ ç”Ÿæˆå›¾è¡¨: 10å¼ ")
print(f"  â€¢ ç”ŸæˆæŠ¥å‘Š: 3ä»½")

print(f"\nğŸ” å…³é”®å‘ç°:")

# Top 5é‡è¦ç‰¹å¾
top5_features = importance_report.head(5)
print(f"\n  ã€æœ€é‡è¦çš„5ä¸ªç‰¹å¾ã€‘:")
for i, row in top5_features.iterrows():
    print(f"    {i + 1}. {row['Feature'][:50]}")
    print(f"       SHAPé‡è¦æ€§: {row['SHAP_Importance']:.6f}")

# ç±»åˆ«