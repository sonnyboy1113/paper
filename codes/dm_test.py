"""
Diebold-Mariano Test Module
============================

ç»Ÿè®¡æ£€éªŒæ¨¡å—ï¼Œç”¨äºæ¯”è¾ƒæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—æ€§

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-10-16
ç‰ˆæœ¬: 1.0.0

ä¸»è¦åŠŸèƒ½:
- å•ä¸€åŸºå‡†æ¨¡å‹å¯¹æ¯”
- ä¸¤ä¸¤é…å¯¹æ¯”è¾ƒ
- å¯è§†åŒ–åˆ†æ
- è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨ç¤ºä¾‹:
    from dm_test import DieboldMarianoTest, quick_dm_analysis

    # å¿«é€Ÿåˆ†æ
    results = quick_dm_analysis(y_true, predictions_dict, save_dir='./results/')

    # æˆ–ä½¿ç”¨å®Œæ•´ç±»
    dm_tester = DieboldMarianoTest()
    results_df = dm_tester.compare_models(y_true, predictions_dict, 'baseline')
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from math import sqrt
from typing import Dict, Tuple, Optional, Union
import warnings

warnings.filterwarnings('ignore')


class DieboldMarianoTest:
    """
    Diebold-Marianoæ£€éªŒç±»

    ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½æ˜¯å¦å­˜åœ¨ç»Ÿè®¡æ˜¾è‘—æ€§å·®å¼‚

    Attributes:
        loss_function (str): æŸå¤±å‡½æ•°ç±»å‹ ('mse', 'mae', 'mape')
        results (dict): å­˜å‚¨æ£€éªŒç»“æœ

    References:
        Diebold, F. X., & Mariano, R. S. (1995).
        Comparing predictive accuracy.
        Journal of Business & Economic Statistics, 13(3), 253-263.
    """

    def __init__(self, loss_function: str = 'mse'):
        """
        åˆå§‹åŒ–DMæ£€éªŒ

        Parameters:
            loss_function: æŸå¤±å‡½æ•°ç±»å‹ï¼Œå¯é€‰ 'mse', 'mae', 'mape'
        """
        self.loss_function = loss_function
        self.results = {}

    def _compute_loss(self, actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:
        """è®¡ç®—æŸå¤±"""
        if self.loss_function == 'mse':
            return (actual - predicted) ** 2
        elif self.loss_function == 'mae':
            return np.abs(actual - predicted)
        elif self.loss_function == 'mape':
            return np.abs((actual - predicted) / (actual + 1e-8))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss_function}")

    def dm_test(self,
                actual: np.ndarray,
                pred1: np.ndarray,
                pred2: np.ndarray,
                h: int = 1,
                crit: str = "MSE",
                power: int = 2) -> Tuple[float, float]:
        """
        æ‰§è¡ŒDiebold-Marianoæ£€éªŒ

        Parameters:
            actual: çœŸå®å€¼
            pred1: æ¨¡å‹1çš„é¢„æµ‹å€¼
            pred2: æ¨¡å‹2çš„é¢„æµ‹å€¼
            h: é¢„æµ‹æ­¥é•¿ï¼ˆç”¨äºè‡ªç›¸å…³è°ƒæ•´ï¼‰
            crit: æŸå¤±å‡½æ•°æ ‡å‡†
            power: æŸå¤±å‡½æ•°çš„å¹‚æ¬¡

        Returns:
            dm_stat: DMç»Ÿè®¡é‡
            p_value: på€¼ï¼ˆåŒä¾§æ£€éªŒï¼‰
        """
        # è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„
        actual = np.asarray(actual).flatten()
        pred1 = np.asarray(pred1).flatten()
        pred2 = np.asarray(pred2).flatten()

        # è®¡ç®—é¢„æµ‹è¯¯å·®
        e1 = actual - pred1
        e2 = actual - pred2

        # è®¡ç®—æŸå¤±å·®å¼‚
        if crit == "MSE":
            d = (e1 ** 2) - (e2 ** 2)
        elif crit == "MAE":
            d = np.abs(e1) - np.abs(e2)
        elif crit == "MAPE":
            d = np.abs(e1 / (actual + 1e-8)) - np.abs(e2 / (actual + 1e-8))
        else:
            d = (np.abs(e1) ** power) - (np.abs(e2) ** power)

        # è®¡ç®—å‡å€¼
        mean_d = np.mean(d)

        # è®¡ç®—è‡ªåæ–¹å·®å‡½æ•°
        def autocovariance(Xi, N, k, Xs):
            autoCov = 0
            for i in range(0, N - k):
                autoCov += ((Xi[i] - Xs) * (Xi[i + k] - Xs))
            return (1 / (N - 1)) * autoCov

        # è®¡ç®—é•¿æœŸæ–¹å·®
        gamma = [autocovariance(d, len(d), lag, mean_d) for lag in range(0, h)]
        V_d = gamma[0] + 2 * sum(gamma[1:])

        # é˜²æ­¢æ–¹å·®ä¸º0æˆ–è´Ÿæ•°
        if V_d <= 0:
            V_d = np.var(d, ddof=1)

        # è®¡ç®—DMç»Ÿè®¡é‡
        DM_stat = mean_d / sqrt(V_d / len(d))

        # è®¡ç®—på€¼ï¼ˆåŒä¾§æ£€éªŒï¼‰
        p_value = 2 * (1 - stats.norm.cdf(abs(DM_stat)))

        return DM_stat, p_value

    def compare_models(self,
                       actual: np.ndarray,
                       predictions_dict: Dict[str, np.ndarray],
                       baseline_model: Optional[str] = None) -> pd.DataFrame:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹ç›¸å¯¹äºåŸºå‡†æ¨¡å‹çš„æ€§èƒ½

        Parameters:
            actual: çœŸå®å€¼
            predictions_dict: æ¨¡å‹åç§°åˆ°é¢„æµ‹å€¼çš„å­—å…¸
            baseline_model: åŸºå‡†æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹

        Returns:
            results_df: åŒ…å«æ‰€æœ‰æ¯”è¾ƒç»“æœçš„DataFrame
        """
        if baseline_model is None:
            baseline_model = list(predictions_dict.keys())[0]

        if baseline_model not in predictions_dict:
            raise ValueError(f"åŸºå‡†æ¨¡å‹ '{baseline_model}' ä¸åœ¨é¢„æµ‹å­—å…¸ä¸­")

        baseline_pred = predictions_dict[baseline_model]

        results = []

        for model_name, pred in predictions_dict.items():
            if model_name == baseline_model:
                continue

            # æ‰§è¡ŒDMæ£€éªŒ
            dm_stat, p_value = self.dm_test(
                actual,
                baseline_pred,
                pred,
                h=1,
                crit="MSE"
            )

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            mse_baseline = np.mean((actual.flatten() - baseline_pred.flatten()) ** 2)
            mse_model = np.mean((actual.flatten() - pred.flatten()) ** 2)
            mse_improvement = ((mse_baseline - mse_model) / mse_baseline) * 100

            # åˆ¤æ–­æ˜¾è‘—æ€§
            if p_value < 0.01:
                significance = "***"
                sig_level = "é«˜åº¦æ˜¾è‘—"
            elif p_value < 0.05:
                significance = "**"
                sig_level = "æ˜¾è‘—"
            elif p_value < 0.1:
                significance = "*"
                sig_level = "è¾¹é™…æ˜¾è‘—"
            else:
                significance = ""
                sig_level = "ä¸æ˜¾è‘—"

            # åˆ¤æ–­æ–¹å‘
            if dm_stat > 0:
                direction = f"{model_name}ä¼˜äº{baseline_model}"
            else:
                direction = f"{baseline_model}ä¼˜äº{model_name}"

            results.append({
                'æ¯”è¾ƒæ¨¡å‹': model_name,
                'DMç»Ÿè®¡é‡': dm_stat,
                'på€¼': p_value,
                'æ˜¾è‘—æ€§': significance,
                'æ˜¾è‘—æ€§æ°´å¹³': sig_level,
                'MSEæ”¹å–„(%)': mse_improvement,
                'æ˜¯å¦æ˜¾è‘—': p_value < 0.05,
                'ç»“è®º': direction
            })

            # ä¿å­˜è¯¦ç»†ç»“æœ
            self.results[f"{baseline_model}_vs_{model_name}"] = {
                'dm_stat': dm_stat,
                'p_value': p_value,
                'significance': sig_level,
                'mse_improvement': mse_improvement
            }

        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('på€¼')

        return results_df

    def pairwise_comparison(self,
                            actual: np.ndarray,
                            predictions_dict: Dict[str, np.ndarray]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ä¸¤ä¸¤æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹

        Parameters:
            actual: çœŸå®å€¼
            predictions_dict: æ¨¡å‹åç§°åˆ°é¢„æµ‹å€¼çš„å­—å…¸

        Returns:
            dm_matrix_df: DMç»Ÿè®¡é‡çŸ©é˜µ
            pvalue_matrix_df: på€¼çŸ©é˜µ
        """
        model_names = list(predictions_dict.keys())
        n_models = len(model_names)

        # åˆå§‹åŒ–çŸ©é˜µ
        dm_matrix = np.zeros((n_models, n_models))
        pvalue_matrix = np.zeros((n_models, n_models))

        # ä¸¤ä¸¤æ¯”è¾ƒ
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i == j:
                    dm_matrix[i, j] = 0
                    pvalue_matrix[i, j] = 1
                elif i < j:
                    pred1 = predictions_dict[model1].flatten()
                    pred2 = predictions_dict[model2].flatten()

                    dm_stat, p_value = self.dm_test(actual, pred1, pred2, h=1, crit="MSE")

                    dm_matrix[i, j] = dm_stat
                    dm_matrix[j, i] = -dm_stat
                    pvalue_matrix[i, j] = p_value
                    pvalue_matrix[j, i] = p_value

        # åˆ›å»ºDataFrame
        dm_df = pd.DataFrame(dm_matrix, index=model_names, columns=model_names)
        pvalue_df = pd.DataFrame(pvalue_matrix, index=model_names, columns=model_names)

        return dm_df, pvalue_df

    def plot_comparison(self,
                        results_df: pd.DataFrame,
                        figsize: Tuple[int, int] = (18, 12),
                        save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶DMæ£€éªŒæ¯”è¾ƒå›¾ï¼ˆä¼˜åŒ–ç‰ˆ - è§£å†³æ–‡å­—é‡å é—®é¢˜ï¼‰

        Parameters:
            results_df: æ¯”è¾ƒç»“æœDataFrame
            figsize: å›¾å½¢å¤§å°
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(2, 2, figsize=figsize)

        # å­å›¾1: DMç»Ÿè®¡é‡ - ä¼˜åŒ–æ ‡ç­¾ä½ç½®
        colors1 = ['green' if x > 0 else 'red' for x in results_df['DMç»Ÿè®¡é‡']]
        axes[0, 0].barh(range(len(results_df)), results_df['DMç»Ÿè®¡é‡'],
                        color=colors1, alpha=0.7, edgecolor='black')
        axes[0, 0].axvline(x=0, color='black', linestyle='--', linewidth=2)
        axes[0, 0].set_yticks(range(len(results_df)))
        axes[0, 0].set_yticklabels([name[:25] for name in results_df['æ¯”è¾ƒæ¨¡å‹']], fontsize=9)
        axes[0, 0].set_xlabel('DMç»Ÿè®¡é‡', fontsize=11, fontweight='bold')
        axes[0, 0].set_title('Diebold-Marianoç»Ÿè®¡é‡\n(æ­£å€¼=ä¼˜äºåŸºå‡†)',
                             fontsize=12, fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3, axis='x')

        # ä¼˜åŒ–: æ™ºèƒ½æ”¾ç½®æ•°å€¼æ ‡ç­¾ï¼Œé¿å…é‡å 
        x_range = results_df['DMç»Ÿè®¡é‡'].max() - results_df['DMç»Ÿè®¡é‡'].min()
        offset = x_range * 0.08  # åŠ¨æ€è®¡ç®—åç§»é‡

        for i, (idx, row) in enumerate(results_df.iterrows()):
            dm_val = row['DMç»Ÿè®¡é‡']
            sig = row['æ˜¾è‘—æ€§']
            label = f"{dm_val:.3f}{sig}"

            # æ ¹æ®æ•°å€¼å¤§å°å’Œç¬¦å·æ™ºèƒ½è°ƒæ•´ä½ç½®
            if abs(dm_val) < x_range * 0.15:  # æ•°å€¼è¾ƒå°æ—¶ï¼Œæ”¾åœ¨å¤–ä¾§
                x_pos = dm_val + (offset if dm_val >= 0 else -offset)
                ha = 'left' if dm_val >= 0 else 'right'
            else:  # æ•°å€¼è¾ƒå¤§æ—¶ï¼Œå¯ä»¥æ”¾åœ¨æŸ±å†…
                x_pos = dm_val * 0.95
                ha = 'right' if dm_val > 0 else 'left'

            axes[0, 0].text(x_pos, i, label, va='center', ha=ha,
                            fontweight='bold', fontsize=8,
                            bbox=dict(boxstyle='round,pad=0.3',
                                      facecolor='white',
                                      edgecolor='gray',
                                      alpha=0.8))

        # å­å›¾2: på€¼ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
        p_colors = ['green' if p < 0.05 else 'orange' if p < 0.1 else 'red'
                    for p in results_df['på€¼']]
        axes[0, 1].barh(range(len(results_df)), results_df['på€¼'],
                        color=p_colors, alpha=0.7, edgecolor='black')
        axes[0, 1].axvline(x=0.05, color='red', linestyle='--',
                           linewidth=2, label='p=0.05', alpha=0.7)
        axes[0, 1].axvline(x=0.1, color='orange', linestyle='--',
                           linewidth=2, label='p=0.1', alpha=0.7)
        axes[0, 1].set_yticks(range(len(results_df)))
        axes[0, 1].set_yticklabels([name[:25] for name in results_df['æ¯”è¾ƒæ¨¡å‹']], fontsize=9)
        axes[0, 1].set_xlabel('på€¼', fontsize=11, fontweight='bold')
        axes[0, 1].set_title('ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ\n(p<0.05ä¸ºæ˜¾è‘—)',
                             fontsize=12, fontweight='bold')
        axes[0, 1].legend(fontsize=9)
        axes[0, 1].grid(True, alpha=0.3, axis='x')

        # å­å›¾3: MSEæ”¹å–„ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
        mse_colors = ['green' if m > 0 else 'red' for m in results_df['MSEæ”¹å–„(%)']]
        axes[1, 0].barh(range(len(results_df)), results_df['MSEæ”¹å–„(%)'],
                        color=mse_colors, alpha=0.7, edgecolor='black')
        axes[1, 0].axvline(x=0, color='black', linestyle='--', linewidth=2)
        axes[1, 0].set_yticks(range(len(results_df)))
        axes[1, 0].set_yticklabels([name[:25] for name in results_df['æ¯”è¾ƒæ¨¡å‹']], fontsize=9)
        axes[1, 0].set_xlabel('MSEæ”¹å–„(%)', fontsize=11, fontweight='bold')
        axes[1, 0].set_title('ç›¸å¯¹åŸºå‡†çš„MSEæ”¹å–„\n(æ­£å€¼=æ€§èƒ½æ›´å¥½)',
                             fontsize=12, fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3, axis='x')

        # å­å›¾4: æ•£ç‚¹å›¾
        scatter_colors = ['green' if sig else 'red' for sig in results_df['æ˜¯å¦æ˜¾è‘—']]
        axes[1, 1].scatter(results_df['DMç»Ÿè®¡é‡'], results_df['MSEæ”¹å–„(%)'],
                           c=scatter_colors, s=150, alpha=0.6, edgecolors='black', linewidth=1.5)
        axes[1, 1].axhline(y=0, color='gray', linestyle='--', linewidth=1)
        axes[1, 1].axvline(x=0, color='gray', linestyle='--', linewidth=1)
        axes[1, 1].axvline(x=1.96, color='red', linestyle=':', linewidth=2,
                           label='DM=Â±1.96 (pâ‰ˆ0.05)', alpha=0.7)
        axes[1, 1].axvline(x=-1.96, color='red', linestyle=':', linewidth=2, alpha=0.7)
        axes[1, 1].set_xlabel('DMç»Ÿè®¡é‡', fontsize=11, fontweight='bold')
        axes[1, 1].set_ylabel('MSEæ”¹å–„(%)', fontsize=11, fontweight='bold')
        axes[1, 1].set_title('ç»Ÿè®¡æ˜¾è‘—æ€§ vs å®é™…æ”¹å–„\n(ç»¿è‰²=æ˜¾è‘—, çº¢è‰²=ä¸æ˜¾è‘—)',
                             fontsize=12, fontweight='bold')
        axes[1, 1].legend(fontsize=9)
        axes[1, 1].grid(True, alpha=0.3)

        # æ ‡æ³¨å‰3ä¸ªæ¨¡å‹
        for _, row in results_df.head(3).iterrows():
            axes[1, 1].annotate(
                row['æ¯”è¾ƒæ¨¡å‹'][:15],
                (row['DMç»Ÿè®¡é‡'], row['MSEæ”¹å–„(%)']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=7, alpha=0.8,
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3)
            )

        plt.suptitle('Diebold-Marianoæ£€éªŒç»¼åˆåˆ†æ',
                     fontsize=14, fontweight='bold', y=0.995)
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {save_path}")

        plt.show()

    def plot_heatmap(self,
                     dm_df: pd.DataFrame,
                     pvalue_df: pd.DataFrame,
                     figsize: Tuple[int, int] = (14, 12),
                     save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶DMæ£€éªŒçƒ­åŠ›å›¾

        Parameters:
            dm_df: DMç»Ÿè®¡é‡çŸ©é˜µ
            pvalue_df: på€¼çŸ©é˜µ
            figsize: å›¾å½¢å¤§å°
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(1, 2, figsize=figsize)

        # DMç»Ÿè®¡é‡çƒ­åŠ›å›¾
        sns.heatmap(dm_df, annot=True, fmt='.3f', cmap='RdYlGn', center=0,
                    cbar_kws={'label': 'DMç»Ÿè®¡é‡'}, ax=axes[0],
                    linewidths=0.5, square=True)
        axes[0].set_title('Diebold-Marianoç»Ÿè®¡é‡çƒ­åŠ›å›¾\n(æ­£å€¼è¡¨ç¤ºè¡Œæ¨¡å‹ä¼˜äºåˆ—æ¨¡å‹)',
                          fontsize=13, fontweight='bold', pad=15)

        # på€¼çƒ­åŠ›å›¾ï¼ˆå¸¦æ˜¾è‘—æ€§æ ‡è®°ï¼‰
        annot_matrix = pvalue_df.copy()
        for i in range(len(pvalue_df)):
            for j in range(len(pvalue_df.columns)):
                p_val = pvalue_df.iloc[i, j]
                if i == j:
                    annot_matrix.iloc[i, j] = "-"
                elif p_val < 0.01:
                    annot_matrix.iloc[i, j] = f"{p_val:.4f}***"
                elif p_val < 0.05:
                    annot_matrix.iloc[i, j] = f"{p_val:.4f}**"
                elif p_val < 0.1:
                    annot_matrix.iloc[i, j] = f"{p_val:.4f}*"
                else:
                    annot_matrix.iloc[i, j] = f"{p_val:.4f}"

        sns.heatmap(pvalue_df, annot=annot_matrix, fmt='', cmap='RdYlGn_r',
                    cbar_kws={'label': 'på€¼'}, ax=axes[1],
                    linewidths=0.5, square=True, vmin=0, vmax=0.1)
        axes[1].set_title('på€¼çƒ­åŠ›å›¾\n(* p<0.1, ** p<0.05, *** p<0.01)',
                          fontsize=13, fontweight='bold', pad=15)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")

        plt.show()

    def generate_report(self,
                        results_df: pd.DataFrame,
                        baseline_model: str,
                        save_path: Optional[str] = None) -> None:
        """
        ç”ŸæˆDMæ£€éªŒæŠ¥å‘Š

        Parameters:
            results_df: æ¯”è¾ƒç»“æœDataFrame
            baseline_model: åŸºå‡†æ¨¡å‹åç§°
            save_path: ä¿å­˜è·¯å¾„
        """
        print("\n" + "=" * 100)
        print(f"Diebold-Marianoæ£€éªŒæŠ¥å‘Š (åŸºå‡†æ¨¡å‹: {baseline_model})".center(100))
        print("=" * 100)

        print(f"\n{'æ¨¡å‹':<25} {'DMç»Ÿè®¡é‡':>12} {'på€¼':>10} {'æ˜¾è‘—æ€§':>8} "
              f"{'MSEæ”¹å–„(%)':>12} {'ç»“è®º':<30}")
        print("-" * 100)

        for _, row in results_df.iterrows():
            print(f"{row['æ¯”è¾ƒæ¨¡å‹']:<25} {row['DMç»Ÿè®¡é‡']:>12.4f} {row['på€¼']:>10.4f} "
                  f"{row['æ˜¾è‘—æ€§']:>8} {row['MSEæ”¹å–„(%)']:>12.2f} {row['ç»“è®º']:<30}")

        print("\n" + "=" * 100)
        print("ç»Ÿè®¡æ‘˜è¦".center(100))
        print("=" * 100)

        n_significant = len(results_df[results_df['på€¼'] < 0.05])
        n_marginal = len(results_df[(results_df['på€¼'] >= 0.05) & (results_df['på€¼'] < 0.1)])
        n_not_significant = len(results_df[results_df['på€¼'] >= 0.1])

        print(f"\næ€»æ¯”è¾ƒæ•°: {len(results_df)}")
        print(f"  - æ˜¾è‘—ä¼˜äºåŸºå‡† (p < 0.05): {n_significant} ä¸ª "
              f"({n_significant / len(results_df) * 100:.1f}%)")
        print(f"  - è¾¹é™…æ˜¾è‘— (0.05 â‰¤ p < 0.1): {n_marginal} ä¸ª "
              f"({n_marginal / len(results_df) * 100:.1f}%)")
        print(f"  - æ— æ˜¾è‘—å·®å¼‚ (p â‰¥ 0.1): {n_not_significant} ä¸ª "
              f"({n_not_significant / len(results_df) * 100:.1f}%)")

        if n_significant > 0:
            best_models = results_df[results_df['på€¼'] < 0.05].sort_values(
                'MSEæ”¹å–„(%)', ascending=False
            )
            print(f"\nğŸ† æ˜¾è‘—ä¼˜äºåŸºå‡†çš„æ¨¡å‹ (Top 3):")
            for i, (_, row) in enumerate(best_models.head(3).iterrows(), 1):
                print(f"  {i}. {row['æ¯”è¾ƒæ¨¡å‹']}: "
                      f"DM={row['DMç»Ÿè®¡é‡']:.4f}, p={row['på€¼']:.4f}, "
                      f"MSEæ”¹å–„={row['MSEæ”¹å–„(%)']:.2f}%")

        print(f"\nğŸ’¡ è§£é‡Šè¯´æ˜:")
        print(f"  - DMç»Ÿè®¡é‡: æ­£å€¼è¡¨ç¤ºè¯¥æ¨¡å‹ä¼˜äº{baseline_model}")
        print(f"  - på€¼: è¶Šå°è¡¨ç¤ºå·®å¼‚è¶Šæ˜¾è‘—")
        print(f"  - æ˜¾è‘—æ€§æ°´å¹³: *** p<0.01, ** p<0.05, * p<0.1")
        print(f"  - MSEæ”¹å–„: æ­£å€¼è¡¨ç¤ºè¯¥æ¨¡å‹MSEæ›´ä½ï¼ˆæ€§èƒ½æ›´å¥½ï¼‰")

        if save_path:
            results_df.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")


# ========================================
# ä¾¿æ·å‡½æ•°
# ========================================

def quick_dm_analysis(y_true: np.ndarray,
                      predictions: Dict[str, np.ndarray],
                      baseline: str = None,
                      save_dir: str = './results/',
                      plot: bool = True,
                      verbose: bool = True) -> pd.DataFrame:
    """
    å¿«é€Ÿæ‰§è¡ŒDMæ£€éªŒåˆ†æï¼ˆä¸€è¡Œä»£ç æå®šï¼‰

    Parameters:
        y_true: çœŸå®å€¼
        predictions: é¢„æµ‹ç»“æœå­—å…¸ {æ¨¡å‹å: é¢„æµ‹å€¼}
        baseline: åŸºå‡†æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
        save_dir: ç»“æœä¿å­˜ç›®å½•
        plot: æ˜¯å¦ç»˜åˆ¶å›¾è¡¨
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        results_df: DMæ£€éªŒç»“æœDataFrame

    Example:
        >>> results = quick_dm_analysis(
        ...     y_true=y_test,
        ...     predictions={
        ...         'åŸºå‡†æ¨¡å‹': pred_baseline,
        ...         'ç­–ç•¥1': pred_strategy1,
        ...         'ç­–ç•¥2': pred_strategy2
        ...     },
        ...     save_dir='./results/'
        ... )
    """
    import os

    # åˆ›å»ºä¿å­˜ç›®å½•
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # åˆå§‹åŒ–
    dm_tester = DieboldMarianoTest(loss_function='mse')

    # æ‰§è¡Œæ¯”è¾ƒ
    if baseline is None:
        baseline = list(predictions.keys())[0]

    results_df = dm_tester.compare_models(y_true, predictions, baseline)

    # ç”ŸæˆæŠ¥å‘Š
    if verbose:
        dm_tester.generate_report(
            results_df,
            baseline,
            save_path=os.path.join(save_dir, 'dm_test_report.csv')
        )

    # ç»˜å›¾
    if plot:
        dm_tester.plot_comparison(
            results_df,
            save_path=os.path.join(save_dir, 'dm_comparison.png')
        )

    return results_df


def pairwise_dm_analysis(y_true: np.ndarray,
                         predictions: Dict[str, np.ndarray],
                         save_dir: str = './results/',
                         plot: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å¿«é€Ÿæ‰§è¡Œä¸¤ä¸¤æ¯”è¾ƒåˆ†æ

    Parameters:
        y_true: çœŸå®å€¼
        predictions: é¢„æµ‹ç»“æœå­—å…¸
        save_dir: ç»“æœä¿å­˜ç›®å½•
        plot: æ˜¯å¦ç»˜åˆ¶çƒ­åŠ›å›¾

    Returns:
        dm_matrix: DMç»Ÿè®¡é‡çŸ©é˜µ
        pvalue_matrix: på€¼çŸ©é˜µ
    """
    import os

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    dm_tester = DieboldMarianoTest()
    dm_matrix, pvalue_matrix = dm_tester.pairwise_comparison(y_true, predictions)

    # ä¿å­˜ç»“æœ
    dm_matrix.to_csv(os.path.join(save_dir, 'dm_matrix.csv'))
    pvalue_matrix.to_csv(os.path.join(save_dir, 'pvalue_matrix.csv'))

    # ç»˜å›¾
    if plot:
        dm_tester.plot_heatmap(
            dm_matrix,
            pvalue_matrix,
            save_path=os.path.join(save_dir, 'dm_heatmap.png')
        )

    return dm_matrix, pvalue_matrix


# ========================================
# ç‰ˆæœ¬ä¿¡æ¯
# ========================================

__version__ = '1.0.0'
__author__ = 'AI Assistant'
__all__ = ['DieboldMarianoTest', 'quick_dm_analysis', 'pairwise_dm_analysis']

if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    print("Diebold-Mariano Test Module")
    print(f"Version: {__version__}")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print(">>> from dm_test import quick_dm_analysis")
    print(">>> results = quick_dm_analysis(y_true, predictions_dict)")